{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MedRAG Multi-Modal","text":"<p>An ongoing journey to build a robust and scaleable multi-modal question answering system for the domain of medicine and life sciences powered by SoTA generative AI, NLP, and computer vision models.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/soumik12345/medrag-multi-modal\ncd medrag-multi-modal\nsh install.sh\n</code></pre>"},{"location":"app/","title":"MedQA Assistant App","text":"<p>The MedQA Assistant App is a Streamlit-based application designed to provide a chat interface for medical question answering. It leverages advanced language models (LLMs) and retrieval augmented generation (RAG) techniques to deliver accurate and informative responses to medical queries.</p>"},{"location":"app/#features","title":"Features","text":"<ul> <li>Interactive Chat Interface: Engage with the app through a user-friendly chat interface.</li> <li>Configurable Settings: Customize model selection and data sources via the sidebar.</li> <li>Retrieval-Augmented Generation: Ensures precise and contextually relevant responses.</li> <li>Figure Annotation Capabilities: Extracts and annotates figures from medical texts.</li> </ul>"},{"location":"app/#usage","title":"Usage","text":"<ol> <li>Install the package using:     <pre><code>uv pip install .\n</code></pre></li> <li>Launch the App: Start the application using Streamlit:     <pre><code>medrag run\n</code></pre></li> <li>Configure Settings: Adjust configuration settings in the sidebar to suit your needs.</li> <li>Ask a Question: Enter your medical question in the chat input field.</li> <li>Receive a Response: Get a detailed answer from the MedQA Assistant.</li> </ol>"},{"location":"app/#configuration","title":"Configuration","text":"<p>The app allows users to customize various settings through the sidebar:</p> <ul> <li>Project Name: Specify the WandB project name.</li> <li>Text Chunk WandB Dataset Name: Define the dataset containing text chunks.</li> <li>WandB Index Artifact Address: Provide the address of the index artifact.</li> <li>WandB Image Artifact Address: Provide the address of the image artifact.</li> <li>LLM Client Model Name: Choose a language model for generating responses.</li> <li>Figure Extraction Model Name: Select a model for extracting figures from images.</li> <li>Structured Output Model Name: Choose a model for generating structured outputs.</li> </ul>"},{"location":"app/#technical-details","title":"Technical Details","text":"<p>The app is built using the following components:</p> <ul> <li>Streamlit: For the user interface.</li> <li>Weave: For project initialization and artifact management.</li> <li>MedQAAssistant: For processing queries and generating responses.</li> <li>LLMClient: For interacting with language models.</li> <li>MedCPTRetriever: For retrieving relevant text chunks.</li> <li>FigureAnnotatorFromPageImage: For annotating figures in medical texts.</li> </ul>"},{"location":"app/#development-and-deployment","title":"Development and Deployment","text":"<ul> <li>Environment Setup: Ensure all dependencies are installed as per the <code>pyproject.toml</code>.</li> <li>Running the App: Use Streamlit to run the app locally.</li> <li>Deployment: coming soon...</li> </ul>"},{"location":"installation/development/","title":"Setting up the development environment","text":""},{"location":"installation/development/#install-poppler","title":"Install Poppler","text":"<p>For MacOS, you need to run</p> <pre><code>brew install poppler\n</code></pre> <p>For Debian/Ubuntu, you need to run</p> <pre><code>sudo apt-get install -y poppler-utils\n</code></pre>"},{"location":"installation/development/#install-the-dependencies","title":"Install the dependencies","text":"<p>Then, you can install the dependencies using uv in the virtual environment <code>.venv</code> using</p> <pre><code>git clone https://github.com/soumik12345/medrag-multi-modal\ncd medrag-multi-modal\npip install -U pip uv\nuv sync\n</code></pre> <p>After this, you need to activate the virtual environment using</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"installation/development/#optional-install-flash-attention","title":"[Optional] Install Flash Attention","text":"<p>In the activated virtual environment, you can optionally install Flash Attention (required for ColPali) using</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre>"},{"location":"installation/install/","title":"Installation","text":"<p>You just need to clone the repository and run the install.sh script</p> <pre><code>git clone https://github.com/soumik12345/medrag-multi-modal\ncd medrag-multi-modal\nsh install.sh\n</code></pre>"},{"location":"rag/chunking/","title":"Chunking","text":""},{"location":"rag/chunking/#medrag_multi_modal.semantic_chunking.SemanticChunker","title":"<code>SemanticChunker</code>","text":"<p>SemanticChunker is a class that chunks documents into smaller segments and publishes them as datasets.</p> <p>This class uses the <code>semchunk</code> library to break down large documents into smaller, manageable chunks based on a specified tokenizer or token counter. This is particularly useful for processing large text datasets where smaller segments are needed for analysis or other operations.</p> <p>Example Usage</p> <pre><code>from medrag_multi_modal.semantic_chunking import SemanticChunker\n\n\nchunker = SemanticChunker(chunk_size=256)\nchunker.chunk(\n    document_dataset=\"geekyrakshit/grays-anatomy-test\",\n    dataset_split=\"pdfplumbertextloader\",\n    chunk_dataset_repo_id=\"geekyrakshit/grays-anatomy-chunks-test\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tokenizer_or_token_counter</code> <code>TOKENIZER_OR_TOKEN_COUNTER</code> <p>The tokenizer or token counter to be used for chunking.</p> <code>'o200k_base'</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The size of each chunk. If not specified, the default chunk size from <code>semchunk</code> will be used.</p> <code>None</code> <code>max_token_chars</code> <code>Optional[int]</code> <p>The maximum number of characters per token. If not specified, the default value from <code>semchunk</code> will be used.</p> <code>None</code> <code>memoize</code> <code>bool</code> <p>Whether to memoize the chunking process for efficiency. Default is True.</p> <code>True</code> Source code in <code>medrag_multi_modal/semantic_chunking.py</code> <pre><code>class SemanticChunker:\n    \"\"\"\n    SemanticChunker is a class that chunks documents into smaller segments and\n    publishes them as datasets.\n\n    This class uses the `semchunk` library to break down large documents into\n    smaller, manageable chunks based on a specified tokenizer or token counter.\n    This is particularly useful for processing large text datasets where\n    smaller segments are needed for analysis or other operations.\n\n    !!! example \"Example Usage\"\n        ```python\n        from medrag_multi_modal.semantic_chunking import SemanticChunker\n\n\n        chunker = SemanticChunker(chunk_size=256)\n        chunker.chunk(\n            document_dataset=\"geekyrakshit/grays-anatomy-test\",\n            dataset_split=\"pdfplumbertextloader\",\n            chunk_dataset_repo_id=\"geekyrakshit/grays-anatomy-chunks-test\",\n        )\n        ```\n\n    Args:\n        tokenizer_or_token_counter (TOKENIZER_OR_TOKEN_COUNTER): The tokenizer or\n            token counter to be used for chunking.\n        chunk_size (Optional[int]): The size of each chunk. If not specified, the\n            default chunk size from `semchunk` will be used.\n        max_token_chars (Optional[int]): The maximum number of characters per token.\n            If not specified, the default value from `semchunk` will be used.\n        memoize (bool): Whether to memoize the chunking process for efficiency.\n            Default is True.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer_or_token_counter: TOKENIZER_OR_TOKEN_COUNTER = \"o200k_base\",\n        chunk_size: Optional[int] = None,\n        max_token_chars: Optional[int] = None,\n        memoize: bool = True,\n        streamlit_mode: bool = False,\n    ) -&gt; None:\n        self.chunker = semchunk.chunkerify(\n            tokenizer_or_token_counter,\n            chunk_size=chunk_size,\n            max_token_chars=max_token_chars,\n            memoize=memoize,\n        )\n        self.streamlit_mode = streamlit_mode\n\n    def chunk(\n        self,\n        document_dataset: Union[Dataset, str],\n        dataset_split: str,\n        chunk_dataset_repo_id: Optional[str] = None,\n        is_dataset_private: bool = False,\n        overwrite_dataset: bool = False,\n    ) -&gt; Dataset:\n        \"\"\"\n        Chunks a document dataset into smaller segments and publishes them as a new dataset.\n\n        This function takes a document dataset, either as a HuggingFace Dataset object or a string\n        representing the dataset repository ID, and chunks the documents into smaller segments using\n        the specified chunker. The resulting chunks are then optionally published to a HuggingFace\n        dataset repository.\n\n        Args:\n            document_dataset (Union[Dataset, str]): The document dataset to be chunked. It can be either\n                a HuggingFace Dataset object or a string representing the dataset repository ID.\n            dataset_split (str): The split of the dataset to publish the chunks to.\n            chunk_dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish\n                the chunks to, if provided.\n            is_dataset_private (bool): Whether the dataset is private.\n            overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists.\n\n        Returns:\n            Dataset: A HuggingFace Dataset object containing the chunks.\n        \"\"\"\n        document_dataset = (\n            load_dataset(document_dataset, split=dataset_split)\n            if isinstance(document_dataset, str)\n            else document_dataset\n        ).to_list()\n\n        chunks = []\n\n        async def process_document(idx, document):\n            document_chunks = self.chunker.chunk(str(document[\"text\"]))\n            for chunk in document_chunks:\n                chunk_dict = {\"document_idx\": idx, \"text\": chunk}\n                for key, value in document.items():\n                    if key not in chunk_dict:\n                        chunk_dict[key] = value\n                chunks.append(chunk_dict)\n\n        async def process_all_documents():\n            tasks = []\n            streamlit_progressbar = (\n                st.progress(0, text=\"Chunking documents\")\n                if self.streamlit_mode\n                else None\n            )\n            for idx, document in track(\n                enumerate(document_dataset),\n                total=len(document_dataset),\n                description=\"Chunking documents\",\n            ):\n                tasks.append(process_document(idx, document))\n                if streamlit_progressbar:\n                    progress_percentage = int((idx / (len(document_dataset) - 1)) * 100)\n                    streamlit_progressbar.progress(\n                        progress_percentage,\n                        text=f\"Chunking documents ({idx}/{len(document_dataset) - 1})\",\n                    )\n            await asyncio.gather(*tasks)\n\n        asyncio.run(process_all_documents())\n\n        chunks.sort(key=lambda x: x[\"document_idx\"])\n\n        dataset = Dataset.from_list(chunks)\n        if chunk_dataset_repo_id:\n            if huggingface_hub.repo_exists(chunk_dataset_repo_id, repo_type=\"dataset\"):\n                existing_dataset = load_dataset(chunk_dataset_repo_id)\n                if not overwrite_dataset:\n                    if dataset_split in existing_dataset:\n                        existing_dataset[dataset_split] = concatenate_datasets(\n                            [dataset, existing_dataset[dataset_split]]\n                        )\n                        dataset = existing_dataset\n                    else:\n                        existing_dataset[dataset_split] = dataset\n                        dataset = existing_dataset\n                else:\n                    existing_dataset[dataset_split] = dataset\n                    dataset = existing_dataset\n            if isinstance(dataset, DatasetDict):\n                if \"train\" in dataset.keys():\n                    del dataset[\"train\"]\n                dataset.push_to_hub(\n                    repo_id=chunk_dataset_repo_id, private=is_dataset_private\n                )\n            else:\n                dataset.push_to_hub(\n                    repo_id=chunk_dataset_repo_id,\n                    private=is_dataset_private,\n                    split=dataset_split,\n                )\n\n        return dataset\n</code></pre>"},{"location":"rag/chunking/#medrag_multi_modal.semantic_chunking.SemanticChunker.chunk","title":"<code>chunk(document_dataset, dataset_split, chunk_dataset_repo_id=None, is_dataset_private=False, overwrite_dataset=False)</code>","text":"<p>Chunks a document dataset into smaller segments and publishes them as a new dataset.</p> <p>This function takes a document dataset, either as a HuggingFace Dataset object or a string representing the dataset repository ID, and chunks the documents into smaller segments using the specified chunker. The resulting chunks are then optionally published to a HuggingFace dataset repository.</p> <p>Parameters:</p> Name Type Description Default <code>document_dataset</code> <code>Union[Dataset, str]</code> <p>The document dataset to be chunked. It can be either a HuggingFace Dataset object or a string representing the dataset repository ID.</p> required <code>dataset_split</code> <code>str</code> <p>The split of the dataset to publish the chunks to.</p> required <code>chunk_dataset_repo_id</code> <code>Optional[str]</code> <p>The repository ID of the HuggingFace dataset to publish the chunks to, if provided.</p> <code>None</code> <code>is_dataset_private</code> <code>bool</code> <p>Whether the dataset is private.</p> <code>False</code> <code>overwrite_dataset</code> <code>bool</code> <p>Whether to overwrite the existing dataset if it exists.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A HuggingFace Dataset object containing the chunks.</p> Source code in <code>medrag_multi_modal/semantic_chunking.py</code> <pre><code>def chunk(\n    self,\n    document_dataset: Union[Dataset, str],\n    dataset_split: str,\n    chunk_dataset_repo_id: Optional[str] = None,\n    is_dataset_private: bool = False,\n    overwrite_dataset: bool = False,\n) -&gt; Dataset:\n    \"\"\"\n    Chunks a document dataset into smaller segments and publishes them as a new dataset.\n\n    This function takes a document dataset, either as a HuggingFace Dataset object or a string\n    representing the dataset repository ID, and chunks the documents into smaller segments using\n    the specified chunker. The resulting chunks are then optionally published to a HuggingFace\n    dataset repository.\n\n    Args:\n        document_dataset (Union[Dataset, str]): The document dataset to be chunked. It can be either\n            a HuggingFace Dataset object or a string representing the dataset repository ID.\n        dataset_split (str): The split of the dataset to publish the chunks to.\n        chunk_dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish\n            the chunks to, if provided.\n        is_dataset_private (bool): Whether the dataset is private.\n        overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists.\n\n    Returns:\n        Dataset: A HuggingFace Dataset object containing the chunks.\n    \"\"\"\n    document_dataset = (\n        load_dataset(document_dataset, split=dataset_split)\n        if isinstance(document_dataset, str)\n        else document_dataset\n    ).to_list()\n\n    chunks = []\n\n    async def process_document(idx, document):\n        document_chunks = self.chunker.chunk(str(document[\"text\"]))\n        for chunk in document_chunks:\n            chunk_dict = {\"document_idx\": idx, \"text\": chunk}\n            for key, value in document.items():\n                if key not in chunk_dict:\n                    chunk_dict[key] = value\n            chunks.append(chunk_dict)\n\n    async def process_all_documents():\n        tasks = []\n        streamlit_progressbar = (\n            st.progress(0, text=\"Chunking documents\")\n            if self.streamlit_mode\n            else None\n        )\n        for idx, document in track(\n            enumerate(document_dataset),\n            total=len(document_dataset),\n            description=\"Chunking documents\",\n        ):\n            tasks.append(process_document(idx, document))\n            if streamlit_progressbar:\n                progress_percentage = int((idx / (len(document_dataset) - 1)) * 100)\n                streamlit_progressbar.progress(\n                    progress_percentage,\n                    text=f\"Chunking documents ({idx}/{len(document_dataset) - 1})\",\n                )\n        await asyncio.gather(*tasks)\n\n    asyncio.run(process_all_documents())\n\n    chunks.sort(key=lambda x: x[\"document_idx\"])\n\n    dataset = Dataset.from_list(chunks)\n    if chunk_dataset_repo_id:\n        if huggingface_hub.repo_exists(chunk_dataset_repo_id, repo_type=\"dataset\"):\n            existing_dataset = load_dataset(chunk_dataset_repo_id)\n            if not overwrite_dataset:\n                if dataset_split in existing_dataset:\n                    existing_dataset[dataset_split] = concatenate_datasets(\n                        [dataset, existing_dataset[dataset_split]]\n                    )\n                    dataset = existing_dataset\n                else:\n                    existing_dataset[dataset_split] = dataset\n                    dataset = existing_dataset\n            else:\n                existing_dataset[dataset_split] = dataset\n                dataset = existing_dataset\n        if isinstance(dataset, DatasetDict):\n            if \"train\" in dataset.keys():\n                del dataset[\"train\"]\n            dataset.push_to_hub(\n                repo_id=chunk_dataset_repo_id, private=is_dataset_private\n            )\n        else:\n            dataset.push_to_hub(\n                repo_id=chunk_dataset_repo_id,\n                private=is_dataset_private,\n                split=dataset_split,\n            )\n\n    return dataset\n</code></pre>"},{"location":"rag/assistant/figure_annotation/","title":"Figure Annotation","text":""},{"location":"rag/assistant/figure_annotation/#medrag_multi_modal.assistant.figure_annotation.FigureAnnotatorFromPageImage","title":"<code>FigureAnnotatorFromPageImage</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>FigureAnnotatorFromPageImage</code> is a class that leverages two LLM clients to annotate figures from a page image of a scientific textbook.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.assistant import (\n    FigureAnnotatorFromPageImage, LLMClient\n)\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nfigure_annotator = FigureAnnotatorFromPageImage(\n    figure_extraction_llm_client=LLMClient(model_name=\"pixtral-12b-2409\"),\n    structured_output_llm_client=LLMClient(model_name=\"gpt-4o\"),\n    image_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-images-marker:v6\",\n)\nannotations = figure_annotator.predict(page_idx=34)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>figure_extraction_llm_client</code> <code>LLMClient</code> <p>An LLM client used to extract figure annotations from the page image.</p> required <code>structured_output_llm_client</code> <code>LLMClient</code> <p>An LLM client used to convert the extracted annotations into a structured format.</p> required <code>image_artifact_address</code> <code>Optional[str]</code> <p>The address of the image artifact containing the page images.</p> <code>None</code> Source code in <code>medrag_multi_modal/assistant/figure_annotation.py</code> <pre><code>class FigureAnnotatorFromPageImage(weave.Model):\n    \"\"\"\n    `FigureAnnotatorFromPageImage` is a class that leverages two LLM clients to annotate\n    figures from a page image of a scientific textbook.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.assistant import (\n            FigureAnnotatorFromPageImage, LLMClient\n        )\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        figure_annotator = FigureAnnotatorFromPageImage(\n            figure_extraction_llm_client=LLMClient(model_name=\"pixtral-12b-2409\"),\n            structured_output_llm_client=LLMClient(model_name=\"gpt-4o\"),\n            image_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-images-marker:v6\",\n        )\n        annotations = figure_annotator.predict(page_idx=34)\n        ```\n\n    Args:\n        figure_extraction_llm_client (LLMClient): An LLM client used to extract figure annotations\n            from the page image.\n        structured_output_llm_client (LLMClient): An LLM client used to convert the extracted\n            annotations into a structured format.\n        image_artifact_address (Optional[str]): The address of the image artifact containing the\n            page images.\n    \"\"\"\n\n    figure_extraction_llm_client: LLMClient\n    structured_output_llm_client: LLMClient\n    _artifact_dir: str\n\n    def __init__(\n        self,\n        figure_extraction_llm_client: LLMClient,\n        structured_output_llm_client: LLMClient,\n        image_artifact_address: Optional[str] = None,\n    ):\n        super().__init__(\n            figure_extraction_llm_client=figure_extraction_llm_client,\n            structured_output_llm_client=structured_output_llm_client,\n        )\n        self._artifact_dir = get_wandb_artifact(image_artifact_address, \"dataset\")\n\n    @weave.op()\n    def annotate_figures(\n        self, page_image: Image.Image\n    ) -&gt; dict[str, Union[Image.Image, str]]:\n        annotation = self.figure_extraction_llm_client.predict(\n            system_prompt=\"\"\"\nYou are an expert in the domain of scientific textbooks, especially medical texts.\nYou are presented with a page from a scientific textbook from the domain of biology, specifically anatomy.\nYou are to first identify all the figures in the page image, which could be images or biological diagrams, charts, graphs, etc.\nThen you are to identify the figure IDs associated with each figure in the page image.\nThen, you are to extract only the exact figure descriptions from the page image.\nYou need to output the figure IDs and figure descriptions only, in a structured manner as a JSON object.\n\nHere are some clues you need to follow:\n1. Figure IDs are unique identifiers for each figure in the page image.\n2. Sometimes figure IDs can also be found as captions to the immediate left, right, top, or bottom of the figure.\n3. Figure IDs are in the form \"Fig X.Y\" where X and Y are integers. For example, 1.1, 1.2, 1.3, etc.\n4. Figure descriptions are contained as captions under the figures in the image, just after the figure ID.\n5. The text in the page image is written in English and is present in a two-column format.\n6. There is a clear distinction between the figure caption and the regular text in the page image in the form of extra white space.\n    You are to carefully identify all the figures in the page image.\n7. There might be multiple figures or even no figures present in the page image. Sometimes the figures can be present side-by-side\n    or one above the other.\n8. The figures may or may not have a distinct border against a white background.\n10. You are not supposed to alter the figure description in any way present in the page image and you are to extract it as is.\n\"\"\",\n            user_prompt=[page_image],\n        )\n        return {\"page_image\": page_image, \"annotations\": annotation}\n\n    @weave.op\n    def extract_structured_output(self, annotations: str) -&gt; FigureAnnotations:\n        return self.structured_output_llm_client.predict(\n            system_prompt=\"You are suppossed to extract a list of figure annotations consisting of figure IDs and corresponding figure descriptions.\",\n            user_prompt=[annotations],\n            schema=FigureAnnotations,\n        )\n\n    @weave.op()\n    def predict(self, page_idx: int) -&gt; dict[int, list[FigureAnnotations]]:\n        \"\"\"\n        Predicts figure annotations for a specific page in a document.\n\n        This function retrieves the artifact directory from the given image artifact address,\n        reads the metadata from the 'metadata.jsonl' file, and iterates through the metadata\n        to find the specified page index. If the page index matches, it reads the page image\n        and associated figure images, and then uses the `annotate_figures` method to extract\n        figure annotations from the page image. The extracted annotations are then structured\n        using the `extract_structured_output` method and returned as a dictionary.\n\n        Args:\n            page_idx (int): The index of the page to annotate.\n            image_artifact_address (str): The address of the image artifact containing the\n                page images.\n\n        Returns:\n            dict: A dictionary containing the page index as the key and the extracted figure\n                annotations as the value.\n        \"\"\"\n\n        metadata = read_jsonl_file(os.path.join(self._artifact_dir, \"metadata.jsonl\"))\n        annotations = {}\n        for item in metadata:\n            if item[\"page_idx\"] == page_idx:\n                page_image_file = os.path.join(\n                    self._artifact_dir, f\"page{item['page_idx']}.png\"\n                )\n                figure_image_files = glob(\n                    os.path.join(self._artifact_dir, f\"page{item['page_idx']}_fig*.png\")\n                )\n                if len(figure_image_files) &gt; 0:\n                    page_image = cv2.imread(page_image_file)\n                    page_image = cv2.cvtColor(page_image, cv2.COLOR_BGR2RGB)\n                    page_image = Image.fromarray(page_image)\n                    figure_extracted_annotations = self.annotate_figures(\n                        page_image=page_image\n                    )\n                    figure_extracted_annotations = self.extract_structured_output(\n                        figure_extracted_annotations[\"annotations\"]\n                    ).model_dump()\n                    annotations[item[\"page_idx\"]] = figure_extracted_annotations[\n                        \"annotations\"\n                    ]\n                break\n        return annotations\n</code></pre>"},{"location":"rag/assistant/figure_annotation/#medrag_multi_modal.assistant.figure_annotation.FigureAnnotatorFromPageImage.predict","title":"<code>predict(page_idx)</code>","text":"<p>Predicts figure annotations for a specific page in a document.</p> <p>This function retrieves the artifact directory from the given image artifact address, reads the metadata from the 'metadata.jsonl' file, and iterates through the metadata to find the specified page index. If the page index matches, it reads the page image and associated figure images, and then uses the <code>annotate_figures</code> method to extract figure annotations from the page image. The extracted annotations are then structured using the <code>extract_structured_output</code> method and returned as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to annotate.</p> required <code>image_artifact_address</code> <code>str</code> <p>The address of the image artifact containing the page images.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[int, list[FigureAnnotations]]</code> <p>A dictionary containing the page index as the key and the extracted figure annotations as the value.</p> Source code in <code>medrag_multi_modal/assistant/figure_annotation.py</code> <pre><code>@weave.op()\ndef predict(self, page_idx: int) -&gt; dict[int, list[FigureAnnotations]]:\n    \"\"\"\n    Predicts figure annotations for a specific page in a document.\n\n    This function retrieves the artifact directory from the given image artifact address,\n    reads the metadata from the 'metadata.jsonl' file, and iterates through the metadata\n    to find the specified page index. If the page index matches, it reads the page image\n    and associated figure images, and then uses the `annotate_figures` method to extract\n    figure annotations from the page image. The extracted annotations are then structured\n    using the `extract_structured_output` method and returned as a dictionary.\n\n    Args:\n        page_idx (int): The index of the page to annotate.\n        image_artifact_address (str): The address of the image artifact containing the\n            page images.\n\n    Returns:\n        dict: A dictionary containing the page index as the key and the extracted figure\n            annotations as the value.\n    \"\"\"\n\n    metadata = read_jsonl_file(os.path.join(self._artifact_dir, \"metadata.jsonl\"))\n    annotations = {}\n    for item in metadata:\n        if item[\"page_idx\"] == page_idx:\n            page_image_file = os.path.join(\n                self._artifact_dir, f\"page{item['page_idx']}.png\"\n            )\n            figure_image_files = glob(\n                os.path.join(self._artifact_dir, f\"page{item['page_idx']}_fig*.png\")\n            )\n            if len(figure_image_files) &gt; 0:\n                page_image = cv2.imread(page_image_file)\n                page_image = cv2.cvtColor(page_image, cv2.COLOR_BGR2RGB)\n                page_image = Image.fromarray(page_image)\n                figure_extracted_annotations = self.annotate_figures(\n                    page_image=page_image\n                )\n                figure_extracted_annotations = self.extract_structured_output(\n                    figure_extracted_annotations[\"annotations\"]\n                ).model_dump()\n                annotations[item[\"page_idx\"]] = figure_extracted_annotations[\n                    \"annotations\"\n                ]\n            break\n    return annotations\n</code></pre>"},{"location":"rag/assistant/llm_client/","title":"LLM Client","text":""},{"location":"rag/assistant/llm_client/#medrag_multi_modal.assistant.llm_client.LLMClient","title":"<code>LLMClient</code>","text":"<p>               Bases: <code>Model</code></p> <p>LLMClient is a class that interfaces with different large language model (LLM) providers such as Google Gemini, Mistral, and OpenAI. It abstracts the complexity of interacting with these different APIs and provides a unified interface for making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to be used for predictions.</p> required <code>client_type</code> <code>Optional[ClientType]</code> <p>The type of client (e.g., GEMINI, MISTRAL, OPENAI). If not provided, it is inferred from the model_name.</p> <code>None</code> Source code in <code>medrag_multi_modal/assistant/llm_client.py</code> <pre><code>class LLMClient(weave.Model):\n    \"\"\"\n    LLMClient is a class that interfaces with different large language model (LLM) providers\n    such as Google Gemini, Mistral, and OpenAI. It abstracts the complexity of interacting with\n    these different APIs and provides a unified interface for making predictions.\n\n    Args:\n        model_name (str): The name of the model to be used for predictions.\n        client_type (Optional[ClientType]): The type of client (e.g., GEMINI, MISTRAL, OPENAI).\n            If not provided, it is inferred from the model_name.\n    \"\"\"\n\n    model_name: str\n    client_type: Optional[ClientType]\n\n    def __init__(self, model_name: str, client_type: Optional[ClientType] = None):\n        if client_type is None:\n            if model_name in GOOGLE_MODELS:\n                client_type = ClientType.GEMINI\n            elif model_name in MISTRAL_MODELS:\n                client_type = ClientType.MISTRAL\n            elif model_name in OPENAI_MODELS:\n                client_type = ClientType.OPENAI\n            else:\n                raise ValueError(f\"Invalid model name: {model_name}\")\n        super().__init__(model_name=model_name, client_type=client_type)\n\n    @weave.op()\n    def execute_gemini_sdk(\n        self,\n        user_prompt: Union[str, list[str]],\n        system_prompt: Optional[Union[str, list[str]]] = None,\n        schema: Optional[Any] = None,\n    ) -&gt; Union[str, Any]:\n        import google.generativeai as genai\n        from google.generativeai.types import HarmBlockThreshold, HarmCategory\n\n        system_prompt = (\n            [system_prompt] if isinstance(system_prompt, str) else system_prompt\n        )\n        user_prompt = [user_prompt] if isinstance(user_prompt, str) else user_prompt\n\n        genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n        model = genai.GenerativeModel(self.model_name, system_instruction=system_prompt)\n        generation_config = (\n            None\n            if schema is None\n            else genai.GenerationConfig(\n                response_mime_type=\"application/json\", response_schema=schema\n            )\n        )\n        response = model.generate_content(\n            user_prompt,\n            generation_config=generation_config,\n            # This is necessary in order to answer questions about anatomy, sexual diseases,\n            # medical devices, medicines, etc.\n            safety_settings={\n                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            },\n        )\n        return response.text if schema is None else json.loads(response.text)\n\n    @weave.op()\n    def execute_mistral_sdk(\n        self,\n        user_prompt: Union[str, list[str]],\n        system_prompt: Optional[Union[str, list[str]]] = None,\n        schema: Optional[Any] = None,\n    ) -&gt; Union[str, Any]:\n        from mistralai import Mistral\n\n        system_prompt = (\n            [system_prompt] if isinstance(system_prompt, str) else system_prompt\n        )\n        user_prompt = [user_prompt] if isinstance(user_prompt, str) else user_prompt\n        system_messages = [{\"type\": \"text\", \"text\": prompt} for prompt in system_prompt]\n        user_messages = []\n        for prompt in user_prompt:\n            if isinstance(prompt, Image.Image):\n                user_messages.append(\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": base64_encode_image(prompt, \"image/png\"),\n                    }\n                )\n            else:\n                user_messages.append({\"type\": \"text\", \"text\": prompt})\n        messages = [\n            {\"role\": \"system\", \"content\": system_messages},\n            {\"role\": \"user\", \"content\": user_messages},\n        ]\n\n        client = Mistral(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n        client = instructor.from_mistral(client) if schema is not None else client\n\n        if schema is None:\n            raise NotImplementedError(\n                \"Mistral does not support structured output using a schema\"\n            )\n        else:\n            response = client.chat.complete(model=self.model_name, messages=messages)\n            return response.choices[0].message.content\n\n    @weave.op()\n    def execute_openai_sdk(\n        self,\n        user_prompt: Union[str, list[str]],\n        system_prompt: Optional[Union[str, list[str]]] = None,\n        schema: Optional[Any] = None,\n    ) -&gt; Union[str, Any]:\n        from openai import OpenAI\n\n        system_prompt = (\n            [system_prompt] if isinstance(system_prompt, str) else system_prompt\n        )\n        user_prompt = [user_prompt] if isinstance(user_prompt, str) else user_prompt\n\n        system_messages = [\n            {\"role\": \"system\", \"content\": prompt} for prompt in system_prompt\n        ]\n        user_messages = []\n        for prompt in user_prompt:\n            if isinstance(prompt, Image.Image):\n                user_messages.append(\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": base64_encode_image(prompt, \"image/png\"),\n                        },\n                    },\n                )\n            else:\n                user_messages.append({\"type\": \"text\", \"text\": prompt})\n        messages = system_messages + [{\"role\": \"user\", \"content\": user_messages}]\n\n        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n        if schema is None:\n            completion = client.chat.completions.create(\n                model=self.model_name, messages=messages\n            )\n            return completion.choices[0].message.content\n\n        completion = weave.op()(client.beta.chat.completions.parse)(\n            model=self.model_name, messages=messages, response_format=schema\n        )\n        return completion.choices[0].message.parsed\n\n    @weave.op()\n    def predict(\n        self,\n        user_prompt: Union[str, list[str]],\n        system_prompt: Optional[Union[str, list[str]]] = None,\n        schema: Optional[Any] = None,\n    ) -&gt; Union[str, Any]:\n        \"\"\"\n        Predicts the response from a language model based on the provided prompts and schema.\n\n        This function determines the client type and calls the appropriate SDK execution function\n        to get the response from the language model. It supports multiple client types including\n        GEMINI, MISTRAL, and OPENAI. Depending on the client type, it calls the corresponding\n        execution function with the provided user and system prompts, and an optional schema.\n\n        Args:\n            user_prompt (Union[str, list[str]]): The user prompt(s) to be sent to the language model.\n            system_prompt (Optional[Union[str, list[str]]]): The system prompt(s) to be sent to the language model.\n            schema (Optional[Any]): The schema to be used for parsing the response, if applicable.\n\n        Returns:\n            Union[str, Any]: The response from the language model, which could be a string or any other type\n            depending on the schema provided.\n\n        Raises:\n            ValueError: If the client type is invalid.\n        \"\"\"\n        if self.client_type == ClientType.GEMINI:\n            return self.execute_gemini_sdk(user_prompt, system_prompt, schema)\n        elif self.client_type == ClientType.MISTRAL:\n            return self.execute_mistral_sdk(user_prompt, system_prompt, schema)\n        elif self.client_type == ClientType.OPENAI:\n            return self.execute_openai_sdk(user_prompt, system_prompt, schema)\n        else:\n            raise ValueError(f\"Invalid client type: {self.client_type}\")\n</code></pre>"},{"location":"rag/assistant/llm_client/#medrag_multi_modal.assistant.llm_client.LLMClient.predict","title":"<code>predict(user_prompt, system_prompt=None, schema=None)</code>","text":"<p>Predicts the response from a language model based on the provided prompts and schema.</p> <p>This function determines the client type and calls the appropriate SDK execution function to get the response from the language model. It supports multiple client types including GEMINI, MISTRAL, and OPENAI. Depending on the client type, it calls the corresponding execution function with the provided user and system prompts, and an optional schema.</p> <p>Parameters:</p> Name Type Description Default <code>user_prompt</code> <code>Union[str, list[str]]</code> <p>The user prompt(s) to be sent to the language model.</p> required <code>system_prompt</code> <code>Optional[Union[str, list[str]]]</code> <p>The system prompt(s) to be sent to the language model.</p> <code>None</code> <code>schema</code> <code>Optional[Any]</code> <p>The schema to be used for parsing the response, if applicable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>Union[str, Any]: The response from the language model, which could be a string or any other type</p> <code>Union[str, Any]</code> <p>depending on the schema provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the client type is invalid.</p> Source code in <code>medrag_multi_modal/assistant/llm_client.py</code> <pre><code>@weave.op()\ndef predict(\n    self,\n    user_prompt: Union[str, list[str]],\n    system_prompt: Optional[Union[str, list[str]]] = None,\n    schema: Optional[Any] = None,\n) -&gt; Union[str, Any]:\n    \"\"\"\n    Predicts the response from a language model based on the provided prompts and schema.\n\n    This function determines the client type and calls the appropriate SDK execution function\n    to get the response from the language model. It supports multiple client types including\n    GEMINI, MISTRAL, and OPENAI. Depending on the client type, it calls the corresponding\n    execution function with the provided user and system prompts, and an optional schema.\n\n    Args:\n        user_prompt (Union[str, list[str]]): The user prompt(s) to be sent to the language model.\n        system_prompt (Optional[Union[str, list[str]]]): The system prompt(s) to be sent to the language model.\n        schema (Optional[Any]): The schema to be used for parsing the response, if applicable.\n\n    Returns:\n        Union[str, Any]: The response from the language model, which could be a string or any other type\n        depending on the schema provided.\n\n    Raises:\n        ValueError: If the client type is invalid.\n    \"\"\"\n    if self.client_type == ClientType.GEMINI:\n        return self.execute_gemini_sdk(user_prompt, system_prompt, schema)\n    elif self.client_type == ClientType.MISTRAL:\n        return self.execute_mistral_sdk(user_prompt, system_prompt, schema)\n    elif self.client_type == ClientType.OPENAI:\n        return self.execute_openai_sdk(user_prompt, system_prompt, schema)\n    else:\n        raise ValueError(f\"Invalid client type: {self.client_type}\")\n</code></pre>"},{"location":"rag/assistant/medqa_assistant/","title":"MedQA Assistant","text":""},{"location":"rag/assistant/medqa_assistant/#medrag_multi_modal.assistant.medqa_assistant.MedQAAssistant","title":"<code>MedQAAssistant</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>MedQAAssistant</code> is a class designed to assist with medical queries by leveraging a language model client, a retriever model, and a figure annotator.</p> <p>Usage Example</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.assistant import (\n    FigureAnnotatorFromPageImage,\n    LLMClient,\n    MedQAAssistant,\n)\nfrom medrag_multi_modal.retrieval import MedCPTRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n\nllm_client = LLMClient(model_name=\"gemini-1.5-flash\")\n\nretriever=MedCPTRetriever.from_wandb_artifact(\n    chunk_dataset_name=\"grays-anatomy-chunks:v0\",\n    index_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-medcpt:v0\",\n)\n\nfigure_annotator=FigureAnnotatorFromPageImage(\n    figure_extraction_llm_client=LLMClient(model_name=\"pixtral-12b-2409\"),\n    structured_output_llm_client=LLMClient(model_name=\"gpt-4o\"),\n    image_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-images-marker:v6\",\n)\nmedqa_assistant = MedQAAssistant(\n    llm_client=llm_client, retriever=retriever, figure_annotator=figure_annotator\n)\nmedqa_assistant.predict(query=\"What is ribosome?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>LLMClient</code> <p>The language model client used to generate responses.</p> required <code>retriever</code> <code>Model</code> <p>The model used to retrieve relevant chunks of text from a medical document.</p> required <code>figure_annotator</code> <code>FigureAnnotatorFromPageImage</code> <p>The annotator used to extract figure descriptions from pages.</p> required <code>top_k_chunks_for_query</code> <code>int</code> <p>The number of top chunks to retrieve based on similarity metric for the query.</p> required <code>top_k_chunks_for_options</code> <code>int</code> <p>The number of top chunks to retrieve based on similarity metric for the options.</p> required <code>retrieval_similarity_metric</code> <code>SimilarityMetric</code> <p>The metric used to measure similarity for retrieval.</p> required Source code in <code>medrag_multi_modal/assistant/medqa_assistant.py</code> <pre><code>class MedQAAssistant(weave.Model):\n    \"\"\"\n    `MedQAAssistant` is a class designed to assist with medical queries by leveraging a\n    language model client, a retriever model, and a figure annotator.\n\n    !!! example \"Usage Example\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.assistant import (\n            FigureAnnotatorFromPageImage,\n            LLMClient,\n            MedQAAssistant,\n        )\n        from medrag_multi_modal.retrieval import MedCPTRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n\n        llm_client = LLMClient(model_name=\"gemini-1.5-flash\")\n\n        retriever=MedCPTRetriever.from_wandb_artifact(\n            chunk_dataset_name=\"grays-anatomy-chunks:v0\",\n            index_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-medcpt:v0\",\n        )\n\n        figure_annotator=FigureAnnotatorFromPageImage(\n            figure_extraction_llm_client=LLMClient(model_name=\"pixtral-12b-2409\"),\n            structured_output_llm_client=LLMClient(model_name=\"gpt-4o\"),\n            image_artifact_address=\"ml-colabs/medrag-multi-modal/grays-anatomy-images-marker:v6\",\n        )\n        medqa_assistant = MedQAAssistant(\n            llm_client=llm_client, retriever=retriever, figure_annotator=figure_annotator\n        )\n        medqa_assistant.predict(query=\"What is ribosome?\")\n        ```\n\n    Args:\n        llm_client (LLMClient): The language model client used to generate responses.\n        retriever (weave.Model): The model used to retrieve relevant chunks of text from a medical document.\n        figure_annotator (FigureAnnotatorFromPageImage): The annotator used to extract figure descriptions from pages.\n        top_k_chunks_for_query (int): The number of top chunks to retrieve based on similarity metric for the query.\n        top_k_chunks_for_options (int): The number of top chunks to retrieve based on similarity metric for the options.\n        retrieval_similarity_metric (SimilarityMetric): The metric used to measure similarity for retrieval.\n    \"\"\"\n\n    llm_client: LLMClient\n    retriever: weave.Model\n    figure_annotator: Optional[FigureAnnotatorFromPageImage] = None\n    top_k_chunks_for_query: int = 2\n    top_k_chunks_for_options: int = 2\n    rely_only_on_context: bool = True\n    retrieval_similarity_metric: SimilarityMetric = SimilarityMetric.COSINE\n\n    @weave.op()\n    def retrieve_chunks_for_query(self, query: str) -&gt; list[dict]:\n        retriever_kwargs = {\"top_k\": self.top_k_chunks_for_query}\n        if not isinstance(self.retriever, BM25sRetriever):\n            retriever_kwargs[\"metric\"] = self.retrieval_similarity_metric\n        return self.retriever.predict(query, **retriever_kwargs)\n\n    @weave.op()\n    def retrieve_chunks_for_options(self, options: list[str]) -&gt; list[dict]:\n        retriever_kwargs = {\"top_k\": self.top_k_chunks_for_options}\n        if not isinstance(self.retriever, BM25sRetriever):\n            retriever_kwargs[\"metric\"] = self.retrieval_similarity_metric\n        retrieved_chunks = []\n        for option in options:\n            retrieved_chunks += self.retriever.predict(query=option, **retriever_kwargs)\n        return retrieved_chunks\n\n    @weave.op()\n    def predict(self, query: str, options: Optional[list[str]] = None) -&gt; MedQAResponse:\n        \"\"\"\n        Generates a response to a medical query by retrieving relevant text chunks and figure descriptions\n        from a medical document and using a language model to generate the final response.\n\n        This function performs the following steps:\n        1. Retrieves relevant text chunks from the medical document based on the query and any provided options\n           using the retriever model.\n        2. Extracts the text and page indices from the retrieved chunks.\n        3. Retrieves figure descriptions from the pages identified in the previous step using the figure annotator.\n        4. Constructs a system prompt and user prompt combining the query, options (if provided), retrieved text chunks,\n           and figure descriptions.\n        5. Uses the language model client to generate a response based on the constructed prompts, either choosing\n           from provided options or generating a free-form response.\n        6. Returns the generated response, which includes the answer and explanation if options were provided.\n\n        The function can operate in two modes:\n        - Multiple choice: When options are provided, it selects the best answer from the options and explains the choice\n        - Free response: When no options are provided, it generates a comprehensive response based on the context\n\n        Args:\n            query (str): The medical query to be answered.\n            options (Optional[list[str]]): The list of options to choose from.\n            rely_only_on_context (bool): Whether to rely only on the context provided or not during response generation.\n\n        Returns:\n            MedQAResponse: The generated response to the query, including source information.\n        \"\"\"\n        retrieved_chunks = self.retrieve_chunks_for_query(query)\n        options = options or []\n        retrieved_chunks += self.retrieve_chunks_for_options(options)\n\n        retrieved_chunk_texts = []\n        page_indices = set()\n        for chunk in retrieved_chunks:\n            retrieved_chunk_texts.append(chunk[\"text\"])\n            page_indices.add(int(chunk[\"page_idx\"]))\n\n        figure_descriptions = []\n        if self.figure_annotator is not None:\n            for page_idx in page_indices:\n                figure_annotations = self.figure_annotator.predict(page_idx=page_idx)[\n                    page_idx\n                ]\n                figure_descriptions += [\n                    item[\"figure_description\"] for item in figure_annotations\n                ]\n\n        system_prompt = \"\"\"You are an expert in medical science. You are given a question\nand a list of excerpts from various medical documents.\n        \"\"\"\n        query = f\"\"\"# Question\n{query}\n        \"\"\"\n\n        if len(options) &gt; 0:\n            system_prompt += \"\"\"\\nYou are also given a list of options to choose your answer from.\nYou are supposed to choose the best possible option based on the context provided. You should also\nexplain your answer to justify why you chose that option.\n\"\"\"\n            query += \"## Options\\n\"\n            for option in options:\n                query += f\"- {option}\\n\"\n        else:\n            system_prompt += \"\\nYou are supposed to answer the question based on the context provided.\"\n\n        if self.rely_only_on_context:\n            system_prompt += \"\"\"\\n\\nYou are only allowed to use the context provided to answer the question.\nYou are not allowed to use any external knowledge to answer the question.\n\"\"\"\n\n        response = self.llm_client.predict(\n            system_prompt=system_prompt,\n            user_prompt=[query, *retrieved_chunk_texts, *figure_descriptions],\n            schema=MedQAMCQResponse if len(options) &gt; 0 else None,\n        )\n\n        # TODO: Add figure citations\n        # TODO: Add source document name from retrieved chunks as citations\n        citations = []\n        for page_idx in page_indices:\n            citations.append(\n                MedQACitation(page_number=page_idx + 1, document_name=\"Gray's Anatomy\")\n            )\n\n        return MedQAResponse(response=response, citations=citations)\n</code></pre>"},{"location":"rag/assistant/medqa_assistant/#medrag_multi_modal.assistant.medqa_assistant.MedQAAssistant.predict","title":"<code>predict(query, options=None)</code>","text":"<p>Generates a response to a medical query by retrieving relevant text chunks and figure descriptions from a medical document and using a language model to generate the final response.</p> <p>This function performs the following steps: 1. Retrieves relevant text chunks from the medical document based on the query and any provided options    using the retriever model. 2. Extracts the text and page indices from the retrieved chunks. 3. Retrieves figure descriptions from the pages identified in the previous step using the figure annotator. 4. Constructs a system prompt and user prompt combining the query, options (if provided), retrieved text chunks,    and figure descriptions. 5. Uses the language model client to generate a response based on the constructed prompts, either choosing    from provided options or generating a free-form response. 6. Returns the generated response, which includes the answer and explanation if options were provided.</p> <p>The function can operate in two modes: - Multiple choice: When options are provided, it selects the best answer from the options and explains the choice - Free response: When no options are provided, it generates a comprehensive response based on the context</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The medical query to be answered.</p> required <code>options</code> <code>Optional[list[str]]</code> <p>The list of options to choose from.</p> <code>None</code> <code>rely_only_on_context</code> <code>bool</code> <p>Whether to rely only on the context provided or not during response generation.</p> required <p>Returns:</p> Name Type Description <code>MedQAResponse</code> <code>MedQAResponse</code> <p>The generated response to the query, including source information.</p> Source code in <code>medrag_multi_modal/assistant/medqa_assistant.py</code> <pre><code>    @weave.op()\n    def predict(self, query: str, options: Optional[list[str]] = None) -&gt; MedQAResponse:\n        \"\"\"\n        Generates a response to a medical query by retrieving relevant text chunks and figure descriptions\n        from a medical document and using a language model to generate the final response.\n\n        This function performs the following steps:\n        1. Retrieves relevant text chunks from the medical document based on the query and any provided options\n           using the retriever model.\n        2. Extracts the text and page indices from the retrieved chunks.\n        3. Retrieves figure descriptions from the pages identified in the previous step using the figure annotator.\n        4. Constructs a system prompt and user prompt combining the query, options (if provided), retrieved text chunks,\n           and figure descriptions.\n        5. Uses the language model client to generate a response based on the constructed prompts, either choosing\n           from provided options or generating a free-form response.\n        6. Returns the generated response, which includes the answer and explanation if options were provided.\n\n        The function can operate in two modes:\n        - Multiple choice: When options are provided, it selects the best answer from the options and explains the choice\n        - Free response: When no options are provided, it generates a comprehensive response based on the context\n\n        Args:\n            query (str): The medical query to be answered.\n            options (Optional[list[str]]): The list of options to choose from.\n            rely_only_on_context (bool): Whether to rely only on the context provided or not during response generation.\n\n        Returns:\n            MedQAResponse: The generated response to the query, including source information.\n        \"\"\"\n        retrieved_chunks = self.retrieve_chunks_for_query(query)\n        options = options or []\n        retrieved_chunks += self.retrieve_chunks_for_options(options)\n\n        retrieved_chunk_texts = []\n        page_indices = set()\n        for chunk in retrieved_chunks:\n            retrieved_chunk_texts.append(chunk[\"text\"])\n            page_indices.add(int(chunk[\"page_idx\"]))\n\n        figure_descriptions = []\n        if self.figure_annotator is not None:\n            for page_idx in page_indices:\n                figure_annotations = self.figure_annotator.predict(page_idx=page_idx)[\n                    page_idx\n                ]\n                figure_descriptions += [\n                    item[\"figure_description\"] for item in figure_annotations\n                ]\n\n        system_prompt = \"\"\"You are an expert in medical science. You are given a question\nand a list of excerpts from various medical documents.\n        \"\"\"\n        query = f\"\"\"# Question\n{query}\n        \"\"\"\n\n        if len(options) &gt; 0:\n            system_prompt += \"\"\"\\nYou are also given a list of options to choose your answer from.\nYou are supposed to choose the best possible option based on the context provided. You should also\nexplain your answer to justify why you chose that option.\n\"\"\"\n            query += \"## Options\\n\"\n            for option in options:\n                query += f\"- {option}\\n\"\n        else:\n            system_prompt += \"\\nYou are supposed to answer the question based on the context provided.\"\n\n        if self.rely_only_on_context:\n            system_prompt += \"\"\"\\n\\nYou are only allowed to use the context provided to answer the question.\nYou are not allowed to use any external knowledge to answer the question.\n\"\"\"\n\n        response = self.llm_client.predict(\n            system_prompt=system_prompt,\n            user_prompt=[query, *retrieved_chunk_texts, *figure_descriptions],\n            schema=MedQAMCQResponse if len(options) &gt; 0 else None,\n        )\n\n        # TODO: Add figure citations\n        # TODO: Add source document name from retrieved chunks as citations\n        citations = []\n        for page_idx in page_indices:\n            citations.append(\n                MedQACitation(page_number=page_idx + 1, document_name=\"Gray's Anatomy\")\n            )\n\n        return MedQAResponse(response=response, citations=citations)\n</code></pre>"},{"location":"rag/document_loader/image_loader/base_img_loader/","title":"Base","text":""},{"location":"rag/document_loader/image_loader/base_img_loader/#load-images-from-pdf-files","title":"Load images from PDF files","text":""},{"location":"rag/document_loader/image_loader/base_img_loader/#medrag_multi_modal.document_loader.image_loader.base_img_loader.BaseImageLoader","title":"<code>BaseImageLoader</code>","text":"<p>               Bases: <code>BaseTextLoader</code></p> Source code in <code>medrag_multi_modal/document_loader/image_loader/base_img_loader.py</code> <pre><code>class BaseImageLoader(BaseTextLoader):\n    def __init__(self, url: str, document_name: str, document_file_path: str):\n        super().__init__(url, document_name, document_file_path)\n\n    @abstractmethod\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Abstract method to process a single page of the PDF and extract the image data.\n\n        Overwrite this method in the subclass to provide the actual implementation and\n        processing logic for each page of the PDF using various PDF processing libraries.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted images.\n            **kwargs: Additional keyword arguments that may be used by underlying libraries.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        pass\n\n    def save_as_dataset(\n        self,\n        start_page: int,\n        end_page: int,\n        image_save_dir: str,\n        dataset_repo_id: Optional[str] = None,\n        overwrite_dataset: bool = False,\n    ):\n        features = Features(\n            {\n                \"page_image\": Image(decode=True),\n                \"page_figure_images\": Sequence(Image(decode=True)),\n                \"document_name\": Value(dtype=\"string\"),\n                \"page_idx\": Value(dtype=\"int32\"),\n            }\n        )\n\n        all_examples = []\n        for page_idx in range(start_page, end_page):\n            page_image_file_paths = glob(\n                os.path.join(image_save_dir, f\"page{page_idx}*.png\")\n            )\n            if len(page_image_file_paths) &gt; 0:\n                page_image_path = page_image_file_paths[0]\n                figure_image_paths = [\n                    image_file_path\n                    for image_file_path in glob(\n                        os.path.join(image_save_dir, f\"page{page_idx}*_fig*.png\")\n                    )\n                ]\n\n                example = {\n                    \"page_image\": page_image_path,\n                    \"page_figure_images\": figure_image_paths,\n                    \"document_name\": self.document_name,\n                    \"page_idx\": page_idx,\n                }\n                all_examples.append(example)\n\n        dataset = Dataset.from_list(all_examples, features=features)\n\n        if dataset_repo_id:\n            if huggingface_hub.repo_exists(dataset_repo_id, repo_type=\"dataset\"):\n                if not overwrite_dataset:\n                    dataset = concatenate_datasets(\n                        [dataset, load_dataset(dataset_repo_id)[\"corpus\"]]\n                    )\n\n            dataset.push_to_hub(dataset_repo_id, split=\"corpus\")\n\n        return dataset\n\n    def cleanup_image_dir(self, image_save_dir: str = \"./images\"):\n        for file in os.listdir(image_save_dir):\n            file_path = os.path.join(image_save_dir, file)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n\n    async def load_data(\n        self,\n        start_page: Optional[int] = None,\n        end_page: Optional[int] = None,\n        dataset_repo_id: Optional[str] = None,\n        overwrite_dataset: bool = False,\n        image_save_dir: str = \"./images\",\n        exclude_file_extensions: list[str] = [],\n        **kwargs,\n    ) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Asynchronously loads images from a PDF file specified by a URL or local file path.\n        The overrided processing abstract method then processes the images,\n        and optionally publishes it to a WandB artifact.\n\n        This function downloads a PDF from a given URL if it does not already exist locally,\n        reads the specified range of pages, scans each page's content to extract images, and\n        returns a list of Page objects containing the images and metadata.\n\n        It uses `PyPDF2` to calculate the number of pages in the PDF and the\n        overriden `extract_page_data` method provides the actual implementation to process\n        each page, extract the image content from the PDF, and convert it to png format.\n        It processes pages concurrently using `asyncio` for efficiency.\n\n        If a wandb_artifact_name is provided, the processed pages are published to a WandB artifact.\n\n        Args:\n            start_page (Optional[int]): The starting page index (0-based) to process.\n            end_page (Optional[int]): The ending page index (0-based) to process.\n            dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish the pages to, if provided.\n            overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists. Defaults to False.\n            image_save_dir (str): The directory to save the extracted images.\n            exclude_file_extensions (list[str]): A list of file extensions to exclude from the image_save_dir.\n            **kwargs: Additional keyword arguments that will be passed to extract_page_data method and the underlying library.\n\n        Returns:\n            Dataset: A HuggingFace dataset containing the processed pages.\n\n        Raises:\n            ValueError: If the specified start_page or end_page is out of bounds of the document's page count.\n        \"\"\"\n        os.makedirs(image_save_dir, exist_ok=True)\n        start_page, end_page = self.get_page_indices(start_page, end_page)\n        pages = []\n        processed_pages_counter: int = 1\n        total_pages = end_page - start_page\n\n        async def process_page(page_idx):\n            nonlocal processed_pages_counter\n            page_data = await self.extract_page_data(page_idx, image_save_dir, **kwargs)\n            pages.append(page_data)\n            rich.print(\n                f\"Processed page idx: {page_idx}, progress: {processed_pages_counter}/{total_pages}\"\n            )\n            processed_pages_counter += 1\n\n        tasks = [process_page(page_idx) for page_idx in range(start_page, end_page)]\n        for task in asyncio.as_completed(tasks):\n            await task\n\n        with jsonlines.open(\n            os.path.join(image_save_dir, \"metadata.jsonl\"), mode=\"w\"\n        ) as writer:\n            writer.write(pages)\n\n        for file in os.listdir(image_save_dir):\n            if file.endswith(tuple(exclude_file_extensions)):\n                os.remove(os.path.join(image_save_dir, file))\n\n        dataset = self.save_as_dataset(\n            start_page, end_page, image_save_dir, dataset_repo_id, overwrite_dataset\n        )\n\n        return dataset\n</code></pre>"},{"location":"rag/document_loader/image_loader/base_img_loader/#medrag_multi_modal.document_loader.image_loader.base_img_loader.BaseImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to process a single page of the PDF and extract the image data.</p> <p>Overwrite this method in the subclass to provide the actual implementation and processing logic for each page of the PDF using various PDF processing libraries.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted images.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by underlying libraries.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/image_loader/base_img_loader.py</code> <pre><code>@abstractmethod\nasync def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Abstract method to process a single page of the PDF and extract the image data.\n\n    Overwrite this method in the subclass to provide the actual implementation and\n    processing logic for each page of the PDF using various PDF processing libraries.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted images.\n        **kwargs: Additional keyword arguments that may be used by underlying libraries.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"rag/document_loader/image_loader/base_img_loader/#medrag_multi_modal.document_loader.image_loader.base_img_loader.BaseImageLoader.load_data","title":"<code>load_data(start_page=None, end_page=None, dataset_repo_id=None, overwrite_dataset=False, image_save_dir='./images', exclude_file_extensions=[], **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously loads images from a PDF file specified by a URL or local file path. The overrided processing abstract method then processes the images, and optionally publishes it to a WandB artifact.</p> <p>This function downloads a PDF from a given URL if it does not already exist locally, reads the specified range of pages, scans each page's content to extract images, and returns a list of Page objects containing the images and metadata.</p> <p>It uses <code>PyPDF2</code> to calculate the number of pages in the PDF and the overriden <code>extract_page_data</code> method provides the actual implementation to process each page, extract the image content from the PDF, and convert it to png format. It processes pages concurrently using <code>asyncio</code> for efficiency.</p> <p>If a wandb_artifact_name is provided, the processed pages are published to a WandB artifact.</p> <p>Parameters:</p> Name Type Description Default <code>start_page</code> <code>Optional[int]</code> <p>The starting page index (0-based) to process.</p> <code>None</code> <code>end_page</code> <code>Optional[int]</code> <p>The ending page index (0-based) to process.</p> <code>None</code> <code>dataset_repo_id</code> <code>Optional[str]</code> <p>The repository ID of the HuggingFace dataset to publish the pages to, if provided.</p> <code>None</code> <code>overwrite_dataset</code> <code>bool</code> <p>Whether to overwrite the existing dataset if it exists. Defaults to False.</p> <code>False</code> <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted images.</p> <code>'./images'</code> <code>exclude_file_extensions</code> <code>list[str]</code> <p>A list of file extensions to exclude from the image_save_dir.</p> <code>[]</code> <code>**kwargs</code> <p>Additional keyword arguments that will be passed to extract_page_data method and the underlying library.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>List[Dict[str, str]]</code> <p>A HuggingFace dataset containing the processed pages.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified start_page or end_page is out of bounds of the document's page count.</p> Source code in <code>medrag_multi_modal/document_loader/image_loader/base_img_loader.py</code> <pre><code>async def load_data(\n    self,\n    start_page: Optional[int] = None,\n    end_page: Optional[int] = None,\n    dataset_repo_id: Optional[str] = None,\n    overwrite_dataset: bool = False,\n    image_save_dir: str = \"./images\",\n    exclude_file_extensions: list[str] = [],\n    **kwargs,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Asynchronously loads images from a PDF file specified by a URL or local file path.\n    The overrided processing abstract method then processes the images,\n    and optionally publishes it to a WandB artifact.\n\n    This function downloads a PDF from a given URL if it does not already exist locally,\n    reads the specified range of pages, scans each page's content to extract images, and\n    returns a list of Page objects containing the images and metadata.\n\n    It uses `PyPDF2` to calculate the number of pages in the PDF and the\n    overriden `extract_page_data` method provides the actual implementation to process\n    each page, extract the image content from the PDF, and convert it to png format.\n    It processes pages concurrently using `asyncio` for efficiency.\n\n    If a wandb_artifact_name is provided, the processed pages are published to a WandB artifact.\n\n    Args:\n        start_page (Optional[int]): The starting page index (0-based) to process.\n        end_page (Optional[int]): The ending page index (0-based) to process.\n        dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish the pages to, if provided.\n        overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists. Defaults to False.\n        image_save_dir (str): The directory to save the extracted images.\n        exclude_file_extensions (list[str]): A list of file extensions to exclude from the image_save_dir.\n        **kwargs: Additional keyword arguments that will be passed to extract_page_data method and the underlying library.\n\n    Returns:\n        Dataset: A HuggingFace dataset containing the processed pages.\n\n    Raises:\n        ValueError: If the specified start_page or end_page is out of bounds of the document's page count.\n    \"\"\"\n    os.makedirs(image_save_dir, exist_ok=True)\n    start_page, end_page = self.get_page_indices(start_page, end_page)\n    pages = []\n    processed_pages_counter: int = 1\n    total_pages = end_page - start_page\n\n    async def process_page(page_idx):\n        nonlocal processed_pages_counter\n        page_data = await self.extract_page_data(page_idx, image_save_dir, **kwargs)\n        pages.append(page_data)\n        rich.print(\n            f\"Processed page idx: {page_idx}, progress: {processed_pages_counter}/{total_pages}\"\n        )\n        processed_pages_counter += 1\n\n    tasks = [process_page(page_idx) for page_idx in range(start_page, end_page)]\n    for task in asyncio.as_completed(tasks):\n        await task\n\n    with jsonlines.open(\n        os.path.join(image_save_dir, \"metadata.jsonl\"), mode=\"w\"\n    ) as writer:\n        writer.write(pages)\n\n    for file in os.listdir(image_save_dir):\n        if file.endswith(tuple(exclude_file_extensions)):\n            os.remove(os.path.join(image_save_dir, file))\n\n    dataset = self.save_as_dataset(\n        start_page, end_page, image_save_dir, dataset_repo_id, overwrite_dataset\n    )\n\n    return dataset\n</code></pre>"},{"location":"rag/document_loader/image_loader/fitzpil_img_loader/","title":"Load images from PDF files (using Fitz &amp; PIL)","text":"Note <p>Underlying Library: <code>fitz</code> &amp; <code>pillow</code></p> <p>Extract images from PDF files using <code>fitz</code> and <code>pillow</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.image_loader import FitzPILImageLoader\n</code></pre></p> <p>For more details, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>Docs</li> <li>GitHub</li> <li>PyPI</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/image_loader/fitzpil_img_loader/#medrag_multi_modal.document_loader.image_loader.fitzpil_img_loader.FitzPILImageLoader","title":"<code>FitzPILImageLoader</code>","text":"<p>               Bases: <code>BaseImageLoader</code></p> <p><code>FitzPILImageLoader</code> is a class that extends the <code>BaseImageLoader</code> class to handle the extraction and loading of pages from a PDF file as images using the fitz and PIL libraries.</p> <p>This class provides functionality to extract images from a PDF file using fitz and PIL libraries, and optionally publish these images to a WandB artifact.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader.image_loader import FitzPILImageLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = FitzPILImageLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF document.</p> required <code>document_name</code> <code>str</code> <p>The name of the document.</p> required <code>document_file_path</code> <code>str</code> <p>The path to the PDF file.</p> required Source code in <code>medrag_multi_modal/document_loader/image_loader/fitzpil_img_loader.py</code> <pre><code>class FitzPILImageLoader(BaseImageLoader):\n    \"\"\"\n    `FitzPILImageLoader` is a class that extends the `BaseImageLoader` class to handle the extraction and\n    loading of pages from a PDF file as images using the fitz and PIL libraries.\n\n    This class provides functionality to extract images from a PDF file using fitz and PIL libraries,\n    and optionally publish these images to a WandB artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader.image_loader import FitzPILImageLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = FitzPILImageLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n        ```\n\n    Args:\n        url (str): The URL of the PDF document.\n        document_name (str): The name of the document.\n        document_file_path (str): The path to the PDF file.\n    \"\"\"\n\n    def __init__(self, url: str, document_name: str, document_file_path: str):\n        super().__init__(url, document_name, document_file_path)\n\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extracts a single page from the PDF as an image using fitz and PIL libraries.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted image.\n            **kwargs: Additional keyword arguments that may be used by fitz and PIL.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"image_file_paths\": (list) the local file paths where the images are stored.\n        \"\"\"\n        image_file_paths = []\n\n        pdf_document = fitz.open(self.document_file_path)\n        page = pdf_document.load_page(page_idx)\n\n        images = page.get_images(full=True)\n        for img_idx, image in enumerate(images):\n            xref = image[0]\n            base_image = pdf_document.extract_image(xref)\n            image_bytes = base_image[\"image\"]\n            image_ext = base_image[\"ext\"]\n\n            try:\n                img = Image.open(io.BytesIO(image_bytes))\n\n                if img.mode in [\"L\"]:\n                    # images in greyscale looks inverted, need to test on other PDFs\n                    img = ImageOps.invert(img)\n\n                if img.mode == \"CMYK\":\n                    img = img.convert(\"RGB\")\n\n                if image_ext not in [\"png\", \"jpg\", \"jpeg\"]:\n                    image_ext = \"png\"\n                    image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n                    image_file_path = os.path.join(image_save_dir, image_file_name)\n\n                    img.save(image_file_path, format=\"PNG\")\n                else:\n                    image_file_name = f\"page{page_idx}_fig{img_idx}.{image_ext}\"\n                    image_file_path = os.path.join(image_save_dir, image_file_name)\n\n                    with open(image_file_path, \"wb\") as image_file:\n                        image_file.write(image_bytes)\n\n                image_file_paths.append(image_file_path)\n\n            except (UnidentifiedImageError, OSError) as e:\n                print(\n                    f\"Skipping image at page {page_idx}, fig {img_idx} due to an error: {e}\"\n                )\n                continue\n\n        pdf_document.close()\n\n        page_image = convert_from_path(\n            self.document_file_path,\n            first_page=page_idx + 1,\n            last_page=page_idx + 1,\n            **kwargs,\n        )[0]\n        page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n        return {\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n            \"image_file_paths\": image_file_paths,\n        }\n</code></pre>"},{"location":"rag/document_loader/image_loader/fitzpil_img_loader/#medrag_multi_modal.document_loader.image_loader.fitzpil_img_loader.FitzPILImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>async</code>","text":"<p>Extracts a single page from the PDF as an image using fitz and PIL libraries.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted image.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by fitz and PIL.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the processed page data.</p> <code>Dict[str, Any]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, Any]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"image_file_paths\": (list) the local file paths where the images are stored.</li> </ul> Source code in <code>medrag_multi_modal/document_loader/image_loader/fitzpil_img_loader.py</code> <pre><code>async def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extracts a single page from the PDF as an image using fitz and PIL libraries.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted image.\n        **kwargs: Additional keyword arguments that may be used by fitz and PIL.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"image_file_paths\": (list) the local file paths where the images are stored.\n    \"\"\"\n    image_file_paths = []\n\n    pdf_document = fitz.open(self.document_file_path)\n    page = pdf_document.load_page(page_idx)\n\n    images = page.get_images(full=True)\n    for img_idx, image in enumerate(images):\n        xref = image[0]\n        base_image = pdf_document.extract_image(xref)\n        image_bytes = base_image[\"image\"]\n        image_ext = base_image[\"ext\"]\n\n        try:\n            img = Image.open(io.BytesIO(image_bytes))\n\n            if img.mode in [\"L\"]:\n                # images in greyscale looks inverted, need to test on other PDFs\n                img = ImageOps.invert(img)\n\n            if img.mode == \"CMYK\":\n                img = img.convert(\"RGB\")\n\n            if image_ext not in [\"png\", \"jpg\", \"jpeg\"]:\n                image_ext = \"png\"\n                image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n                image_file_path = os.path.join(image_save_dir, image_file_name)\n\n                img.save(image_file_path, format=\"PNG\")\n            else:\n                image_file_name = f\"page{page_idx}_fig{img_idx}.{image_ext}\"\n                image_file_path = os.path.join(image_save_dir, image_file_name)\n\n                with open(image_file_path, \"wb\") as image_file:\n                    image_file.write(image_bytes)\n\n            image_file_paths.append(image_file_path)\n\n        except (UnidentifiedImageError, OSError) as e:\n            print(\n                f\"Skipping image at page {page_idx}, fig {img_idx} due to an error: {e}\"\n            )\n            continue\n\n    pdf_document.close()\n\n    page_image = convert_from_path(\n        self.document_file_path,\n        first_page=page_idx + 1,\n        last_page=page_idx + 1,\n        **kwargs,\n    )[0]\n    page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n    return {\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n        \"image_file_paths\": image_file_paths,\n    }\n</code></pre>"},{"location":"rag/document_loader/image_loader/marker_img_loader/","title":"Load images from PDF files (using Marker)","text":"Note <p>Underlying Library: <code>marker-pdf</code></p> <p>Extract images from PDF files using <code>marker-pdf</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.image_loader import MarkerImageLoader\n</code></pre></p> <p>For details, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>DataLab</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/image_loader/marker_img_loader/#medrag_multi_modal.document_loader.image_loader.marker_img_loader.MarkerImageLoader","title":"<code>MarkerImageLoader</code>","text":"<p>               Bases: <code>BaseImageLoader</code></p> <p><code>MarkerImageLoader</code> is a class that extends the <code>BaseImageLoader</code> class to handle the extraction and loading of pages from a PDF file as images using the marker library.</p> <p>This class provides functionality to extract images from a PDF file using marker library, and optionally publish these images to a WandB artifact.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader.image_loader import MarkerImageLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = MarkerImageLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF document.</p> required <code>document_name</code> <code>str</code> <p>The name of the document.</p> required <code>document_file_path</code> <code>str</code> <p>The path to the PDF file.</p> required <code>save_page_image</code> <code>bool</code> <p>Whether to additionally save the image of the entire page.</p> <code>False</code> Source code in <code>medrag_multi_modal/document_loader/image_loader/marker_img_loader.py</code> <pre><code>class MarkerImageLoader(BaseImageLoader):\n    \"\"\"\n    `MarkerImageLoader` is a class that extends the `BaseImageLoader` class to handle the extraction and\n    loading of pages from a PDF file as images using the marker library.\n\n    This class provides functionality to extract images from a PDF file using marker library,\n    and optionally publish these images to a WandB artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader.image_loader import MarkerImageLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = MarkerImageLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n        ```\n\n    Args:\n        url (str): The URL of the PDF document.\n        document_name (str): The name of the document.\n        document_file_path (str): The path to the PDF file.\n        save_page_image (bool): Whether to additionally save the image of the entire page.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        document_name: str,\n        document_file_path: str,\n        save_page_image: bool = False,\n    ):\n        super().__init__(url, document_name, document_file_path)\n        self.save_page_image = save_page_image\n        self.model_lst = load_all_models()\n\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extracts a single page from the PDF as an image using marker library.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted image.\n            **kwargs: Additional keyword arguments that may be used by marker.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"image_file_path\": (str) the local file path where the image is stored.\n        \"\"\"\n        _, images, _ = convert_single_pdf(\n            self.document_file_path,\n            self.model_lst,\n            max_pages=1,\n            batch_multiplier=1,\n            start_page=page_idx,\n            ocr_all_pages=True,\n            **kwargs,\n        )\n\n        image_file_paths = []\n        for img_idx, (_, image) in enumerate(images.items()):\n            image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n            image_file_path = os.path.join(image_save_dir, image_file_name)\n            image.save(image_file_path, \"png\")\n            image_file_paths.append(image_file_path)\n\n        page_image = convert_from_path(\n            self.document_file_path,\n            first_page=page_idx + 1,\n            last_page=page_idx + 1,\n            **kwargs,\n        )[0]\n        page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n        return {\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n            \"image_file_paths\": os.path.join(image_save_dir, \"*.png\"),\n        }\n\n    def load_data(\n        self,\n        start_page: int | None = None,\n        end_page: int | None = None,\n        wandb_artifact_name: str | None = None,\n        image_save_dir: str = \"./images\",\n        exclude_file_extensions: list[str] = [],\n        cleanup: bool = False,\n        **kwargs,\n    ) -&gt; Coroutine[Any, Any, List[Dict[str, str]]]:\n        start_page = start_page - 1 if start_page is not None else None\n        end_page = end_page - 1 if end_page is not None else None\n        return super().load_data(\n            start_page,\n            end_page,\n            wandb_artifact_name,\n            image_save_dir,\n            exclude_file_extensions,\n            cleanup,\n            **kwargs,\n        )\n</code></pre>"},{"location":"rag/document_loader/image_loader/marker_img_loader/#medrag_multi_modal.document_loader.image_loader.marker_img_loader.MarkerImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>async</code>","text":"<p>Extracts a single page from the PDF as an image using marker library.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted image.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by marker.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the processed page data.</p> <code>Dict[str, Any]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, Any]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"image_file_path\": (str) the local file path where the image is stored.</li> </ul> Source code in <code>medrag_multi_modal/document_loader/image_loader/marker_img_loader.py</code> <pre><code>async def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extracts a single page from the PDF as an image using marker library.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted image.\n        **kwargs: Additional keyword arguments that may be used by marker.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"image_file_path\": (str) the local file path where the image is stored.\n    \"\"\"\n    _, images, _ = convert_single_pdf(\n        self.document_file_path,\n        self.model_lst,\n        max_pages=1,\n        batch_multiplier=1,\n        start_page=page_idx,\n        ocr_all_pages=True,\n        **kwargs,\n    )\n\n    image_file_paths = []\n    for img_idx, (_, image) in enumerate(images.items()):\n        image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n        image_file_path = os.path.join(image_save_dir, image_file_name)\n        image.save(image_file_path, \"png\")\n        image_file_paths.append(image_file_path)\n\n    page_image = convert_from_path(\n        self.document_file_path,\n        first_page=page_idx + 1,\n        last_page=page_idx + 1,\n        **kwargs,\n    )[0]\n    page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n    return {\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n        \"image_file_paths\": os.path.join(image_save_dir, \"*.png\"),\n    }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pdf2image_img_loader/","title":"Load images from PDF files (using PDF2Image)","text":"<p>Warning</p> <p>Unlike other image extraction methods in <code>document_loader.image_loader</code>, this loader does not extract embedded images from the PDF. Instead, it creates a snapshot image version of each selected page from the PDF.</p> Note <p>Underlying Library: <code>pdf2image</code></p> <p>Extract images from PDF files using <code>pdf2image</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.image_loader import PDF2ImageLoader\n</code></pre></p> <p>For details and available <code>**kwargs</code>, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>DataLab</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/image_loader/pdf2image_img_loader/#medrag_multi_modal.document_loader.image_loader.pdf2image_img_loader.PDF2ImageLoader","title":"<code>PDF2ImageLoader</code>","text":"<p>               Bases: <code>BaseImageLoader</code></p> <p><code>PDF2ImageLoader</code> is a class that extends the <code>BaseImageLoader</code> class to handle the extraction and loading of pages from a PDF file as images using the pdf2image library.</p> <p>This class provides functionality to convert specific pages of a PDF document into images and optionally publish these images to a WandB artifact. It is like a snapshot image version of each of the pages from the PDF.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader.image_loader import PDF2ImageLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PDF2ImageLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF document.</p> required <code>document_name</code> <code>str</code> <p>The name of the document.</p> required <code>document_file_path</code> <code>str</code> <p>The path to the PDF file.</p> required Source code in <code>medrag_multi_modal/document_loader/image_loader/pdf2image_img_loader.py</code> <pre><code>class PDF2ImageLoader(BaseImageLoader):\n    \"\"\"\n    `PDF2ImageLoader` is a class that extends the `BaseImageLoader` class to handle the extraction and\n    loading of pages from a PDF file as images using the pdf2image library.\n\n    This class provides functionality to convert specific pages of a PDF document into images\n    and optionally publish these images to a WandB artifact.\n    It is like a snapshot image version of each of the pages from the PDF.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader.image_loader import PDF2ImageLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PDF2ImageLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n        ```\n\n    Args:\n        url (str): The URL of the PDF document.\n        document_name (str): The name of the document.\n        document_file_path (str): The path to the PDF file.\n    \"\"\"\n\n    def __init__(self, url: str, document_name: str, document_file_path: str):\n        super().__init__(url, document_name, document_file_path)\n\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extracts a single page from the PDF as an image using pdf2image library.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted image.\n            **kwargs: Additional keyword arguments that may be used by pdf2image.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"image_file_path\": (str) the local file path where the image is stored.\n        \"\"\"\n        image = convert_from_path(\n            self.document_file_path,\n            first_page=page_idx + 1,\n            last_page=page_idx + 1,\n            **kwargs,\n        )[0]\n\n        image_file_name = f\"page{page_idx}.png\"\n        image_file_path = os.path.join(image_save_dir, image_file_name)\n        image.save(image_file_path)\n\n        return {\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n            \"image_file_path\": image_file_path,\n        }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pdf2image_img_loader/#medrag_multi_modal.document_loader.image_loader.pdf2image_img_loader.PDF2ImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>async</code>","text":"<p>Extracts a single page from the PDF as an image using pdf2image library.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted image.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by pdf2image.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the processed page data.</p> <code>Dict[str, Any]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, Any]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"image_file_path\": (str) the local file path where the image is stored.</li> </ul> Source code in <code>medrag_multi_modal/document_loader/image_loader/pdf2image_img_loader.py</code> <pre><code>async def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extracts a single page from the PDF as an image using pdf2image library.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted image.\n        **kwargs: Additional keyword arguments that may be used by pdf2image.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"image_file_path\": (str) the local file path where the image is stored.\n    \"\"\"\n    image = convert_from_path(\n        self.document_file_path,\n        first_page=page_idx + 1,\n        last_page=page_idx + 1,\n        **kwargs,\n    )[0]\n\n    image_file_name = f\"page{page_idx}.png\"\n    image_file_path = os.path.join(image_save_dir, image_file_name)\n    image.save(image_file_path)\n\n    return {\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n        \"image_file_path\": image_file_path,\n    }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pdfplumber_img_loader/","title":"Load images from PDF files (using PDFPlumber)","text":"Note <p>Underlying Library: <code>pdfplumber</code></p> <p>Extract images from PDF files using <code>pdfplumber</code>.</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.image_loader import PDFPlumberImageLoader\n</code></pre></p> <p>For details, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/image_loader/pdfplumber_img_loader/#medrag_multi_modal.document_loader.image_loader.pdfplumber_img_loader.PDFPlumberImageLoader","title":"<code>PDFPlumberImageLoader</code>","text":"<p>               Bases: <code>BaseImageLoader</code></p> <p><code>PDFPlumberImageLoader</code> is a class that extends the <code>BaseImageLoader</code> class to handle the extraction and loading of pages from a PDF file as images using the pdfplumber library.</p> <p>This class provides functionality to extract images from a PDF file using pdfplumber library, and optionally publish these images to a WandB artifact.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader.image_loader import PDFPlumberImageLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PDFPlumberImageLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF document.</p> required <code>document_name</code> <code>str</code> <p>The name of the document.</p> required <code>document_file_path</code> <code>str</code> <p>The path to the PDF file.</p> required Source code in <code>medrag_multi_modal/document_loader/image_loader/pdfplumber_img_loader.py</code> <pre><code>class PDFPlumberImageLoader(BaseImageLoader):\n    \"\"\"\n    `PDFPlumberImageLoader` is a class that extends the `BaseImageLoader` class to handle the extraction and\n    loading of pages from a PDF file as images using the pdfplumber library.\n\n    This class provides functionality to extract images from a PDF file using pdfplumber library,\n    and optionally publish these images to a WandB artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader.image_loader import PDFPlumberImageLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PDFPlumberImageLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n        ```\n\n    Args:\n        url (str): The URL of the PDF document.\n        document_name (str): The name of the document.\n        document_file_path (str): The path to the PDF file.\n    \"\"\"\n\n    def __init__(self, url: str, document_name: str, document_file_path: str):\n        super().__init__(url, document_name, document_file_path)\n\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extracts a single page from the PDF as an image using pdfplumber library.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted image.\n            **kwargs: Additional keyword arguments that may be used by pdfplumber.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"image_file_path\": (str) the local file path where the image is stored.\n        \"\"\"\n        with pdfplumber.open(self.document_file_path) as pdf:\n            page = pdf.pages[page_idx]\n            images = page.images\n\n            image_file_paths = []\n            for img_idx, image in enumerate(images):\n                extracted_image = page.crop(\n                    (\n                        image[\"x0\"],\n                        image[\"top\"],\n                        image[\"x1\"],\n                        image[\"bottom\"],\n                    )\n                ).to_image(resolution=300)\n\n                image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n                image_file_path = os.path.join(image_save_dir, image_file_name)\n\n                extracted_image.save(image_file_path, \"png\")\n                image_file_paths.append(image_file_path)\n\n        page_image = convert_from_path(\n            self.document_file_path,\n            first_page=page_idx + 1,\n            last_page=page_idx + 1,\n            **kwargs,\n        )[0]\n        page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n        return {\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n            \"image_file_paths\": image_file_paths,\n        }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pdfplumber_img_loader/#medrag_multi_modal.document_loader.image_loader.pdfplumber_img_loader.PDFPlumberImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>async</code>","text":"<p>Extracts a single page from the PDF as an image using pdfplumber library.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted image.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by pdfplumber.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the processed page data.</p> <code>Dict[str, Any]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, Any]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"image_file_path\": (str) the local file path where the image is stored.</li> </ul> Source code in <code>medrag_multi_modal/document_loader/image_loader/pdfplumber_img_loader.py</code> <pre><code>async def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extracts a single page from the PDF as an image using pdfplumber library.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted image.\n        **kwargs: Additional keyword arguments that may be used by pdfplumber.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"image_file_path\": (str) the local file path where the image is stored.\n    \"\"\"\n    with pdfplumber.open(self.document_file_path) as pdf:\n        page = pdf.pages[page_idx]\n        images = page.images\n\n        image_file_paths = []\n        for img_idx, image in enumerate(images):\n            extracted_image = page.crop(\n                (\n                    image[\"x0\"],\n                    image[\"top\"],\n                    image[\"x1\"],\n                    image[\"bottom\"],\n                )\n            ).to_image(resolution=300)\n\n            image_file_name = f\"page{page_idx}_fig{img_idx}.png\"\n            image_file_path = os.path.join(image_save_dir, image_file_name)\n\n            extracted_image.save(image_file_path, \"png\")\n            image_file_paths.append(image_file_path)\n\n    page_image = convert_from_path(\n        self.document_file_path,\n        first_page=page_idx + 1,\n        last_page=page_idx + 1,\n        **kwargs,\n    )[0]\n    page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n    return {\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n        \"image_file_paths\": image_file_paths,\n    }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pymupdf_img_loader/","title":"Load images from PDF files (using PyMuPDF)","text":"Note <p>Underlying Library: <code>pymupdf</code></p> <p>PyMuPDF is a high performance Python library for data extraction, analysis, conversion &amp; manipulation of PDF (and other) documents.</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.image_loader import PyMuPDFImageLoader\n</code></pre></p> <p>For details, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>Docs</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/image_loader/pymupdf_img_loader/#medrag_multi_modal.document_loader.image_loader.pymupdf_img_loader.PyMuPDFImageLoader","title":"<code>PyMuPDFImageLoader</code>","text":"<p>               Bases: <code>BaseImageLoader</code></p> <p><code>PyMuPDFImageLoader</code> is a class that extends the <code>BaseImageLoader</code> class to handle the extraction and loading of pages from a PDF file as images using the pymupdf library.</p> <p>This class provides functionality to extract images from a PDF file using pymupdf library, and optionally publish these images to a WandB artifact.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader.image_loader import PyMuPDFImageLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PyMuPDFImageLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF document.</p> required <code>document_name</code> <code>str</code> <p>The name of the document.</p> required <code>document_file_path</code> <code>str</code> <p>The path to the PDF file.</p> required Source code in <code>medrag_multi_modal/document_loader/image_loader/pymupdf_img_loader.py</code> <pre><code>class PyMuPDFImageLoader(BaseImageLoader):\n    \"\"\"\n    `PyMuPDFImageLoader` is a class that extends the `BaseImageLoader` class to handle the extraction and\n    loading of pages from a PDF file as images using the pymupdf library.\n\n    This class provides functionality to extract images from a PDF file using pymupdf library,\n    and optionally publish these images to a WandB artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader.image_loader import PyMuPDFImageLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PyMuPDFImageLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=32, end_page=37))\n        ```\n\n    Args:\n        url (str): The URL of the PDF document.\n        document_name (str): The name of the document.\n        document_file_path (str): The path to the PDF file.\n    \"\"\"\n\n    def __init__(self, url: str, document_name: str, document_file_path: str):\n        super().__init__(url, document_name, document_file_path)\n\n    async def extract_page_data(\n        self, page_idx: int, image_save_dir: str, **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extracts a single page from the PDF as an image using pymupdf library.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            image_save_dir (str): The directory to save the extracted image.\n            **kwargs: Additional keyword arguments that may be used by pymupdf.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"image_file_paths\": (list) the local file paths where the images are stored.\n        \"\"\"\n        image_file_paths = []\n\n        pdf_document = fitz.open(self.document_file_path)\n        page = pdf_document[page_idx]\n\n        images = page.get_images(full=True)\n        for img_idx, image in enumerate(images):\n            xref = image[0]\n            base_image = pdf_document.extract_image(xref)\n            image_bytes = base_image[\"image\"]\n            image_ext = base_image[\"ext\"]\n\n            if image_ext == \"jb2\":\n                image_ext = \"png\"\n            elif image_ext == \"jpx\":\n                image_ext = \"jpg\"\n\n            image_file_name = f\"page{page_idx}_fig{img_idx}.{image_ext}\"\n            image_file_path = os.path.join(image_save_dir, image_file_name)\n\n            # For JBIG2 and JPEG2000, we need to convert the image\n            if base_image[\"ext\"] in [\"jb2\", \"jpx\"]:\n                try:\n                    pix = fitz.Pixmap(image_bytes)\n                    pix.save(image_file_path)\n                except Exception as err_fitz:\n                    print(f\"Error processing image with fitz: {err_fitz}\")\n                    # Fallback to using PIL for image conversion\n                    try:\n                        img = Image.open(io.BytesIO(image_bytes))\n                        img.save(image_file_path)\n                    except Exception as err_pil:\n                        print(f\"Failed to process image with PIL: {err_pil}\")\n                        continue  # Skip this image if both methods fail\n            else:\n                with open(image_file_path, \"wb\") as image_file:\n                    image_file.write(image_bytes)\n\n            image_file_paths.append(image_file_path)\n\n        pdf_document.close()\n\n        page_image = convert_from_path(\n            self.document_file_path,\n            first_page=page_idx + 1,\n            last_page=page_idx + 1,\n            **kwargs,\n        )[0]\n        page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n        return {\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n            \"image_file_paths\": image_file_paths,\n        }\n</code></pre>"},{"location":"rag/document_loader/image_loader/pymupdf_img_loader/#medrag_multi_modal.document_loader.image_loader.pymupdf_img_loader.PyMuPDFImageLoader.extract_page_data","title":"<code>extract_page_data(page_idx, image_save_dir, **kwargs)</code>  <code>async</code>","text":"<p>Extracts a single page from the PDF as an image using pymupdf library.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>image_save_dir</code> <code>str</code> <p>The directory to save the extracted image.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by pymupdf.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the processed page data.</p> <code>Dict[str, Any]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, Any]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, Any]</code> <ul> <li>\"image_file_paths\": (list) the local file paths where the images are stored.</li> </ul> Source code in <code>medrag_multi_modal/document_loader/image_loader/pymupdf_img_loader.py</code> <pre><code>async def extract_page_data(\n    self, page_idx: int, image_save_dir: str, **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extracts a single page from the PDF as an image using pymupdf library.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        image_save_dir (str): The directory to save the extracted image.\n        **kwargs: Additional keyword arguments that may be used by pymupdf.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"image_file_paths\": (list) the local file paths where the images are stored.\n    \"\"\"\n    image_file_paths = []\n\n    pdf_document = fitz.open(self.document_file_path)\n    page = pdf_document[page_idx]\n\n    images = page.get_images(full=True)\n    for img_idx, image in enumerate(images):\n        xref = image[0]\n        base_image = pdf_document.extract_image(xref)\n        image_bytes = base_image[\"image\"]\n        image_ext = base_image[\"ext\"]\n\n        if image_ext == \"jb2\":\n            image_ext = \"png\"\n        elif image_ext == \"jpx\":\n            image_ext = \"jpg\"\n\n        image_file_name = f\"page{page_idx}_fig{img_idx}.{image_ext}\"\n        image_file_path = os.path.join(image_save_dir, image_file_name)\n\n        # For JBIG2 and JPEG2000, we need to convert the image\n        if base_image[\"ext\"] in [\"jb2\", \"jpx\"]:\n            try:\n                pix = fitz.Pixmap(image_bytes)\n                pix.save(image_file_path)\n            except Exception as err_fitz:\n                print(f\"Error processing image with fitz: {err_fitz}\")\n                # Fallback to using PIL for image conversion\n                try:\n                    img = Image.open(io.BytesIO(image_bytes))\n                    img.save(image_file_path)\n                except Exception as err_pil:\n                    print(f\"Failed to process image with PIL: {err_pil}\")\n                    continue  # Skip this image if both methods fail\n        else:\n            with open(image_file_path, \"wb\") as image_file:\n                image_file.write(image_bytes)\n\n        image_file_paths.append(image_file_path)\n\n    pdf_document.close()\n\n    page_image = convert_from_path(\n        self.document_file_path,\n        first_page=page_idx + 1,\n        last_page=page_idx + 1,\n        **kwargs,\n    )[0]\n    page_image.save(os.path.join(image_save_dir, f\"page{page_idx}.png\"))\n\n    return {\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n        \"image_file_paths\": image_file_paths,\n    }\n</code></pre>"},{"location":"rag/document_loader/text_loader/base_text_loader/","title":"Base","text":""},{"location":"rag/document_loader/text_loader/base_text_loader/#load-text-from-pdf-files","title":"Load text from PDF files","text":""},{"location":"rag/document_loader/text_loader/base_text_loader/#medrag_multi_modal.document_loader.text_loader.base_text_loader.BaseTextLoader","title":"<code>BaseTextLoader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for loading text from a PDF file, processing it into markdown, and optionally publishing it to a Weave dataset.</p> <p>This class handles the downloading of a PDF file from a given URL if it does not already exist locally. Subclasses should implement the specific PDF reading, text extraction, and markdown conversion methods.</p> <p>The processed pages are finally stored in a list of Page objects, which can be optionally published to a Weave dataset.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF file to download if not present locally.</p> required <code>document_name</code> <code>str</code> <p>The name of the document for metadata purposes.</p> required <code>document_file_path</code> <code>str</code> <p>The local file path where the PDF is stored or will be downloaded.</p> required <code>metadata</code> <code>Optional[dict[str, any]]</code> <p>Additional metadata to be added to each row of the dataset.</p> <code>None</code> Source code in <code>medrag_multi_modal/document_loader/text_loader/base_text_loader.py</code> <pre><code>class BaseTextLoader(ABC):\n    \"\"\"\n    An abstract base class for loading text from a PDF file, processing it into markdown,\n    and optionally publishing it to a Weave dataset.\n\n    This class handles the downloading of a PDF file from a given URL if it does not already\n    exist locally. Subclasses should implement the specific PDF reading, text extraction,\n    and markdown conversion methods.\n\n    The processed pages are finally stored in a list of Page objects, which can be optionally\n    published to a Weave dataset.\n\n    Args:\n        url (str): The URL of the PDF file to download if not present locally.\n        document_name (str): The name of the document for metadata purposes.\n        document_file_path (str): The local file path where the PDF is stored or will be downloaded.\n        metadata (Optional[dict[str, any]]): Additional metadata to be added to each row of the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        document_name: str,\n        document_file_path: str,\n        metadata: Optional[dict[str, Any]] = None,\n        streamlit_mode: bool = False,\n        preview_in_app: bool = False,\n    ):\n        self.url = url\n        self.document_name = document_name\n        self.document_file_path = document_file_path\n        self.metadata = metadata or {}\n        self.streamlit_mode = streamlit_mode\n        self.preview_in_app = preview_in_app\n        if not os.path.exists(self.document_file_path):\n            if self.url.startswith(\"http\"):\n                FireRequests().download(self.url, filenames=self.document_file_path)\n            else:\n                self.url = f\"https://drive.google.com/uc?id={self.url}\"\n                gdown.download(self.url, output=self.document_file_path)\n\n        with open(self.document_file_path, \"rb\") as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            self.page_count = len(pdf_reader.pages)\n\n    def get_page_indices(\n        self, start_page: Optional[int] = None, end_page: Optional[int] = None\n    ) -&gt; tuple[int, int]:\n        \"\"\"\n        Get the start and end page indices for processing.\n\n        Args:\n            start_page (Optional[int]): The starting page index (0-based) to process.\n            end_page (Optional[int]): The ending page index (0-based) to process.\n\n        Returns:\n            tuple[int, int]: A tuple containing the start and end page indices.\n        \"\"\"\n\n        if start_page:\n            if start_page &gt; self.page_count:\n                raise ValueError(\n                    f\"Start page {start_page} is greater than the total page count {self.page_count}\"\n                )\n        else:\n            start_page = 0\n        if end_page:\n            if end_page &gt; self.page_count:\n                raise ValueError(\n                    f\"End page {end_page} is greater than the total page count {self.page_count}\"\n                )\n        else:\n            end_page = self.page_count - 1\n        return start_page, end_page\n\n    @abstractmethod\n    async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n        \"\"\"\n        Abstract method to process a single page of the PDF and extract the text data.\n\n        Overwrite this method in the subclass to provide the actual implementation and\n        processing logic for each page of the PDF using various PDF processing libraries.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            **kwargs: Additional keyword arguments that may be used by underlying libraries.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        pass\n\n    async def load_data(\n        self,\n        start_page: Optional[int] = None,\n        end_page: Optional[int] = None,\n        exclude_pages: Optional[list[int]] = None,\n        dataset_repo_id: Optional[str] = None,\n        dataset_split: Optional[str] = None,\n        is_dataset_private: bool = False,\n        overwrite_dataset: bool = False,\n        **kwargs,\n    ) -&gt; Dataset:\n        \"\"\"\n        Asynchronously loads text from a PDF file specified by a URL or local file path.\n        The overrided processing abstract method then processes the text into markdown format,\n        and optionally publishes it to a Weave dataset.\n\n        This function downloads a PDF from a given URL if it does not already exist locally,\n        reads the specified range of pages, converts each page's content to markdown, and\n        returns a list of Page objects containing the text and metadata.\n\n        It uses `PyPDF2` to calculate the number of pages in the PDF and the\n        overriden `extract_page_data` method provides the actual implementation to process\n        each page, extract the text from the PDF, and convert it to markdown.\n        It processes pages concurrently using `asyncio` for efficiency.\n\n        If a `dataset_repo_id` is provided, the processed pages are published to a HuggingFace dataset.\n\n        Args:\n            start_page (Optional[int]): The starting page index (0-based) to process.\n            end_page (Optional[int]): The ending page index (0-based) to process.\n            exclude_pages (Optional[list[int]]): The list of page indices to exclude from processing.\n            dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish the pages to, if provided.\n            dataset_split (Optional[str]): The split of the HuggingFace dataset to publish the pages to, if provided.\n            is_dataset_private (bool): Whether the dataset should be private.\n            overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists.\n            **kwargs: Additional keyword arguments that will be passed to extract_page_data method and the underlying library.\n\n        Returns:\n            Dataset: A HuggingFace Dataset object containing the text and metadata for processed pages.\n            Each entry in the dataset will have the following keys and values:\n\n            - \"text\": (str) the processed page data in markdown format.\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"loader_name\": (str) the name of the loader class used to process the page.\n\n        Raises:\n            ValueError: If the specified start_page or end_page is out of bounds of the document's page count.\n        \"\"\"\n        start_page, end_page = self.get_page_indices(start_page, end_page)\n        pages = []\n        processed_pages_counter: int = 1\n        total_pages = end_page - start_page\n        exclude_pages = exclude_pages or []\n        loader_name = self.__class__.__name__\n        dataset_split = loader_name.lower() if dataset_split is None else dataset_split\n\n        if self.preview_in_app and total_pages - len(exclude_pages) &gt; 10:\n            warning_message = \"Previewing more than 10 pages in app is not recommended due to performance issues.\"\n            if self.streamlit_mode:\n                st.warning(warning_message)\n            raise ResourceWarning(warning_message)\n\n        streamlit_progressbar = (\n            st.progress(\n                0,\n                text=f\"Loading page {processed_pages_counter}/{total_pages} using {loader_name}\",\n            )\n            if self.streamlit_mode\n            else None\n        )\n\n        async def process_page(page_idx):\n            nonlocal processed_pages_counter\n            nonlocal streamlit_progressbar\n            page_data = await self.extract_page_data(page_idx, **kwargs)\n            page_data[\"loader_name\"] = loader_name\n            for key, value in self.metadata.items():\n                if key not in page_data:\n                    page_data[key] = value\n            pages.append(page_data)\n            progress.update(\n                task_id,\n                advance=1,\n                description=f\"Loading page {page_idx} using {loader_name}\",\n            )\n            if streamlit_progressbar:\n                progress_percentage = min(\n                    100, max(0, int((processed_pages_counter / total_pages) * 100))\n                )\n                streamlit_progressbar.progress(\n                    progress_percentage,\n                    text=f\"Loading page {page_idx} using {loader_name} ({processed_pages_counter}/{total_pages + 1})\",\n                )\n                if self.preview_in_app:\n                    with st.expander(f\"Page Index: {page_idx}\"):\n                        st.markdown(page_data[\"text\"])\n            processed_pages_counter += 1\n\n        progress = Progress()\n        with progress:\n            task_id = progress.add_task(\"Starting...\", total=total_pages)\n            tasks = [\n                process_page(page_idx)\n                for page_idx in range(start_page, end_page + 1)\n                if page_idx not in exclude_pages\n            ]\n            for task in asyncio.as_completed(tasks):\n                await task\n\n        pages.sort(key=lambda x: x[\"page_idx\"])\n\n        dataset = Dataset.from_list(pages)\n        if dataset_repo_id:\n            if huggingface_hub.repo_exists(dataset_repo_id, repo_type=\"dataset\"):\n                existing_dataset = load_dataset(dataset_repo_id)\n                if not overwrite_dataset:\n                    if dataset_split in existing_dataset:\n                        existing_dataset[dataset_split] = concatenate_datasets(\n                            [dataset, existing_dataset[dataset_split]]\n                        )\n                        dataset = existing_dataset\n                    else:\n                        existing_dataset[dataset_split] = dataset\n                        dataset = existing_dataset\n                else:\n                    existing_dataset[dataset_split] = dataset\n                    dataset = existing_dataset\n\n            if isinstance(dataset, DatasetDict):\n                if \"train\" in dataset.keys():\n                    del dataset[\"train\"]\n                dataset.push_to_hub(repo_id=dataset_repo_id, private=is_dataset_private)\n            else:\n                dataset.push_to_hub(\n                    repo_id=dataset_repo_id,\n                    private=is_dataset_private,\n                    split=dataset_split,\n                )\n\n        return dataset\n</code></pre>"},{"location":"rag/document_loader/text_loader/base_text_loader/#medrag_multi_modal.document_loader.text_loader.base_text_loader.BaseTextLoader.extract_page_data","title":"<code>extract_page_data(page_idx, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to process a single page of the PDF and extract the text data.</p> <p>Overwrite this method in the subclass to provide the actual implementation and processing logic for each page of the PDF using various PDF processing libraries.</p> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>**kwargs</code> <p>Additional keyword arguments that may be used by underlying libraries.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/base_text_loader.py</code> <pre><code>@abstractmethod\nasync def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"\n    Abstract method to process a single page of the PDF and extract the text data.\n\n    Overwrite this method in the subclass to provide the actual implementation and\n    processing logic for each page of the PDF using various PDF processing libraries.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        **kwargs: Additional keyword arguments that may be used by underlying libraries.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"rag/document_loader/text_loader/base_text_loader/#medrag_multi_modal.document_loader.text_loader.base_text_loader.BaseTextLoader.get_page_indices","title":"<code>get_page_indices(start_page=None, end_page=None)</code>","text":"<p>Get the start and end page indices for processing.</p> <p>Parameters:</p> Name Type Description Default <code>start_page</code> <code>Optional[int]</code> <p>The starting page index (0-based) to process.</p> <code>None</code> <code>end_page</code> <code>Optional[int]</code> <p>The ending page index (0-based) to process.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: A tuple containing the start and end page indices.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/base_text_loader.py</code> <pre><code>def get_page_indices(\n    self, start_page: Optional[int] = None, end_page: Optional[int] = None\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Get the start and end page indices for processing.\n\n    Args:\n        start_page (Optional[int]): The starting page index (0-based) to process.\n        end_page (Optional[int]): The ending page index (0-based) to process.\n\n    Returns:\n        tuple[int, int]: A tuple containing the start and end page indices.\n    \"\"\"\n\n    if start_page:\n        if start_page &gt; self.page_count:\n            raise ValueError(\n                f\"Start page {start_page} is greater than the total page count {self.page_count}\"\n            )\n    else:\n        start_page = 0\n    if end_page:\n        if end_page &gt; self.page_count:\n            raise ValueError(\n                f\"End page {end_page} is greater than the total page count {self.page_count}\"\n            )\n    else:\n        end_page = self.page_count - 1\n    return start_page, end_page\n</code></pre>"},{"location":"rag/document_loader/text_loader/base_text_loader/#medrag_multi_modal.document_loader.text_loader.base_text_loader.BaseTextLoader.load_data","title":"<code>load_data(start_page=None, end_page=None, exclude_pages=None, dataset_repo_id=None, dataset_split=None, is_dataset_private=False, overwrite_dataset=False, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously loads text from a PDF file specified by a URL or local file path. The overrided processing abstract method then processes the text into markdown format, and optionally publishes it to a Weave dataset.</p> <p>This function downloads a PDF from a given URL if it does not already exist locally, reads the specified range of pages, converts each page's content to markdown, and returns a list of Page objects containing the text and metadata.</p> <p>It uses <code>PyPDF2</code> to calculate the number of pages in the PDF and the overriden <code>extract_page_data</code> method provides the actual implementation to process each page, extract the text from the PDF, and convert it to markdown. It processes pages concurrently using <code>asyncio</code> for efficiency.</p> <p>If a <code>dataset_repo_id</code> is provided, the processed pages are published to a HuggingFace dataset.</p> <p>Parameters:</p> Name Type Description Default <code>start_page</code> <code>Optional[int]</code> <p>The starting page index (0-based) to process.</p> <code>None</code> <code>end_page</code> <code>Optional[int]</code> <p>The ending page index (0-based) to process.</p> <code>None</code> <code>exclude_pages</code> <code>Optional[list[int]]</code> <p>The list of page indices to exclude from processing.</p> <code>None</code> <code>dataset_repo_id</code> <code>Optional[str]</code> <p>The repository ID of the HuggingFace dataset to publish the pages to, if provided.</p> <code>None</code> <code>dataset_split</code> <code>Optional[str]</code> <p>The split of the HuggingFace dataset to publish the pages to, if provided.</p> <code>None</code> <code>is_dataset_private</code> <code>bool</code> <p>Whether the dataset should be private.</p> <code>False</code> <code>overwrite_dataset</code> <code>bool</code> <p>Whether to overwrite the existing dataset if it exists.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments that will be passed to extract_page_data method and the underlying library.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A HuggingFace Dataset object containing the text and metadata for processed pages.</p> <code>Dataset</code> <p>Each entry in the dataset will have the following keys and values:</p> <code>Dataset</code> <ul> <li>\"text\": (str) the processed page data in markdown format.</li> </ul> <code>Dataset</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dataset</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dataset</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dataset</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dataset</code> <ul> <li>\"loader_name\": (str) the name of the loader class used to process the page.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified start_page or end_page is out of bounds of the document's page count.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/base_text_loader.py</code> <pre><code>async def load_data(\n    self,\n    start_page: Optional[int] = None,\n    end_page: Optional[int] = None,\n    exclude_pages: Optional[list[int]] = None,\n    dataset_repo_id: Optional[str] = None,\n    dataset_split: Optional[str] = None,\n    is_dataset_private: bool = False,\n    overwrite_dataset: bool = False,\n    **kwargs,\n) -&gt; Dataset:\n    \"\"\"\n    Asynchronously loads text from a PDF file specified by a URL or local file path.\n    The overrided processing abstract method then processes the text into markdown format,\n    and optionally publishes it to a Weave dataset.\n\n    This function downloads a PDF from a given URL if it does not already exist locally,\n    reads the specified range of pages, converts each page's content to markdown, and\n    returns a list of Page objects containing the text and metadata.\n\n    It uses `PyPDF2` to calculate the number of pages in the PDF and the\n    overriden `extract_page_data` method provides the actual implementation to process\n    each page, extract the text from the PDF, and convert it to markdown.\n    It processes pages concurrently using `asyncio` for efficiency.\n\n    If a `dataset_repo_id` is provided, the processed pages are published to a HuggingFace dataset.\n\n    Args:\n        start_page (Optional[int]): The starting page index (0-based) to process.\n        end_page (Optional[int]): The ending page index (0-based) to process.\n        exclude_pages (Optional[list[int]]): The list of page indices to exclude from processing.\n        dataset_repo_id (Optional[str]): The repository ID of the HuggingFace dataset to publish the pages to, if provided.\n        dataset_split (Optional[str]): The split of the HuggingFace dataset to publish the pages to, if provided.\n        is_dataset_private (bool): Whether the dataset should be private.\n        overwrite_dataset (bool): Whether to overwrite the existing dataset if it exists.\n        **kwargs: Additional keyword arguments that will be passed to extract_page_data method and the underlying library.\n\n    Returns:\n        Dataset: A HuggingFace Dataset object containing the text and metadata for processed pages.\n        Each entry in the dataset will have the following keys and values:\n\n        - \"text\": (str) the processed page data in markdown format.\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"loader_name\": (str) the name of the loader class used to process the page.\n\n    Raises:\n        ValueError: If the specified start_page or end_page is out of bounds of the document's page count.\n    \"\"\"\n    start_page, end_page = self.get_page_indices(start_page, end_page)\n    pages = []\n    processed_pages_counter: int = 1\n    total_pages = end_page - start_page\n    exclude_pages = exclude_pages or []\n    loader_name = self.__class__.__name__\n    dataset_split = loader_name.lower() if dataset_split is None else dataset_split\n\n    if self.preview_in_app and total_pages - len(exclude_pages) &gt; 10:\n        warning_message = \"Previewing more than 10 pages in app is not recommended due to performance issues.\"\n        if self.streamlit_mode:\n            st.warning(warning_message)\n        raise ResourceWarning(warning_message)\n\n    streamlit_progressbar = (\n        st.progress(\n            0,\n            text=f\"Loading page {processed_pages_counter}/{total_pages} using {loader_name}\",\n        )\n        if self.streamlit_mode\n        else None\n    )\n\n    async def process_page(page_idx):\n        nonlocal processed_pages_counter\n        nonlocal streamlit_progressbar\n        page_data = await self.extract_page_data(page_idx, **kwargs)\n        page_data[\"loader_name\"] = loader_name\n        for key, value in self.metadata.items():\n            if key not in page_data:\n                page_data[key] = value\n        pages.append(page_data)\n        progress.update(\n            task_id,\n            advance=1,\n            description=f\"Loading page {page_idx} using {loader_name}\",\n        )\n        if streamlit_progressbar:\n            progress_percentage = min(\n                100, max(0, int((processed_pages_counter / total_pages) * 100))\n            )\n            streamlit_progressbar.progress(\n                progress_percentage,\n                text=f\"Loading page {page_idx} using {loader_name} ({processed_pages_counter}/{total_pages + 1})\",\n            )\n            if self.preview_in_app:\n                with st.expander(f\"Page Index: {page_idx}\"):\n                    st.markdown(page_data[\"text\"])\n        processed_pages_counter += 1\n\n    progress = Progress()\n    with progress:\n        task_id = progress.add_task(\"Starting...\", total=total_pages)\n        tasks = [\n            process_page(page_idx)\n            for page_idx in range(start_page, end_page + 1)\n            if page_idx not in exclude_pages\n        ]\n        for task in asyncio.as_completed(tasks):\n            await task\n\n    pages.sort(key=lambda x: x[\"page_idx\"])\n\n    dataset = Dataset.from_list(pages)\n    if dataset_repo_id:\n        if huggingface_hub.repo_exists(dataset_repo_id, repo_type=\"dataset\"):\n            existing_dataset = load_dataset(dataset_repo_id)\n            if not overwrite_dataset:\n                if dataset_split in existing_dataset:\n                    existing_dataset[dataset_split] = concatenate_datasets(\n                        [dataset, existing_dataset[dataset_split]]\n                    )\n                    dataset = existing_dataset\n                else:\n                    existing_dataset[dataset_split] = dataset\n                    dataset = existing_dataset\n            else:\n                existing_dataset[dataset_split] = dataset\n                dataset = existing_dataset\n\n        if isinstance(dataset, DatasetDict):\n            if \"train\" in dataset.keys():\n                del dataset[\"train\"]\n            dataset.push_to_hub(repo_id=dataset_repo_id, private=is_dataset_private)\n        else:\n            dataset.push_to_hub(\n                repo_id=dataset_repo_id,\n                private=is_dataset_private,\n                split=dataset_split,\n            )\n\n    return dataset\n</code></pre>"},{"location":"rag/document_loader/text_loader/marker_text_loader/","title":"Marker","text":""},{"location":"rag/document_loader/text_loader/marker_text_loader/#load-text-from-pdf-files-using-marker","title":"Load text from PDF files (using Marker)","text":"Note <p>Underlying Library: <code>marker-pdf</code></p> <p>Convert PDF to markdown quickly and accurately using a pipeline of deep learning models.</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.text_loader import MarkerTextLoader\n</code></pre></p> <p>For details and available <code>**kwargs</code>, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>DataLab</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/text_loader/marker_text_loader/#medrag_multi_modal.document_loader.text_loader.marker_text_loader.MarkerTextLoader","title":"<code>MarkerTextLoader</code>","text":"<p>               Bases: <code>BaseTextLoader</code></p> <p>A concrete implementation of the BaseTextLoader for loading text from a PDF file using <code>marker-pdf</code>, processing it into a structured text format, and optionally publishing it to a Weave dataset.</p> <p>This class extends the BaseTextLoader and implements the abstract methods to load and process pages from a PDF file using marker-pdf, which is a pipeline of deep learning models.</p> <p>This class will handle the downloading of a PDF file from a given URL if it does not already exist locally. It uses marker-pdf to read the PDF and extract structured text from each page. The processed pages are stored in a list of Page objects, which can be optionally published to a Weave dataset.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader import MarkerTextLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = MarkerTextLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF file to download if not present locally.</p> required <code>document_name</code> <code>str</code> <p>The name of the document for metadata purposes.</p> required <code>document_file_path</code> <code>str</code> <p>The local file path where the PDF is stored or will be downloaded.</p> required Source code in <code>medrag_multi_modal/document_loader/text_loader/marker_text_loader.py</code> <pre><code>class MarkerTextLoader(BaseTextLoader):\n    \"\"\"\n    A concrete implementation of the BaseTextLoader for loading text from a PDF file\n    using `marker-pdf`, processing it into a structured text format, and optionally publishing\n    it to a Weave dataset.\n\n    This class extends the BaseTextLoader and implements the abstract methods to\n    load and process pages from a PDF file using marker-pdf, which is a pipeline of deep learning models.\n\n    This class will handle the downloading of a PDF file from a given URL if it does not already exist locally.\n    It uses marker-pdf to read the PDF and extract structured text from each page. The processed pages are stored\n    in a list of Page objects, which can be optionally published to a Weave dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader import MarkerTextLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = MarkerTextLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n        ```\n\n    Args:\n        url (str): The URL of the PDF file to download if not present locally.\n        document_name (str): The name of the document for metadata purposes.\n        document_file_path (str): The local file path where the PDF is stored or will be downloaded.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.models_list = create_model_dict()\n\n    async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n        \"\"\"\n        Process a single page of the PDF and extract its structured text using marker-pdf.\n\n        Returns:\n            Dict[str, str]: A dictionary with the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"text\": (str) the extracted structured text from the page.\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n            - \"meta\": (dict) the metadata extracted from the page by marker-pdf.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            **kwargs: Additional keyword arguments to be passed to `marker.convert.convert_single_pdf`.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        config_parser = ConfigParser({\"page_range\": f\"{page_idx},{page_idx+1}\"})\n        config_dict = config_parser.generate_config_dict()\n        config_dict[\"pdftext_workers\"] = 1\n        rendered = PdfConverter(\n            config=config_dict,\n            artifact_dict=self.models_list,\n        )(self.document_file_path)\n\n        return {\n            \"text\": rendered.markdown,\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n        }\n</code></pre>"},{"location":"rag/document_loader/text_loader/marker_text_loader/#medrag_multi_modal.document_loader.text_loader.marker_text_loader.MarkerTextLoader.extract_page_data","title":"<code>extract_page_data(page_idx, **kwargs)</code>  <code>async</code>","text":"<p>Process a single page of the PDF and extract its structured text using marker-pdf.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary with the processed page data.</p> <code>Dict[str, str]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, str]</code> <ul> <li>\"text\": (str) the extracted structured text from the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"meta\": (dict) the metadata extracted from the page by marker-pdf.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>marker.convert.convert_single_pdf</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/marker_text_loader.py</code> <pre><code>async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"\n    Process a single page of the PDF and extract its structured text using marker-pdf.\n\n    Returns:\n        Dict[str, str]: A dictionary with the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"text\": (str) the extracted structured text from the page.\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n        - \"meta\": (dict) the metadata extracted from the page by marker-pdf.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        **kwargs: Additional keyword arguments to be passed to `marker.convert.convert_single_pdf`.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    config_parser = ConfigParser({\"page_range\": f\"{page_idx},{page_idx+1}\"})\n    config_dict = config_parser.generate_config_dict()\n    config_dict[\"pdftext_workers\"] = 1\n    rendered = PdfConverter(\n        config=config_dict,\n        artifact_dict=self.models_list,\n    )(self.document_file_path)\n\n    return {\n        \"text\": rendered.markdown,\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n    }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pdfplumber_text_loader/","title":"PDFPlumber","text":""},{"location":"rag/document_loader/text_loader/pdfplumber_text_loader/#load-text-from-pdf-files-using-pdfplumber","title":"Load text from PDF files (using PDFPlumber)","text":"Note <p>Underlying Library: <code>pdfplumber</code></p> <p>Plumb a PDF for detailed information about each char, rectangle, line, et cetera \u2014 and easily extract text and tables.</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.text_loader import PDFPlumberTextLoader\n</code></pre></p> <p>For details and available <code>**kwargs</code>, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/text_loader/pdfplumber_text_loader/#medrag_multi_modal.document_loader.text_loader.pdfplumber_text_loader.PDFPlumberTextLoader","title":"<code>PDFPlumberTextLoader</code>","text":"<p>               Bases: <code>BaseTextLoader</code></p> <p>A concrete implementation of the BaseTextLoader for loading text from a PDF file using <code>pdfplumber</code>, processing it into a simple text format, and optionally publishing it to a Weave dataset.</p> <p>This class extends the BaseTextLoader and implements the abstract methods to load and process pages from a PDF file.</p> <p>This class will handle the downloading of a PDF file from a given URL if it does not already exist locally. It uses pdfplumber to read the PDF and extract text from each page. The processed pages are stored in a list of Page objects, which can be optionally published to a Weave dataset.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader import PDFPlumberTextLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PDFPlumberTextLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF file to download if not present locally.</p> required <code>document_name</code> <code>str</code> <p>The name of the document for metadata purposes.</p> required <code>document_file_path</code> <code>str</code> <p>The local file path where the PDF is stored or will be downloaded.</p> required Source code in <code>medrag_multi_modal/document_loader/text_loader/pdfplumber_text_loader.py</code> <pre><code>class PDFPlumberTextLoader(BaseTextLoader):\n    \"\"\"\n    A concrete implementation of the BaseTextLoader for loading text from a PDF file\n    using `pdfplumber`, processing it into a simple text format, and optionally publishing\n    it to a Weave dataset.\n\n    This class extends the BaseTextLoader and implements the abstract methods to\n    load and process pages from a PDF file.\n\n    This class will handle the downloading of a PDF file from a given URL if it does not already exist locally.\n    It uses pdfplumber to read the PDF and extract text from each page. The processed pages are stored in a list\n    of Page objects, which can be optionally published to a Weave dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader import PDFPlumberTextLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PDFPlumberTextLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n        ```\n\n    Args:\n        url (str): The URL of the PDF file to download if not present locally.\n        document_name (str): The name of the document for metadata purposes.\n        document_file_path (str): The local file path where the PDF is stored or will be downloaded.\n    \"\"\"\n\n    async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n        \"\"\"\n        Process a single page of the PDF and extract its text using pdfplumber.\n\n        Returns:\n            Dict[str, str]: A dictionary with the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"text\": (str) the extracted text from the page.\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            **kwargs: Additional keyword arguments to be passed to `pdfplumber.Page.extract_text`.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        with pdfplumber.open(self.document_file_path) as pdf:\n            page = pdf.pages[page_idx]\n            text = page.extract_text(**kwargs)\n\n        return {\n            \"text\": text,\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n        }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pdfplumber_text_loader/#medrag_multi_modal.document_loader.text_loader.pdfplumber_text_loader.PDFPlumberTextLoader.extract_page_data","title":"<code>extract_page_data(page_idx, **kwargs)</code>  <code>async</code>","text":"<p>Process a single page of the PDF and extract its text using pdfplumber.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary with the processed page data.</p> <code>Dict[str, str]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, str]</code> <ul> <li>\"text\": (str) the extracted text from the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>pdfplumber.Page.extract_text</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/pdfplumber_text_loader.py</code> <pre><code>async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"\n    Process a single page of the PDF and extract its text using pdfplumber.\n\n    Returns:\n        Dict[str, str]: A dictionary with the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"text\": (str) the extracted text from the page.\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        **kwargs: Additional keyword arguments to be passed to `pdfplumber.Page.extract_text`.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    with pdfplumber.open(self.document_file_path) as pdf:\n        page = pdf.pages[page_idx]\n        text = page.extract_text(**kwargs)\n\n    return {\n        \"text\": text,\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n    }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pymupdf4llm_text_loader/","title":"PyMuPDF4LLM","text":""},{"location":"rag/document_loader/text_loader/pymupdf4llm_text_loader/#load-text-from-pdf-files-using-pymupdf4llm","title":"Load text from PDF files (using PyMuPDF4LLM)","text":"Note <p>Underlying Library: <code>pymupdf4llm</code></p> <p>PyMuPDF is a high performance Python library for data extraction, analysis, conversion &amp; manipulation of PDF (and other) documents.</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.text_loader import PyMuPDF4LLMTextLoader\n</code></pre></p> <p>For details and available <code>**kwargs</code>, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>Docs</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/text_loader/pymupdf4llm_text_loader/#medrag_multi_modal.document_loader.text_loader.pymupdf4llm_text_loader.PyMuPDF4LLMTextLoader","title":"<code>PyMuPDF4LLMTextLoader</code>","text":"<p>               Bases: <code>BaseTextLoader</code></p> <p>A concrete implementation of the BaseTextLoader for loading text from a PDF file, processing it into markdown using <code>pymupdf4llm</code>, and optionally publishing it to a Weave dataset.</p> <p>This class extends the BaseTextLoader and implements the abstract methods to load and process pages from a PDF file.</p> <p>This class will handle the downloading of a PDF file from a given URL if it does not already exist locally. It uses PyPDF2 to read the PDF and pymupdf4llm to convert pages to markdown. The processed pages are stored in a list of Page objects, which can be optionally published to a Weave dataset.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader import PyMuPDF4LLMTextLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PyMuPDF4LLMTextLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF file to download if not present locally.</p> required <code>document_name</code> <code>str</code> <p>The name of the document for metadata purposes.</p> required <code>document_file_path</code> <code>str</code> <p>The local file path where the PDF is stored or will be downloaded.</p> required Source code in <code>medrag_multi_modal/document_loader/text_loader/pymupdf4llm_text_loader.py</code> <pre><code>class PyMuPDF4LLMTextLoader(BaseTextLoader):\n    \"\"\"\n    A concrete implementation of the BaseTextLoader for loading text from a PDF file,\n    processing it into markdown using `pymupdf4llm`, and optionally publishing it to a Weave dataset.\n\n    This class extends the BaseTextLoader and implements the abstract methods to load and process pages from a PDF file.\n\n    This class will handle the downloading of a PDF file from a given URL if it does not already exist locally.\n    It uses PyPDF2 to read the PDF and pymupdf4llm to convert pages to markdown. The processed pages are stored in a list\n    of Page objects, which can be optionally published to a Weave dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader import PyMuPDF4LLMTextLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PyMuPDF4LLMTextLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n        ```\n\n    Args:\n        url (str): The URL of the PDF file to download if not present locally.\n        document_name (str): The name of the document for metadata purposes.\n        document_file_path (str): The local file path where the PDF is stored or will be downloaded.\n    \"\"\"\n\n    async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n        \"\"\"\n        Process a single page of the PDF and convert it to markdown using `pymupdf4llm`.\n\n        Returns:\n            Dict[str, str]: A dictionary with the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"text\": (str) the processed page data in markdown format.\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            **kwargs: Additional keyword arguments to be passed to `pymupdf4llm.to_markdown`.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        text = pymupdf4llm.to_markdown(\n            doc=self.document_file_path, pages=[page_idx], show_progress=False, **kwargs\n        )\n        return {\n            \"text\": text,\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n        }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pymupdf4llm_text_loader/#medrag_multi_modal.document_loader.text_loader.pymupdf4llm_text_loader.PyMuPDF4LLMTextLoader.extract_page_data","title":"<code>extract_page_data(page_idx, **kwargs)</code>  <code>async</code>","text":"<p>Process a single page of the PDF and convert it to markdown using <code>pymupdf4llm</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary with the processed page data.</p> <code>Dict[str, str]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, str]</code> <ul> <li>\"text\": (str) the processed page data in markdown format.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>pymupdf4llm.to_markdown</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/pymupdf4llm_text_loader.py</code> <pre><code>async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"\n    Process a single page of the PDF and convert it to markdown using `pymupdf4llm`.\n\n    Returns:\n        Dict[str, str]: A dictionary with the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"text\": (str) the processed page data in markdown format.\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        **kwargs: Additional keyword arguments to be passed to `pymupdf4llm.to_markdown`.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    text = pymupdf4llm.to_markdown(\n        doc=self.document_file_path, pages=[page_idx], show_progress=False, **kwargs\n    )\n    return {\n        \"text\": text,\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n    }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pypdf2_text_loader/","title":"PyPDF2","text":""},{"location":"rag/document_loader/text_loader/pypdf2_text_loader/#load-text-from-pdf-files-using-pypdf2","title":"Load text from PDF files (using PyPDF2)","text":"Note <p>Underlying Library: <code>pypdf2</code></p> <p>A pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files</p> <p>You can interact with the underlying library and fine-tune the outputs via <code>**kwargs</code>.</p> <p>Use it in our library with: <pre><code>from medrag_multi_modal.document_loader.text_loader import PyPDF2TextLoader\n</code></pre></p> <p>For details and available <code>**kwargs</code>, please refer to the sources below.</p> <p>Sources:</p> <ul> <li>Docs</li> <li>GitHub</li> <li>PyPI</li> </ul>"},{"location":"rag/document_loader/text_loader/pypdf2_text_loader/#medrag_multi_modal.document_loader.text_loader.pypdf2_text_loader.PyPDF2TextLoader","title":"<code>PyPDF2TextLoader</code>","text":"<p>               Bases: <code>BaseTextLoader</code></p> <p>A concrete implementation of the BaseTextLoader for loading text from a PDF file using <code>PyPDF2</code>, processing it into a simple text format, and optionally publishing it to a Weave dataset.</p> <p>This class extends the BaseTextLoader and implements the abstract methods to load and process pages from a PDF file.</p> <p>This class will handle the downloading of a PDF file from a given URL if it does not already exist locally. It uses PyPDF2 to read the PDF and extract text from each page. The processed pages are stored in a list of Page objects, which can be optionally published to a Weave dataset.</p> <p>Example Usage</p> <pre><code>import asyncio\n\nfrom medrag_multi_modal.document_loader import PyPDF2TextLoader\n\nURL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\nloader = PyPDF2TextLoader(\n    url=URL,\n    document_name=\"Gray's Anatomy\",\n    document_file_path=\"grays_anatomy.pdf\",\n)\ndataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PDF file to download if not present locally.</p> required <code>document_name</code> <code>str</code> <p>The name of the document for metadata purposes.</p> required <code>document_file_path</code> <code>str</code> <p>The local file path where the PDF is stored or will be downloaded.</p> required Source code in <code>medrag_multi_modal/document_loader/text_loader/pypdf2_text_loader.py</code> <pre><code>class PyPDF2TextLoader(BaseTextLoader):\n    \"\"\"\n    A concrete implementation of the BaseTextLoader for loading text from a PDF file\n    using `PyPDF2`, processing it into a simple text format, and optionally publishing\n    it to a Weave dataset.\n\n    This class extends the BaseTextLoader and implements the abstract methods to\n    load and process pages from a PDF file.\n\n    This class will handle the downloading of a PDF file from a given URL if it does not already exist locally.\n    It uses PyPDF2 to read the PDF and extract text from each page. The processed pages are stored in a list\n    of Page objects, which can be optionally published to a Weave dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import asyncio\n\n        from medrag_multi_modal.document_loader import PyPDF2TextLoader\n\n        URL = \"https://archive.org/download/GraysAnatomy41E2015PDF/Grays%20Anatomy-41%20E%20%282015%29%20%5BPDF%5D.pdf\"\n\n        loader = PyPDF2TextLoader(\n            url=URL,\n            document_name=\"Gray's Anatomy\",\n            document_file_path=\"grays_anatomy.pdf\",\n        )\n        dataset = asyncio.run(loader.load_data(start_page=31, end_page=36))\n        ```\n\n    Args:\n        url (str): The URL of the PDF file to download if not present locally.\n        document_name (str): The name of the document for metadata purposes.\n        document_file_path (str): The local file path where the PDF is stored or will be downloaded.\n    \"\"\"\n\n    async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n        \"\"\"\n        Process a single page of the PDF and extract its text using PyPDF2.\n\n        Returns:\n            Dict[str, str]: A dictionary with the processed page data.\n            The dictionary will have the following keys and values:\n\n            - \"text\": (str) the extracted text from the page.\n            - \"page_idx\": (int) the index of the page.\n            - \"document_name\": (str) the name of the document.\n            - \"file_path\": (str) the local file path where the PDF is stored.\n            - \"file_url\": (str) the URL of the PDF file.\n\n        Args:\n            page_idx (int): The index of the page to process.\n            **kwargs: Additional keyword arguments to be passed to `PyPDF2.PdfReader.pages[0].extract_text`.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the processed page data.\n        \"\"\"\n        with open(self.document_file_path, \"rb\") as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            page = pdf_reader.pages[page_idx]\n            text = page.extract_text(**kwargs)\n\n        return {\n            \"text\": text,\n            \"page_idx\": page_idx,\n            \"document_name\": self.document_name,\n            \"file_path\": self.document_file_path,\n            \"file_url\": self.url,\n        }\n</code></pre>"},{"location":"rag/document_loader/text_loader/pypdf2_text_loader/#medrag_multi_modal.document_loader.text_loader.pypdf2_text_loader.PyPDF2TextLoader.extract_page_data","title":"<code>extract_page_data(page_idx, **kwargs)</code>  <code>async</code>","text":"<p>Process a single page of the PDF and extract its text using PyPDF2.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary with the processed page data.</p> <code>Dict[str, str]</code> <p>The dictionary will have the following keys and values:</p> <code>Dict[str, str]</code> <ul> <li>\"text\": (str) the extracted text from the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"page_idx\": (int) the index of the page.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"document_name\": (str) the name of the document.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_path\": (str) the local file path where the PDF is stored.</li> </ul> <code>Dict[str, str]</code> <ul> <li>\"file_url\": (str) the URL of the PDF file.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>page_idx</code> <code>int</code> <p>The index of the page to process.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>PyPDF2.PdfReader.pages[0].extract_text</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary containing the processed page data.</p> Source code in <code>medrag_multi_modal/document_loader/text_loader/pypdf2_text_loader.py</code> <pre><code>async def extract_page_data(self, page_idx: int, **kwargs) -&gt; Dict[str, str]:\n    \"\"\"\n    Process a single page of the PDF and extract its text using PyPDF2.\n\n    Returns:\n        Dict[str, str]: A dictionary with the processed page data.\n        The dictionary will have the following keys and values:\n\n        - \"text\": (str) the extracted text from the page.\n        - \"page_idx\": (int) the index of the page.\n        - \"document_name\": (str) the name of the document.\n        - \"file_path\": (str) the local file path where the PDF is stored.\n        - \"file_url\": (str) the URL of the PDF file.\n\n    Args:\n        page_idx (int): The index of the page to process.\n        **kwargs: Additional keyword arguments to be passed to `PyPDF2.PdfReader.pages[0].extract_text`.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the processed page data.\n    \"\"\"\n    with open(self.document_file_path, \"rb\") as file:\n        pdf_reader = PyPDF2.PdfReader(file)\n        page = pdf_reader.pages[page_idx]\n        text = page.extract_text(**kwargs)\n\n    return {\n        \"text\": text,\n        \"page_idx\": page_idx,\n        \"document_name\": self.document_name,\n        \"file_path\": self.document_file_path,\n        \"file_url\": self.url,\n    }\n</code></pre>"},{"location":"rag/evals/eval/","title":"Evaluation","text":"<p>We make use of the Weave Evaluation API to evaluate the performance of different stochastic components in the pipeline.</p> <p>Evaluations can be run using the CLI <code>medrag evaluate</code>.</p> <pre><code>\u25b6 medrag evaluate --help\nusage: medrag evaluate [-h] [--test-file TEST_FILE] [--test-case TEST_CASE]\n\noptions:\n  -h, --help            show this help message and exit\n  --test-file TEST_FILE\n                        Path to test file\n  --test-case TEST_CASE\n                        Only run tests which match the given substring expression\n  --model-name MODEL_NAME\n                        Model name to use for evaluation\n</code></pre>"},{"location":"rag/evals/metrics/base/","title":"Base Metrics","text":""},{"location":"rag/evals/metrics/base/#medrag_multi_modal.metrics.base.BaseAccuracyMetric","title":"<code>BaseAccuracyMetric</code>","text":"<p>               Bases: <code>Scorer</code></p> <p>BaseAccuracyMetric is a class that extends the <code>weave.Scorer</code> to provide a comprehensive evaluation of accuracy metrics for a given set of score rows.</p> <p>This class is designed to process a list of score rows, each containing a 'correct' key that indicates whether a particular prediction was correct. The <code>summarize</code> method calculates various statistical measures and metrics based on this data, including:</p> <ul> <li>True and false counts: The number of true and false predictions.</li> <li>True and false fractions: The proportion of true and false predictions.</li> <li>Standard error: The standard error of the mean for the true predictions.</li> <li>Precision: The ratio of true positive predictions to the total number of   positive predictions.</li> <li>Recall: The ratio of true positive predictions to the total number of   actual positives.</li> <li>F1 Score: The harmonic mean of precision and recall, providing a balance   between the two metrics.</li> </ul> <p>The <code>summarize</code> method returns a dictionary containing these metrics, allowing for a detailed analysis of the model's performance.</p> <p>Methods:</p> Name Description <code>summarize</code> <p>list) -&gt; Optional[dict]: Processes the input score rows to compute and return a dictionary of accuracy metrics.</p> Source code in <code>medrag_multi_modal/metrics/base.py</code> <pre><code>class BaseAccuracyMetric(weave.Scorer):\n    \"\"\"\n    BaseAccuracyMetric is a class that extends the\n    [`weave.Scorer`](https://weave-docs.wandb.ai/guides/evaluation/scorers#class-based-scorers)\n    to provide a comprehensive evaluation of accuracy metrics for a given set of score rows.\n\n    This class is designed to process a list of score rows, each containing a\n    'correct' key that indicates whether a particular prediction was correct.\n    The `summarize` method calculates various statistical measures and metrics\n    based on this data, including:\n\n    - True and false counts: The number of true and false predictions.\n    - True and false fractions: The proportion of true and false predictions.\n    - Standard error: The standard error of the mean for the true predictions.\n    - Precision: The ratio of true positive predictions to the total number of\n      positive predictions.\n    - Recall: The ratio of true positive predictions to the total number of\n      actual positives.\n    - F1 Score: The harmonic mean of precision and recall, providing a balance\n      between the two metrics.\n\n    The `summarize` method returns a dictionary containing these metrics,\n    allowing for a detailed analysis of the model's performance.\n\n    Methods:\n        summarize(score_rows: list) -&gt; Optional[dict]:\n            Processes the input score rows to compute and return a dictionary\n            of accuracy metrics.\n    \"\"\"\n\n    @weave.op()\n    def summarize(self, score_rows: list) -&gt; Optional[dict]:\n        \"\"\"\n        Summarizes the accuracy metrics from a list of score rows.\n\n        This method processes a list of score rows, each containing a 'correct' key\n        that indicates whether a particular prediction was correct. It calculates\n        various statistical measures and metrics based on this data, including:\n\n        - True and false counts: The number of true and false predictions.\n        - True and false fractions: The proportion of true and false predictions.\n        - Standard error: The standard error of the mean for the true predictions.\n        - Precision: The ratio of true positive predictions to the total number of\n          positive predictions.\n        - Recall: The ratio of true positive predictions to the total number of\n          actual positives.\n        - F1 Score: The harmonic mean of precision and recall, providing a balance\n          between the two metrics.\n\n        The method returns a dictionary containing these metrics, allowing for a\n        detailed analysis of the model's performance.\n\n        Args:\n            score_rows (list): A list of dictionaries, each containing a 'correct'\n                key with a boolean value indicating the correctness of a prediction.\n\n        Returns:\n            Optional[dict]: A dictionary containing the calculated accuracy metrics,\n                or None if the input list is empty.\n        \"\"\"\n        valid_data = [\n            x.get(\"correct\") for x in score_rows if x.get(\"correct\") is not None\n        ]\n        count_true = list(valid_data).count(True)\n        int_data = [int(x) for x in valid_data]\n\n        sample_mean = np.mean(int_data) if int_data else 0\n        sample_variance = np.var(int_data) if int_data else 0\n        sample_error = np.sqrt(sample_variance / len(int_data)) if int_data else 0\n\n        # Calculate precision, recall, and F1 score\n        true_positives = count_true\n        false_positives = len(valid_data) - count_true\n        false_negatives = len(score_rows) - len(valid_data)\n\n        precision = (\n            true_positives / (true_positives + false_positives)\n            if (true_positives + false_positives) &gt; 0\n            else 0\n        )\n        recall = (\n            true_positives / (true_positives + false_negatives)\n            if (true_positives + false_negatives) &gt; 0\n            else 0\n        )\n        f1_score = (\n            (2 * precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0\n        )\n\n        return {\n            \"correct\": {\n                \"true_count\": count_true,\n                \"false_count\": len(score_rows) - count_true,\n                \"true_fraction\": float(sample_mean),\n                \"false_fraction\": 1.0 - float(sample_mean),\n                \"stderr\": float(sample_error),\n                \"precision\": precision,\n                \"recall\": recall,\n                \"f1_score\": f1_score,\n            }\n        }\n</code></pre>"},{"location":"rag/evals/metrics/base/#medrag_multi_modal.metrics.base.BaseAccuracyMetric.summarize","title":"<code>summarize(score_rows)</code>","text":"<p>Summarizes the accuracy metrics from a list of score rows.</p> <p>This method processes a list of score rows, each containing a 'correct' key that indicates whether a particular prediction was correct. It calculates various statistical measures and metrics based on this data, including:</p> <ul> <li>True and false counts: The number of true and false predictions.</li> <li>True and false fractions: The proportion of true and false predictions.</li> <li>Standard error: The standard error of the mean for the true predictions.</li> <li>Precision: The ratio of true positive predictions to the total number of   positive predictions.</li> <li>Recall: The ratio of true positive predictions to the total number of   actual positives.</li> <li>F1 Score: The harmonic mean of precision and recall, providing a balance   between the two metrics.</li> </ul> <p>The method returns a dictionary containing these metrics, allowing for a detailed analysis of the model's performance.</p> <p>Parameters:</p> Name Type Description Default <code>score_rows</code> <code>list</code> <p>A list of dictionaries, each containing a 'correct' key with a boolean value indicating the correctness of a prediction.</p> required <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>Optional[dict]: A dictionary containing the calculated accuracy metrics, or None if the input list is empty.</p> Source code in <code>medrag_multi_modal/metrics/base.py</code> <pre><code>@weave.op()\ndef summarize(self, score_rows: list) -&gt; Optional[dict]:\n    \"\"\"\n    Summarizes the accuracy metrics from a list of score rows.\n\n    This method processes a list of score rows, each containing a 'correct' key\n    that indicates whether a particular prediction was correct. It calculates\n    various statistical measures and metrics based on this data, including:\n\n    - True and false counts: The number of true and false predictions.\n    - True and false fractions: The proportion of true and false predictions.\n    - Standard error: The standard error of the mean for the true predictions.\n    - Precision: The ratio of true positive predictions to the total number of\n      positive predictions.\n    - Recall: The ratio of true positive predictions to the total number of\n      actual positives.\n    - F1 Score: The harmonic mean of precision and recall, providing a balance\n      between the two metrics.\n\n    The method returns a dictionary containing these metrics, allowing for a\n    detailed analysis of the model's performance.\n\n    Args:\n        score_rows (list): A list of dictionaries, each containing a 'correct'\n            key with a boolean value indicating the correctness of a prediction.\n\n    Returns:\n        Optional[dict]: A dictionary containing the calculated accuracy metrics,\n            or None if the input list is empty.\n    \"\"\"\n    valid_data = [\n        x.get(\"correct\") for x in score_rows if x.get(\"correct\") is not None\n    ]\n    count_true = list(valid_data).count(True)\n    int_data = [int(x) for x in valid_data]\n\n    sample_mean = np.mean(int_data) if int_data else 0\n    sample_variance = np.var(int_data) if int_data else 0\n    sample_error = np.sqrt(sample_variance / len(int_data)) if int_data else 0\n\n    # Calculate precision, recall, and F1 score\n    true_positives = count_true\n    false_positives = len(valid_data) - count_true\n    false_negatives = len(score_rows) - len(valid_data)\n\n    precision = (\n        true_positives / (true_positives + false_positives)\n        if (true_positives + false_positives) &gt; 0\n        else 0\n    )\n    recall = (\n        true_positives / (true_positives + false_negatives)\n        if (true_positives + false_negatives) &gt; 0\n        else 0\n    )\n    f1_score = (\n        (2 * precision * recall) / (precision + recall)\n        if (precision + recall) &gt; 0\n        else 0\n    )\n\n    return {\n        \"correct\": {\n            \"true_count\": count_true,\n            \"false_count\": len(score_rows) - count_true,\n            \"true_fraction\": float(sample_mean),\n            \"false_fraction\": 1.0 - float(sample_mean),\n            \"stderr\": float(sample_error),\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1_score,\n        }\n    }\n</code></pre>"},{"location":"rag/evals/metrics/mmlu/","title":"MMLU Metrics","text":""},{"location":"rag/evals/metrics/mmlu/#medrag_multi_modal.metrics.mmlu.MMLUOptionAccuracy","title":"<code>MMLUOptionAccuracy</code>","text":"<p>               Bases: <code>BaseAccuracyMetric</code></p> <p>MMLUOptionAccuracy is a metric class that inherits from <code>BaseAccuracyMetric</code>.</p> <p>This class is designed to evaluate the accuracy of a multiple-choice question response by comparing the provided answer with the correct answer from the given options. It uses the MedQAResponse schema to extract the response and checks if it matches the correct answer.</p>"},{"location":"rag/evals/metrics/mmlu/#medrag_multi_modal.metrics.mmlu.MMLUOptionAccuracy--methods","title":"Methods:","text":"<p>score(output: MedQAResponse, options: list[str], answer: str) -&gt; dict:     Compares the provided answer with the correct answer and returns a     dictionary indicating whether the answer is correct.</p> Source code in <code>medrag_multi_modal/metrics/mmlu.py</code> <pre><code>class MMLUOptionAccuracy(BaseAccuracyMetric):\n    \"\"\"\n    MMLUOptionAccuracy is a metric class that inherits from `BaseAccuracyMetric`.\n\n    This class is designed to evaluate the accuracy of a multiple-choice question\n    response by comparing the provided answer with the correct answer from the\n    given options. It uses the MedQAResponse schema to extract the response\n    and checks if it matches the correct answer.\n\n    Methods:\n    --------\n    score(output: MedQAResponse, options: list[str], answer: str) -&gt; dict:\n        Compares the provided answer with the correct answer and returns a\n        dictionary indicating whether the answer is correct.\n    \"\"\"\n\n    @weave.op()\n    def score(self, output: MedQAResponse, options: list[str], answer: str):\n        return {\"correct\": options[answer] == output.response.answer}\n</code></pre>"},{"location":"rag/retrieval/colpali/","title":"ColPali Retrieval","text":""},{"location":"rag/retrieval/colpali/#medrag_multi_modal.retrieval.colpali_retrieval.CalPaliRetriever","title":"<code>CalPaliRetriever</code>","text":"<p>               Bases: <code>Model</code></p> <p>CalPaliRetriever is a class that facilitates the retrieval of page images using ColPali.</p> <p>This class leverages the <code>byaldi.RAGMultiModalModel</code> to perform document retrieval tasks. It can be initialized with a pre-trained model or from a specified W&amp;B artifact. The class also provides methods to index new data and to predict/retrieve documents based on a query.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model to be used for retrieval.</p> Source code in <code>medrag_multi_modal/retrieval/colpali_retrieval.py</code> <pre><code>class CalPaliRetriever(weave.Model):\n    \"\"\"\n    CalPaliRetriever is a class that facilitates the retrieval of page images using ColPali.\n\n    This class leverages the `byaldi.RAGMultiModalModel` to perform document retrieval tasks.\n    It can be initialized with a pre-trained model or from a specified W&amp;B artifact. The class\n    also provides methods to index new data and to predict/retrieve documents based on a query.\n\n    Attributes:\n        model_name (str): The name of the model to be used for retrieval.\n    \"\"\"\n\n    model_name: str\n    _docs_retrieval_model: Optional[\"RAGMultiModalModel\"] = None\n    _metadata: Optional[dict] = None\n    _data_artifact_dir: Optional[str] = None\n\n    def __init__(\n        self,\n        model_name: str = \"vidore/colpali-v1.2\",\n        docs_retrieval_model: Optional[\"RAGMultiModalModel\"] = None,\n        data_artifact_dir: Optional[str] = None,\n        metadata_dataset_name: Optional[str] = None,\n    ):\n        super().__init__(model_name=model_name)\n        from byaldi import RAGMultiModalModel\n\n        self._docs_retrieval_model = (\n            docs_retrieval_model or RAGMultiModalModel.from_pretrained(self.model_name)\n        )\n        self._data_artifact_dir = data_artifact_dir\n        self._metadata = (\n            [dict(row) for row in weave.ref(metadata_dataset_name).get().rows]\n            if metadata_dataset_name\n            else None\n        )\n\n    def index(self, data_artifact_name: str, weave_dataset_name: str, index_name: str):\n        \"\"\"\n        Indexes a dataset of documents and saves the index as a Weave artifact.\n\n        This method retrieves a dataset of documents from a Weave artifact using the provided\n        data artifact name. It then indexes the documents using the document retrieval model\n        and assigns the specified index name. The index is stored locally without storing the\n        collection with the index and overwrites any existing index with the same name.\n\n        If a Weave run is active, the method creates a new Weave artifact with the specified\n        index name and type \"colpali-index\". It adds the local index directory to the artifact\n        and saves it to Weave, including metadata with the provided Weave dataset name.\n\n        !!! example \"Indexing Data\"\n            First you need to install `Byaldi` library by Answer.ai.\n\n            ```bash\n            uv pip install Byaldi&gt;=0.0.5\n            ```\n\n            Next, you can index the data by running the following code:\n\n            ```python\n            import wandb\n            from medrag_multi_modal.retrieval import CalPaliRetriever\n\n            wandb.init(project=\"medrag-multi-modal\", entity=\"ml-colabs\", job_type=\"index\")\n            retriever = CalPaliRetriever()\n            retriever.index(\n                data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n                weave_dataset_name=\"grays-anatomy-images:v0\",\n                index_name=\"grays-anatomy\",\n            )\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            data_artifact_name (str): The name of the Weave artifact containing the dataset.\n            weave_dataset_name (str): The name of the Weave dataset to include in the artifact metadata.\n            index_name (str): The name to assign to the created index.\n        \"\"\"\n        data_artifact_dir = get_wandb_artifact(data_artifact_name, \"dataset\")\n        self._docs_retrieval_model.index(\n            input_path=data_artifact_dir,\n            index_name=index_name,\n            store_collection_with_index=False,\n            overwrite=True,\n        )\n        if wandb.run:\n            artifact = wandb.Artifact(\n                name=index_name,\n                type=\"colpali-index\",\n                metadata={\"weave_dataset_name\": weave_dataset_name},\n            )\n            artifact.add_dir(\n                local_path=os.path.join(\".byaldi\", index_name), name=\"index\"\n            )\n            artifact.save()\n\n    @classmethod\n    def from_wandb_artifact(\n        cls,\n        index_artifact_name: str,\n        metadata_dataset_name: str,\n        data_artifact_name: str,\n    ):\n        \"\"\"\n        Creates an instance of the class from Weights &amp; Biases (wandb) artifacts.\n\n        This method retrieves the necessary artifacts from wandb to initialize the\n        ColPaliRetriever. It fetches the index artifact directory and the data artifact\n        directory using the provided artifact names. It then loads the document retrieval\n        model from the index path within the index artifact directory. Finally, it returns\n        an instance of the class initialized with the retrieved document retrieval model,\n        metadata dataset name, and data artifact directory.\n\n        !!! example \"Retrieving Documents\"\n            First you need to install `Byaldi` library by Answer.ai.\n\n            ```bash\n            uv pip install Byaldi&gt;=0.0.5\n            ```\n\n            Next, you can retrieve the documents by running the following code:\n\n            ```python\n            import weave\n\n            import wandb\n            from medrag_multi_modal.retrieval import CalPaliRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = CalPaliRetriever.from_wandb_artifact(\n                index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n                metadata_dataset_name=\"grays-anatomy-images:v0\",\n                data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n            )\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            index_artifact_name (str): The name of the wandb artifact containing the index.\n            metadata_dataset_name (str): The name of the dataset containing metadata.\n            data_artifact_name (str): The name of the wandb artifact containing the data.\n\n        Returns:\n            An instance of the class initialized with the retrieved document retrieval model,\n            metadata dataset name, and data artifact directory.\n        \"\"\"\n        from byaldi import RAGMultiModalModel\n\n        index_artifact_dir = get_wandb_artifact(index_artifact_name, \"colpali-index\")\n        data_artifact_dir = get_wandb_artifact(data_artifact_name, \"dataset\")\n        docs_retrieval_model = RAGMultiModalModel.from_index(\n            index_path=os.path.join(index_artifact_dir, \"index\")\n        )\n        return cls(\n            docs_retrieval_model=docs_retrieval_model,\n            metadata_dataset_name=metadata_dataset_name,\n            data_artifact_dir=data_artifact_dir,\n        )\n\n    @weave.op()\n    def predict(self, query: str, top_k: int = 3) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Predicts and retrieves the top-k most relevant documents/images for a given query\n        using ColPali.\n\n        This function uses the document retrieval model to search for the most relevant\n        documents based on the provided query. It returns a list of dictionaries, each\n        containing the document image, document ID, and the relevance score.\n\n        !!! example \"Retrieving Documents\"\n            First you need to install `Byaldi` library by Answer.ai.\n\n            ```bash\n            uv pip install Byaldi&gt;=0.0.5\n            ```\n\n            Next, you can retrieve the documents by running the following code:\n\n            ```python\n            import weave\n\n            import wandb\n            from medrag_multi_modal.retrieval import CalPaliRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = CalPaliRetriever.from_wandb_artifact(\n                index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n                metadata_dataset_name=\"grays-anatomy-images:v0\",\n                data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n            )\n            retriever.predict(\n                query=\"which neurotransmitters convey information between Merkel cells and sensory afferents?\",\n                top_k=3,\n            )\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            query (str): The search query string.\n            top_k (int, optional): The number of top results to retrieve. Defaults to 10.\n\n        Returns:\n            list[dict[str, Any]]: A list of dictionaries where each dictionary contains:\n                - \"doc_image\" (PIL.Image.Image): The image of the document.\n                - \"doc_id\" (str): The ID of the document.\n                - \"score\" (float): The relevance score of the document.\n        \"\"\"\n        results = self._docs_retrieval_model.search(query=query, k=top_k)\n        retrieved_results = []\n        for result in results:\n            retrieved_results.append(\n                {\n                    \"doc_image\": Image.open(\n                        os.path.join(self._data_artifact_dir, f\"{result['doc_id']}.png\")\n                    ),\n                    \"doc_id\": result[\"doc_id\"],\n                    \"score\": result[\"score\"],\n                }\n            )\n        return retrieved_results\n</code></pre>"},{"location":"rag/retrieval/colpali/#medrag_multi_modal.retrieval.colpali_retrieval.CalPaliRetriever.from_wandb_artifact","title":"<code>from_wandb_artifact(index_artifact_name, metadata_dataset_name, data_artifact_name)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class from Weights &amp; Biases (wandb) artifacts.</p> <p>This method retrieves the necessary artifacts from wandb to initialize the ColPaliRetriever. It fetches the index artifact directory and the data artifact directory using the provided artifact names. It then loads the document retrieval model from the index path within the index artifact directory. Finally, it returns an instance of the class initialized with the retrieved document retrieval model, metadata dataset name, and data artifact directory.</p> <p>Retrieving Documents</p> <p>First you need to install <code>Byaldi</code> library by Answer.ai.</p> <pre><code>uv pip install Byaldi&gt;=0.0.5\n</code></pre> <p>Next, you can retrieve the documents by running the following code:</p> <pre><code>import weave\n\nimport wandb\nfrom medrag_multi_modal.retrieval import CalPaliRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = CalPaliRetriever.from_wandb_artifact(\n    index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n    metadata_dataset_name=\"grays-anatomy-images:v0\",\n    data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n)\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for ColPali by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index_artifact_name</code> <code>str</code> <p>The name of the wandb artifact containing the index.</p> required <code>metadata_dataset_name</code> <code>str</code> <p>The name of the dataset containing metadata.</p> required <code>data_artifact_name</code> <code>str</code> <p>The name of the wandb artifact containing the data.</p> required <p>Returns:</p> Type Description <p>An instance of the class initialized with the retrieved document retrieval model,</p> <p>metadata dataset name, and data artifact directory.</p> Source code in <code>medrag_multi_modal/retrieval/colpali_retrieval.py</code> <pre><code>@classmethod\ndef from_wandb_artifact(\n    cls,\n    index_artifact_name: str,\n    metadata_dataset_name: str,\n    data_artifact_name: str,\n):\n    \"\"\"\n    Creates an instance of the class from Weights &amp; Biases (wandb) artifacts.\n\n    This method retrieves the necessary artifacts from wandb to initialize the\n    ColPaliRetriever. It fetches the index artifact directory and the data artifact\n    directory using the provided artifact names. It then loads the document retrieval\n    model from the index path within the index artifact directory. Finally, it returns\n    an instance of the class initialized with the retrieved document retrieval model,\n    metadata dataset name, and data artifact directory.\n\n    !!! example \"Retrieving Documents\"\n        First you need to install `Byaldi` library by Answer.ai.\n\n        ```bash\n        uv pip install Byaldi&gt;=0.0.5\n        ```\n\n        Next, you can retrieve the documents by running the following code:\n\n        ```python\n        import weave\n\n        import wandb\n        from medrag_multi_modal.retrieval import CalPaliRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = CalPaliRetriever.from_wandb_artifact(\n            index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n            metadata_dataset_name=\"grays-anatomy-images:v0\",\n            data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n        )\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        index_artifact_name (str): The name of the wandb artifact containing the index.\n        metadata_dataset_name (str): The name of the dataset containing metadata.\n        data_artifact_name (str): The name of the wandb artifact containing the data.\n\n    Returns:\n        An instance of the class initialized with the retrieved document retrieval model,\n        metadata dataset name, and data artifact directory.\n    \"\"\"\n    from byaldi import RAGMultiModalModel\n\n    index_artifact_dir = get_wandb_artifact(index_artifact_name, \"colpali-index\")\n    data_artifact_dir = get_wandb_artifact(data_artifact_name, \"dataset\")\n    docs_retrieval_model = RAGMultiModalModel.from_index(\n        index_path=os.path.join(index_artifact_dir, \"index\")\n    )\n    return cls(\n        docs_retrieval_model=docs_retrieval_model,\n        metadata_dataset_name=metadata_dataset_name,\n        data_artifact_dir=data_artifact_dir,\n    )\n</code></pre>"},{"location":"rag/retrieval/colpali/#medrag_multi_modal.retrieval.colpali_retrieval.CalPaliRetriever.index","title":"<code>index(data_artifact_name, weave_dataset_name, index_name)</code>","text":"<p>Indexes a dataset of documents and saves the index as a Weave artifact.</p> <p>This method retrieves a dataset of documents from a Weave artifact using the provided data artifact name. It then indexes the documents using the document retrieval model and assigns the specified index name. The index is stored locally without storing the collection with the index and overwrites any existing index with the same name.</p> <p>If a Weave run is active, the method creates a new Weave artifact with the specified index name and type \"colpali-index\". It adds the local index directory to the artifact and saves it to Weave, including metadata with the provided Weave dataset name.</p> <p>Indexing Data</p> <p>First you need to install <code>Byaldi</code> library by Answer.ai.</p> <pre><code>uv pip install Byaldi&gt;=0.0.5\n</code></pre> <p>Next, you can index the data by running the following code:</p> <pre><code>import wandb\nfrom medrag_multi_modal.retrieval import CalPaliRetriever\n\nwandb.init(project=\"medrag-multi-modal\", entity=\"ml-colabs\", job_type=\"index\")\nretriever = CalPaliRetriever()\nretriever.index(\n    data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n    weave_dataset_name=\"grays-anatomy-images:v0\",\n    index_name=\"grays-anatomy\",\n)\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for ColPali by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_artifact_name</code> <code>str</code> <p>The name of the Weave artifact containing the dataset.</p> required <code>weave_dataset_name</code> <code>str</code> <p>The name of the Weave dataset to include in the artifact metadata.</p> required <code>index_name</code> <code>str</code> <p>The name to assign to the created index.</p> required Source code in <code>medrag_multi_modal/retrieval/colpali_retrieval.py</code> <pre><code>def index(self, data_artifact_name: str, weave_dataset_name: str, index_name: str):\n    \"\"\"\n    Indexes a dataset of documents and saves the index as a Weave artifact.\n\n    This method retrieves a dataset of documents from a Weave artifact using the provided\n    data artifact name. It then indexes the documents using the document retrieval model\n    and assigns the specified index name. The index is stored locally without storing the\n    collection with the index and overwrites any existing index with the same name.\n\n    If a Weave run is active, the method creates a new Weave artifact with the specified\n    index name and type \"colpali-index\". It adds the local index directory to the artifact\n    and saves it to Weave, including metadata with the provided Weave dataset name.\n\n    !!! example \"Indexing Data\"\n        First you need to install `Byaldi` library by Answer.ai.\n\n        ```bash\n        uv pip install Byaldi&gt;=0.0.5\n        ```\n\n        Next, you can index the data by running the following code:\n\n        ```python\n        import wandb\n        from medrag_multi_modal.retrieval import CalPaliRetriever\n\n        wandb.init(project=\"medrag-multi-modal\", entity=\"ml-colabs\", job_type=\"index\")\n        retriever = CalPaliRetriever()\n        retriever.index(\n            data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n            weave_dataset_name=\"grays-anatomy-images:v0\",\n            index_name=\"grays-anatomy\",\n        )\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        data_artifact_name (str): The name of the Weave artifact containing the dataset.\n        weave_dataset_name (str): The name of the Weave dataset to include in the artifact metadata.\n        index_name (str): The name to assign to the created index.\n    \"\"\"\n    data_artifact_dir = get_wandb_artifact(data_artifact_name, \"dataset\")\n    self._docs_retrieval_model.index(\n        input_path=data_artifact_dir,\n        index_name=index_name,\n        store_collection_with_index=False,\n        overwrite=True,\n    )\n    if wandb.run:\n        artifact = wandb.Artifact(\n            name=index_name,\n            type=\"colpali-index\",\n            metadata={\"weave_dataset_name\": weave_dataset_name},\n        )\n        artifact.add_dir(\n            local_path=os.path.join(\".byaldi\", index_name), name=\"index\"\n        )\n        artifact.save()\n</code></pre>"},{"location":"rag/retrieval/colpali/#medrag_multi_modal.retrieval.colpali_retrieval.CalPaliRetriever.predict","title":"<code>predict(query, top_k=3)</code>","text":"<p>Predicts and retrieves the top-k most relevant documents/images for a given query using ColPali.</p> <p>This function uses the document retrieval model to search for the most relevant documents based on the provided query. It returns a list of dictionaries, each containing the document image, document ID, and the relevance score.</p> <p>Retrieving Documents</p> <p>First you need to install <code>Byaldi</code> library by Answer.ai.</p> <pre><code>uv pip install Byaldi&gt;=0.0.5\n</code></pre> <p>Next, you can retrieve the documents by running the following code:</p> <pre><code>import weave\n\nimport wandb\nfrom medrag_multi_modal.retrieval import CalPaliRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = CalPaliRetriever.from_wandb_artifact(\n    index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n    metadata_dataset_name=\"grays-anatomy-images:v0\",\n    data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n)\nretriever.predict(\n    query=\"which neurotransmitters convey information between Merkel cells and sensory afferents?\",\n    top_k=3,\n)\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for ColPali by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>top_k</code> <code>int</code> <p>The number of top results to retrieve. Defaults to 10.</p> <code>3</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: A list of dictionaries where each dictionary contains: - \"doc_image\" (PIL.Image.Image): The image of the document. - \"doc_id\" (str): The ID of the document. - \"score\" (float): The relevance score of the document.</p> Source code in <code>medrag_multi_modal/retrieval/colpali_retrieval.py</code> <pre><code>@weave.op()\ndef predict(self, query: str, top_k: int = 3) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Predicts and retrieves the top-k most relevant documents/images for a given query\n    using ColPali.\n\n    This function uses the document retrieval model to search for the most relevant\n    documents based on the provided query. It returns a list of dictionaries, each\n    containing the document image, document ID, and the relevance score.\n\n    !!! example \"Retrieving Documents\"\n        First you need to install `Byaldi` library by Answer.ai.\n\n        ```bash\n        uv pip install Byaldi&gt;=0.0.5\n        ```\n\n        Next, you can retrieve the documents by running the following code:\n\n        ```python\n        import weave\n\n        import wandb\n        from medrag_multi_modal.retrieval import CalPaliRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = CalPaliRetriever.from_wandb_artifact(\n            index_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy:v0\",\n            metadata_dataset_name=\"grays-anatomy-images:v0\",\n            data_artifact_name=\"ml-colabs/medrag-multi-modal/grays-anatomy-images:v1\",\n        )\n        retriever.predict(\n            query=\"which neurotransmitters convey information between Merkel cells and sensory afferents?\",\n            top_k=3,\n        )\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for ColPali by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        query (str): The search query string.\n        top_k (int, optional): The number of top results to retrieve. Defaults to 10.\n\n    Returns:\n        list[dict[str, Any]]: A list of dictionaries where each dictionary contains:\n            - \"doc_image\" (PIL.Image.Image): The image of the document.\n            - \"doc_id\" (str): The ID of the document.\n            - \"score\" (float): The relevance score of the document.\n    \"\"\"\n    results = self._docs_retrieval_model.search(query=query, k=top_k)\n    retrieved_results = []\n    for result in results:\n        retrieved_results.append(\n            {\n                \"doc_image\": Image.open(\n                    os.path.join(self._data_artifact_dir, f\"{result['doc_id']}.png\")\n                ),\n                \"doc_id\": result[\"doc_id\"],\n                \"score\": result[\"score\"],\n            }\n        )\n    return retrieved_results\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/bm25s/","title":"BM25-Sparse Retrieval","text":""},{"location":"rag/retrieval/text_retrieval/bm25s/#medrag_multi_modal.retrieval.text_retrieval.bm25s_retrieval.BM25sRetriever","title":"<code>BM25sRetriever</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>BM25sRetriever</code> is a class that provides functionality for indexing and retrieving documents using the BM25-Sparse.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>The language of the documents to be indexed and retrieved.</p> <code>'english'</code> <code>use_stemmer</code> <code>bool</code> <p>A flag indicating whether to use stemming during tokenization.</p> <code>True</code> <code>retriever</code> <code>Optional[BM25]</code> <p>An instance of the BM25 retriever. If not provided, a new instance is created.</p> <code>None</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/bm25s_retrieval.py</code> <pre><code>class BM25sRetriever(weave.Model):\n    \"\"\"\n    `BM25sRetriever` is a class that provides functionality for indexing and\n    retrieving documents using the [BM25-Sparse](https://github.com/xhluca/bm25s).\n\n    Args:\n        language (str): The language of the documents to be indexed and retrieved.\n        use_stemmer (bool): A flag indicating whether to use stemming during tokenization.\n        retriever (Optional[bm25s.BM25]): An instance of the BM25 retriever. If not provided,\n            a new instance is created.\n    \"\"\"\n\n    language: Optional[str]\n    use_stemmer: bool = True\n    _retriever: Optional[BM25]\n\n    def __init__(\n        self,\n        language: str = \"english\",\n        use_stemmer: bool = True,\n        retriever: Optional[BM25] = None,\n    ):\n        super().__init__(language=language, use_stemmer=use_stemmer)\n        self._retriever = retriever or BM25()\n\n    def index(\n        self,\n        chunk_dataset: Union[Dataset, str],\n        chunk_dataset_split: str,\n        index_repo_id: Optional[str] = None,\n        cleanup: bool = True,\n    ):\n        \"\"\"\n        Indexes a dataset of text chunks using the BM25 algorithm.\n\n        This method retrieves a dataset of text chunks from a specified source, tokenizes\n        the text using the BM25 tokenizer with optional stemming, and indexes the tokenized\n        text using the BM25 retriever. If an `index_repo_id` is provided, the index is saved\n        to disk and optionally logged as a Huggingface artifact.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = BM25sRetriever()\n            retriever.index(\n                chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n                index_repo_id=\"geekyrakshit/grays-anatomy-index\",\n            )\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            chunk_dataset_split (str): The split of the dataset to be indexed.\n            index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n            cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n        \"\"\"\n        chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        corpus = [row[\"text\"] for row in chunk_dataset]\n        corpus_tokens = bm25s.tokenize(\n            corpus,\n            stopwords=LANGUAGE_DICT[self.language],\n            stemmer=Stemmer(self.language) if self.use_stemmer else None,\n        )\n        self._retriever.index(corpus_tokens)\n        if index_repo_id:\n            os.makedirs(\".huggingface\", exist_ok=True)\n            index_save_dir = os.path.join(\".huggingface\", index_repo_id.split(\"/\")[-1])\n            self._retriever.save(\n                index_save_dir, corpus=[dict(row) for row in chunk_dataset]\n            )\n            commit_type = (\n                \"update\"\n                if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                else \"add\"\n            )\n            with open(os.path.join(index_save_dir, \"config.json\"), \"w\") as config_file:\n                json.dump(\n                    {\n                        \"language\": self.language,\n                        \"use_stemmer\": self.use_stemmer,\n                    },\n                    config_file,\n                    indent=4,\n                )\n            save_to_huggingface(\n                index_repo_id,\n                index_save_dir,\n                commit_message=f\"{commit_type}: BM25s index\",\n            )\n            if cleanup:\n                shutil.rmtree(index_save_dir)\n\n    @classmethod\n    def from_index(\n        cls,\n        index_repo_id: str,\n    ):\n        \"\"\"\n        Creates an instance of the class from a Huggingface repository.\n\n        This class method retrieves a BM25 index artifact from a Huggingface repository,\n        downloads the artifact, and loads the BM25 retriever with the index and its\n        associated corpus. The method also extracts metadata from the artifact to\n        initialize the class instance with the appropriate language and stemming\n        settings.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = BM25sRetriever()\n            retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n            ```\n\n        Args:\n            index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n\n        Returns:\n            An instance of the class initialized with the BM25 retriever and metadata\n            from the artifact.\n        \"\"\"\n        index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n        retriever = bm25s.BM25.load(index_dir, load_corpus=True)\n        with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n            config = json.load(config_file)\n        return cls(retriever=retriever, **config)\n\n    @weave.op()\n    def retrieve(self, query: str, top_k: int = 2):\n        \"\"\"\n        Retrieves the top-k most relevant chunks for a given query using the BM25 algorithm.\n\n        This method tokenizes the input query using the BM25 tokenizer, which takes into\n        account the language-specific stopwords and optional stemming. It then retrieves\n        the top-k most relevant chunks from the BM25 index based on the tokenized query.\n        The results are returned as a list of dictionaries, each containing a chunk and\n        its corresponding relevance score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = BM25sRetriever()\n            retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n            retrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its\n                relevance score.\n        \"\"\"\n        query_tokens = bm25s.tokenize(\n            query,\n            stopwords=LANGUAGE_DICT[self.language],\n            stemmer=Stemmer(self.language) if self.use_stemmer else None,\n        )\n        results = self._retriever.retrieve(query_tokens, k=top_k)\n        retrieved_chunks = []\n        for chunk, score in zip(\n            results.documents.flatten().tolist(),\n            results.scores.flatten().tolist(),\n        ):\n            retrieved_chunks.append({**chunk, **{\"score\": score}})\n        return retrieved_chunks\n\n    @weave.op()\n    def predict(self, query: str, top_k: int = 2):\n        \"\"\"\n        Predicts the top-k most relevant chunks for a given query using the BM25 algorithm.\n\n        This function is a wrapper around the `retrieve` method. It takes an input query string,\n        tokenizes it using the BM25 tokenizer, and retrieves the top-k most relevant chunks from\n        the BM25 index. The results are returned as a list of dictionaries, each containing a chunk\n        and its corresponding relevance score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = BM25sRetriever()\n            retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n            retrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        return self.retrieve(query, top_k)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/bm25s/#medrag_multi_modal.retrieval.text_retrieval.bm25s_retrieval.BM25sRetriever.from_index","title":"<code>from_index(index_repo_id)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class from a Huggingface repository.</p> <p>This class method retrieves a BM25 index artifact from a Huggingface repository, downloads the artifact, and loads the BM25 retriever with the index and its associated corpus. The method also extracts metadata from the artifact to initialize the class instance with the appropriate language and stemming settings.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = BM25sRetriever()\nretriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index_repo_id</code> <code>Optional[str]</code> <p>The Huggingface repository of the index artifact to be saved.</p> required <p>Returns:</p> Type Description <p>An instance of the class initialized with the BM25 retriever and metadata</p> <p>from the artifact.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/bm25s_retrieval.py</code> <pre><code>@classmethod\ndef from_index(\n    cls,\n    index_repo_id: str,\n):\n    \"\"\"\n    Creates an instance of the class from a Huggingface repository.\n\n    This class method retrieves a BM25 index artifact from a Huggingface repository,\n    downloads the artifact, and loads the BM25 retriever with the index and its\n    associated corpus. The method also extracts metadata from the artifact to\n    initialize the class instance with the appropriate language and stemming\n    settings.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = BM25sRetriever()\n        retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n        ```\n\n    Args:\n        index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n\n    Returns:\n        An instance of the class initialized with the BM25 retriever and metadata\n        from the artifact.\n    \"\"\"\n    index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n    retriever = bm25s.BM25.load(index_dir, load_corpus=True)\n    with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n        config = json.load(config_file)\n    return cls(retriever=retriever, **config)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/bm25s/#medrag_multi_modal.retrieval.text_retrieval.bm25s_retrieval.BM25sRetriever.index","title":"<code>index(chunk_dataset, chunk_dataset_split, index_repo_id=None, cleanup=True)</code>","text":"<p>Indexes a dataset of text chunks using the BM25 algorithm.</p> <p>This method retrieves a dataset of text chunks from a specified source, tokenizes the text using the BM25 tokenizer with optional stemming, and indexes the tokenized text using the BM25 retriever. If an <code>index_repo_id</code> is provided, the index is saved to disk and optionally logged as a Huggingface artifact.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = BM25sRetriever()\nretriever.index(\n    chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n    index_repo_id=\"geekyrakshit/grays-anatomy-index\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>chunk_dataset_split</code> <code>str</code> <p>The split of the dataset to be indexed.</p> required <code>index_repo_id</code> <code>Optional[str]</code> <p>The Huggingface repository of the index artifact to be saved.</p> <code>None</code> <code>cleanup</code> <code>bool</code> <p>Whether to delete the local index directory after saving the vector index.</p> <code>True</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/bm25s_retrieval.py</code> <pre><code>def index(\n    self,\n    chunk_dataset: Union[Dataset, str],\n    chunk_dataset_split: str,\n    index_repo_id: Optional[str] = None,\n    cleanup: bool = True,\n):\n    \"\"\"\n    Indexes a dataset of text chunks using the BM25 algorithm.\n\n    This method retrieves a dataset of text chunks from a specified source, tokenizes\n    the text using the BM25 tokenizer with optional stemming, and indexes the tokenized\n    text using the BM25 retriever. If an `index_repo_id` is provided, the index is saved\n    to disk and optionally logged as a Huggingface artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = BM25sRetriever()\n        retriever.index(\n            chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n            index_repo_id=\"geekyrakshit/grays-anatomy-index\",\n        )\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        chunk_dataset_split (str): The split of the dataset to be indexed.\n        index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n        cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n    \"\"\"\n    chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    corpus = [row[\"text\"] for row in chunk_dataset]\n    corpus_tokens = bm25s.tokenize(\n        corpus,\n        stopwords=LANGUAGE_DICT[self.language],\n        stemmer=Stemmer(self.language) if self.use_stemmer else None,\n    )\n    self._retriever.index(corpus_tokens)\n    if index_repo_id:\n        os.makedirs(\".huggingface\", exist_ok=True)\n        index_save_dir = os.path.join(\".huggingface\", index_repo_id.split(\"/\")[-1])\n        self._retriever.save(\n            index_save_dir, corpus=[dict(row) for row in chunk_dataset]\n        )\n        commit_type = (\n            \"update\"\n            if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n            else \"add\"\n        )\n        with open(os.path.join(index_save_dir, \"config.json\"), \"w\") as config_file:\n            json.dump(\n                {\n                    \"language\": self.language,\n                    \"use_stemmer\": self.use_stemmer,\n                },\n                config_file,\n                indent=4,\n            )\n        save_to_huggingface(\n            index_repo_id,\n            index_save_dir,\n            commit_message=f\"{commit_type}: BM25s index\",\n        )\n        if cleanup:\n            shutil.rmtree(index_save_dir)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/bm25s/#medrag_multi_modal.retrieval.text_retrieval.bm25s_retrieval.BM25sRetriever.predict","title":"<code>predict(query, top_k=2)</code>","text":"<p>Predicts the top-k most relevant chunks for a given query using the BM25 algorithm.</p> <p>This function is a wrapper around the <code>retrieve</code> method. It takes an input query string, tokenizes it using the BM25 tokenizer, and retrieves the top-k most relevant chunks from the BM25 index. The results are returned as a list of dictionaries, each containing a chunk and its corresponding relevance score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = BM25sRetriever()\nretriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\nretrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/bm25s_retrieval.py</code> <pre><code>@weave.op()\ndef predict(self, query: str, top_k: int = 2):\n    \"\"\"\n    Predicts the top-k most relevant chunks for a given query using the BM25 algorithm.\n\n    This function is a wrapper around the `retrieve` method. It takes an input query string,\n    tokenizes it using the BM25 tokenizer, and retrieves the top-k most relevant chunks from\n    the BM25 index. The results are returned as a list of dictionaries, each containing a chunk\n    and its corresponding relevance score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = BM25sRetriever()\n        retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n        retrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    return self.retrieve(query, top_k)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/bm25s/#medrag_multi_modal.retrieval.text_retrieval.bm25s_retrieval.BM25sRetriever.retrieve","title":"<code>retrieve(query, top_k=2)</code>","text":"<p>Retrieves the top-k most relevant chunks for a given query using the BM25 algorithm.</p> <p>This method tokenizes the input query using the BM25 tokenizer, which takes into account the language-specific stopwords and optional stemming. It then retrieves the top-k most relevant chunks from the BM25 index based on the tokenized query. The results are returned as a list of dictionaries, each containing a chunk and its corresponding relevance score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = BM25sRetriever()\nretriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\nretrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/bm25s_retrieval.py</code> <pre><code>@weave.op()\ndef retrieve(self, query: str, top_k: int = 2):\n    \"\"\"\n    Retrieves the top-k most relevant chunks for a given query using the BM25 algorithm.\n\n    This method tokenizes the input query using the BM25 tokenizer, which takes into\n    account the language-specific stopwords and optional stemming. It then retrieves\n    the top-k most relevant chunks from the BM25 index based on the tokenized query.\n    The results are returned as a list of dictionaries, each containing a chunk and\n    its corresponding relevance score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import BM25sRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = BM25sRetriever()\n        retriever = BM25sRetriever().from_index(index_repo_id=\"geekyrakshit/grays-anatomy-index\")\n        retrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its\n            relevance score.\n    \"\"\"\n    query_tokens = bm25s.tokenize(\n        query,\n        stopwords=LANGUAGE_DICT[self.language],\n        stemmer=Stemmer(self.language) if self.use_stemmer else None,\n    )\n    results = self._retriever.retrieve(query_tokens, k=top_k)\n    retrieved_chunks = []\n    for chunk, score in zip(\n        results.documents.flatten().tolist(),\n        results.scores.flatten().tolist(),\n    ):\n        retrieved_chunks.append({**chunk, **{\"score\": score}})\n    return retrieved_chunks\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/contriever/","title":"Contriever Retrieval","text":""},{"location":"rag/retrieval/text_retrieval/contriever/#medrag_multi_modal.retrieval.text_retrieval.contriever_retrieval.ContrieverRetriever","title":"<code>ContrieverRetriever</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>ContrieverRetriever</code> is a class to perform retrieval tasks using the Contriever model.</p> <p>It provides methods to encode text data into embeddings, index a dataset of text chunks, and retrieve the most relevant chunks for a given query based on similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained model to use for encoding.</p> <code>'facebook/contriever'</code> <code>vector_index</code> <code>Optional[Tensor]</code> <p>The tensor containing the vector representations of the indexed chunks.</p> <code>None</code> <code>chunk_dataset</code> <code>Optional[list[dict]]</code> <p>The weave dataset of text chunks to be indexed.</p> <code>None</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/contriever_retrieval.py</code> <pre><code>class ContrieverRetriever(weave.Model):\n    \"\"\"\n    `ContrieverRetriever` is a class to perform retrieval tasks using the Contriever model.\n\n    It provides methods to encode text data into embeddings, index a dataset of text chunks,\n    and retrieve the most relevant chunks for a given query based on similarity metrics.\n\n    Args:\n        model_name (str): The name of the pre-trained model to use for encoding.\n        vector_index (Optional[torch.Tensor]): The tensor containing the vector representations\n            of the indexed chunks.\n        chunk_dataset (Optional[list[dict]]): The weave dataset of text chunks to be indexed.\n    \"\"\"\n\n    model_name: str\n    _chunk_dataset: Optional[list[dict]]\n    _tokenizer: PreTrainedTokenizerFast\n    _model: BertPreTrainedModel\n    _vector_index: Optional[torch.Tensor]\n\n    def __init__(\n        self,\n        model_name: str = \"facebook/contriever\",\n        vector_index: Optional[torch.Tensor] = None,\n        chunk_dataset: Optional[list[dict]] = None,\n    ):\n        super().__init__(model_name=model_name)\n        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self._model = AutoModel.from_pretrained(self.model_name).to(get_torch_backend())\n        self._vector_index = vector_index\n        self._chunk_dataset = chunk_dataset\n\n    def encode(\n        self, corpus: list[str], batch_size: int, streamlit_mode: bool = False\n    ) -&gt; torch.Tensor:\n        embeddings = []\n        iterable = (\n            track(\n                range(0, len(corpus), batch_size),\n                description=f\"Encoding corpus using {self.model_name}\",\n            )\n            if batch_size &gt; 1\n            else range(0, len(corpus), batch_size)\n        )\n        streamlit_progressbar = (\n            st.progress(\n                0,\n                text=\"Indexing batches\",\n            )\n            if streamlit_mode and batch_size &gt; 1\n            else None\n        )\n        batch_idx = 1\n        for idx in iterable:\n            batch = corpus[idx : idx + batch_size]\n            inputs = self._tokenizer(\n                batch, padding=True, truncation=True, return_tensors=\"pt\"\n            ).to(get_torch_backend())\n            with torch.no_grad():\n                outputs = self._model(**inputs)\n                batch_embeddings = mean_pooling(outputs[0], inputs[\"attention_mask\"])\n                embeddings.append(batch_embeddings)\n            if streamlit_progressbar:\n                progress_percentage = min(\n                    100, max(0, int(((idx + batch_size) / len(corpus)) * 100))\n                )\n                total = (len(corpus) // batch_size) + 1\n                streamlit_progressbar.progress(\n                    progress_percentage,\n                    text=f\"Indexing batch ({batch_idx}/{total})\",\n                )\n                batch_idx += 1\n        embeddings = torch.cat(embeddings, dim=0)\n        return embeddings\n\n    def index(\n        self,\n        chunk_dataset: Union[str, Dataset],\n        chunk_dataset_split: str,\n        index_repo_id: Optional[str] = None,\n        cleanup: bool = True,\n        batch_size: int = 32,\n        streamlit_mode: bool = False,\n    ):\n        \"\"\"\n        Indexes a dataset of text chunks and optionally saves the vector index to a file.\n\n        This method retrieves a dataset of text chunks from a Weave reference, encodes the\n        text chunks into vector representations using the Contriever model, and stores the\n        resulting vector index. If an index name is provided, the vector index is saved to\n        a file in the safetensors format. Additionally, if a Weave run is active, the vector\n        index file is logged as an artifact to Weave.\n\n        !!! example \"Example Usage\"\n            ```python\n            from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n            retriever = ContrieverRetriever()\n            retriever.index(\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-contriever\",\n                batch_size=256,\n            )\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n            cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n            batch_size (int, optional): The batch size to use for encoding the corpus.\n            streamlit_mode (bool): Whether to use streamlit mode.\n        \"\"\"\n        self._chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        corpus = [row[\"text\"] for row in self._chunk_dataset]\n        with torch.no_grad():\n            vector_index = self.encode(corpus, batch_size, streamlit_mode)\n            self._vector_index = vector_index\n            if index_repo_id:\n                index_save_dir = os.path.join(\n                    \".huggingface\", index_repo_id.split(\"/\")[-1]\n                )\n                os.makedirs(index_save_dir, exist_ok=True)\n                safetensors.torch.save_file(\n                    {\"vector_index\": vector_index.cpu()},\n                    os.path.join(index_save_dir, \"vector_index.safetensors\"),\n                )\n                commit_type = (\n                    \"update\"\n                    if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                    else \"add\"\n                )\n                with open(\n                    os.path.join(index_save_dir, \"config.json\"), \"w\"\n                ) as config_file:\n                    json.dump(\n                        {\"model_name\": self.model_name},\n                        config_file,\n                        indent=4,\n                    )\n                save_to_huggingface(\n                    index_repo_id,\n                    index_save_dir,\n                    commit_message=f\"{commit_type}: Contriever index\",\n                )\n                if cleanup:\n                    shutil.rmtree(index_save_dir)\n\n    @classmethod\n    def from_index(\n        cls,\n        chunk_dataset: Union[str, Dataset],\n        index_repo_id: str,\n        chunk_dataset_split: Optional[str] = None,\n    ):\n        \"\"\"\n        Creates an instance of the class from a Weave artifact.\n\n        This method retrieves a vector index and metadata from a Weave artifact stored in\n        Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave\n        reference. The vector index is loaded from a safetensors file and moved to the\n        appropriate device (CPU or GPU). The text chunks are converted into a list of\n        dictionaries. The method then returns an instance of the class initialized with\n        the retrieved model name, vector index, and chunk dataset.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n            load_dotenv()\n            retriever = ContrieverRetriever().from_index(\n                index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n                chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n            )\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n            chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n        Returns:\n            An instance of the class initialized with the retrieved model name, vector index,\n            and chunk dataset.\n        \"\"\"\n        index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n        with safetensors.torch.safe_open(\n            os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n        ) as f:\n            vector_index = f.get_tensor(\"vector_index\")\n        device = torch.device(get_torch_backend())\n        vector_index = vector_index.to(device)\n        chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n            metadata = json.load(config_file)\n        return cls(\n            model_name=metadata[\"model_name\"],\n            vector_index=vector_index,\n            chunk_dataset=chunk_dataset,\n        )\n\n    @weave.op()\n    def retrieve(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method encodes the input query into an embedding and computes similarity scores between\n        the query embedding and the precomputed vector index. The similarity metric can be either\n        cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n        are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = ContrieverRetriever().from_index(\n                index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n                chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n            )\n            retrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        query = [query]\n        device = torch.device(get_torch_backend())\n        with torch.no_grad():\n            query_embedding = self.encode(query, batch_size=1).to(device)\n            if metric == SimilarityMetric.EUCLIDEAN:\n                scores = torch.squeeze(query_embedding @ self._vector_index.T)\n            else:\n                scores = F.cosine_similarity(query_embedding, self._vector_index)\n            scores = scores.cpu().numpy().tolist()\n        scores = argsort_scores(scores, descending=True)[:top_k]\n        retrieved_chunks = []\n        for score in scores:\n            retrieved_chunks.append(\n                {\n                    **self._chunk_dataset[score[\"original_index\"]],\n                    **{\"score\": score[\"item\"]},\n                }\n            )\n        return retrieved_chunks\n\n    @weave.op()\n    def predict(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This function is a wrapper around the `retrieve` method. It takes an input query string,\n        retrieves the top-k most relevant chunks from the precomputed vector index based on the\n        specified similarity metric, and returns the results as a list of dictionaries, each containing\n        a chunk and its corresponding relevance score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n            load_dotenv()\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = ContrieverRetriever().from_index(\n                index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n                chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n            )\n            retrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/contriever/#medrag_multi_modal.retrieval.text_retrieval.contriever_retrieval.ContrieverRetriever.from_index","title":"<code>from_index(chunk_dataset, index_repo_id, chunk_dataset_split=None)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class from a Weave artifact.</p> <p>This method retrieves a vector index and metadata from a Weave artifact stored in Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave reference. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The text chunks are converted into a list of dictionaries. The method then returns an instance of the class initialized with the retrieved model name, vector index, and chunk dataset.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\nload_dotenv()\nretriever = ContrieverRetriever().from_index(\n    index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n    chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>str</code> <p>The Huggingface repository of the index artifact to be saved.</p> required <code>chunk_dataset_split</code> <code>Optional[str]</code> <p>The split of the dataset to be indexed.</p> <code>None</code> <p>Returns:</p> Type Description <p>An instance of the class initialized with the retrieved model name, vector index,</p> <p>and chunk dataset.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/contriever_retrieval.py</code> <pre><code>@classmethod\ndef from_index(\n    cls,\n    chunk_dataset: Union[str, Dataset],\n    index_repo_id: str,\n    chunk_dataset_split: Optional[str] = None,\n):\n    \"\"\"\n    Creates an instance of the class from a Weave artifact.\n\n    This method retrieves a vector index and metadata from a Weave artifact stored in\n    Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave\n    reference. The vector index is loaded from a safetensors file and moved to the\n    appropriate device (CPU or GPU). The text chunks are converted into a list of\n    dictionaries. The method then returns an instance of the class initialized with\n    the retrieved model name, vector index, and chunk dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n        load_dotenv()\n        retriever = ContrieverRetriever().from_index(\n            index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n            chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n        )\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n        chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n    Returns:\n        An instance of the class initialized with the retrieved model name, vector index,\n        and chunk dataset.\n    \"\"\"\n    index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n    with safetensors.torch.safe_open(\n        os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n    ) as f:\n        vector_index = f.get_tensor(\"vector_index\")\n    device = torch.device(get_torch_backend())\n    vector_index = vector_index.to(device)\n    chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n        metadata = json.load(config_file)\n    return cls(\n        model_name=metadata[\"model_name\"],\n        vector_index=vector_index,\n        chunk_dataset=chunk_dataset,\n    )\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/contriever/#medrag_multi_modal.retrieval.text_retrieval.contriever_retrieval.ContrieverRetriever.index","title":"<code>index(chunk_dataset, chunk_dataset_split, index_repo_id=None, cleanup=True, batch_size=32, streamlit_mode=False)</code>","text":"<p>Indexes a dataset of text chunks and optionally saves the vector index to a file.</p> <p>This method retrieves a dataset of text chunks from a Weave reference, encodes the text chunks into vector representations using the Contriever model, and stores the resulting vector index. If an index name is provided, the vector index is saved to a file in the safetensors format. Additionally, if a Weave run is active, the vector index file is logged as an artifact to Weave.</p> <p>Example Usage</p> <pre><code>from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\nretriever = ContrieverRetriever()\nretriever.index(\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-contriever\",\n    batch_size=256,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>Optional[str]</code> <p>The Huggingface repository of the index artifact to be saved.</p> <code>None</code> <code>cleanup</code> <code>bool</code> <p>Whether to delete the local index directory after saving the vector index.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for encoding the corpus.</p> <code>32</code> <code>streamlit_mode</code> <code>bool</code> <p>Whether to use streamlit mode.</p> <code>False</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/contriever_retrieval.py</code> <pre><code>def index(\n    self,\n    chunk_dataset: Union[str, Dataset],\n    chunk_dataset_split: str,\n    index_repo_id: Optional[str] = None,\n    cleanup: bool = True,\n    batch_size: int = 32,\n    streamlit_mode: bool = False,\n):\n    \"\"\"\n    Indexes a dataset of text chunks and optionally saves the vector index to a file.\n\n    This method retrieves a dataset of text chunks from a Weave reference, encodes the\n    text chunks into vector representations using the Contriever model, and stores the\n    resulting vector index. If an index name is provided, the vector index is saved to\n    a file in the safetensors format. Additionally, if a Weave run is active, the vector\n    index file is logged as an artifact to Weave.\n\n    !!! example \"Example Usage\"\n        ```python\n        from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n        retriever = ContrieverRetriever()\n        retriever.index(\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-contriever\",\n            batch_size=256,\n        )\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n        cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n        batch_size (int, optional): The batch size to use for encoding the corpus.\n        streamlit_mode (bool): Whether to use streamlit mode.\n    \"\"\"\n    self._chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    corpus = [row[\"text\"] for row in self._chunk_dataset]\n    with torch.no_grad():\n        vector_index = self.encode(corpus, batch_size, streamlit_mode)\n        self._vector_index = vector_index\n        if index_repo_id:\n            index_save_dir = os.path.join(\n                \".huggingface\", index_repo_id.split(\"/\")[-1]\n            )\n            os.makedirs(index_save_dir, exist_ok=True)\n            safetensors.torch.save_file(\n                {\"vector_index\": vector_index.cpu()},\n                os.path.join(index_save_dir, \"vector_index.safetensors\"),\n            )\n            commit_type = (\n                \"update\"\n                if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                else \"add\"\n            )\n            with open(\n                os.path.join(index_save_dir, \"config.json\"), \"w\"\n            ) as config_file:\n                json.dump(\n                    {\"model_name\": self.model_name},\n                    config_file,\n                    indent=4,\n                )\n            save_to_huggingface(\n                index_repo_id,\n                index_save_dir,\n                commit_message=f\"{commit_type}: Contriever index\",\n            )\n            if cleanup:\n                shutil.rmtree(index_save_dir)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/contriever/#medrag_multi_modal.retrieval.text_retrieval.contriever_retrieval.ContrieverRetriever.predict","title":"<code>predict(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Predicts the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This function is a wrapper around the <code>retrieve</code> method. It takes an input query string, retrieves the top-k most relevant chunks from the precomputed vector index based on the specified similarity metric, and returns the results as a list of dictionaries, each containing a chunk and its corresponding relevance score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = ContrieverRetriever().from_index(\n    index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n    chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n)\nretrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring. Defaults to cosine similarity.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/contriever_retrieval.py</code> <pre><code>@weave.op()\ndef predict(\n    self,\n    query: str,\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n    This function is a wrapper around the `retrieve` method. It takes an input query string,\n    retrieves the top-k most relevant chunks from the precomputed vector index based on the\n    specified similarity metric, and returns the results as a list of dictionaries, each containing\n    a chunk and its corresponding relevance score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = ContrieverRetriever().from_index(\n            index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n            chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n        )\n        retrieved_chunks = retriever.predict(query=\"What are Ribosomes?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/contriever/#medrag_multi_modal.retrieval.text_retrieval.contriever_retrieval.ContrieverRetriever.retrieve","title":"<code>retrieve(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This method encodes the input query into an embedding and computes similarity scores between the query embedding and the precomputed vector index. The similarity metric can be either cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores are returned as a list of dictionaries, each containing a chunk and its corresponding score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\nload_dotenv()\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = ContrieverRetriever().from_index(\n    index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n    chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n)\nretrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/contriever_retrieval.py</code> <pre><code>@weave.op()\ndef retrieve(\n    self,\n    query: str,\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n    This method encodes the input query into an embedding and computes similarity scores between\n    the query embedding and the precomputed vector index. The similarity metric can be either\n    cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n    are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import ContrieverRetriever\n\n        load_dotenv()\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = ContrieverRetriever().from_index(\n            index_repo_id=\"geekyrakshit/grays-anatomy-index-contriever\",\n            chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n        )\n        retrieved_chunks = retriever.retrieve(query=\"What are Ribosomes?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    query = [query]\n    device = torch.device(get_torch_backend())\n    with torch.no_grad():\n        query_embedding = self.encode(query, batch_size=1).to(device)\n        if metric == SimilarityMetric.EUCLIDEAN:\n            scores = torch.squeeze(query_embedding @ self._vector_index.T)\n        else:\n            scores = F.cosine_similarity(query_embedding, self._vector_index)\n        scores = scores.cpu().numpy().tolist()\n    scores = argsort_scores(scores, descending=True)[:top_k]\n    retrieved_chunks = []\n    for score in scores:\n        retrieved_chunks.append(\n            {\n                **self._chunk_dataset[score[\"original_index\"]],\n                **{\"score\": score[\"item\"]},\n            }\n        )\n    return retrieved_chunks\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/medcpt/","title":"MedCPT Retrieval","text":""},{"location":"rag/retrieval/text_retrieval/medcpt/#medrag_multi_modal.retrieval.text_retrieval.medcpt_retrieval.MedCPTRetriever","title":"<code>MedCPTRetriever</code>","text":"<p>               Bases: <code>Model</code></p> <p>A class to retrieve relevant text chunks using MedCPT models.</p> <p>This class provides methods to index a dataset of text chunks and retrieve the most relevant chunks for a given query using MedCPT models. It uses separate models for encoding queries and articles, and supports both cosine similarity and Euclidean distance as similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>query_encoder_model_name</code> <code>str</code> <p>The name of the model used for encoding queries.</p> <code>'ncbi/MedCPT-Query-Encoder'</code> <code>article_encoder_model_name</code> <code>str</code> <p>The name of the model used for encoding articles.</p> <code>'ncbi/MedCPT-Article-Encoder'</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The maximum length of text chunks.</p> <code>512</code> <code>vector_index</code> <code>Optional[Tensor]</code> <p>The vector index of encoded text chunks.</p> <code>None</code> <code>chunk_dataset</code> <code>Optional[list[dict]]</code> <p>The dataset of text chunks.</p> <code>None</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/medcpt_retrieval.py</code> <pre><code>class MedCPTRetriever(weave.Model):\n    \"\"\"\n    A class to retrieve relevant text chunks using MedCPT models.\n\n    This class provides methods to index a dataset of text chunks and retrieve the most relevant\n    chunks for a given query using MedCPT models. It uses separate models for encoding queries\n    and articles, and supports both cosine similarity and Euclidean distance as similarity metrics.\n\n    Args:\n        query_encoder_model_name (str): The name of the model used for encoding queries.\n        article_encoder_model_name (str): The name of the model used for encoding articles.\n        chunk_size (Optional[int]): The maximum length of text chunks.\n        vector_index (Optional[torch.Tensor]): The vector index of encoded text chunks.\n        chunk_dataset (Optional[list[dict]]): The dataset of text chunks.\n    \"\"\"\n\n    query_encoder_model_name: str\n    article_encoder_model_name: str\n    chunk_size: Optional[int]\n    _chunk_dataset: Optional[list[dict]]\n    _query_tokenizer: PreTrainedTokenizerFast\n    _article_tokenizer: PreTrainedTokenizerFast\n    _query_encoder_model: BertPreTrainedModel\n    _article_encoder_model: BertPreTrainedModel\n    _vector_index: Optional[torch.Tensor]\n\n    def __init__(\n        self,\n        query_encoder_model_name: str = \"ncbi/MedCPT-Query-Encoder\",\n        article_encoder_model_name: str = \"ncbi/MedCPT-Article-Encoder\",\n        chunk_size: Optional[int] = 512,\n        vector_index: Optional[torch.Tensor] = None,\n        chunk_dataset: Optional[list[dict]] = None,\n    ):\n        super().__init__(\n            query_encoder_model_name=query_encoder_model_name,\n            article_encoder_model_name=article_encoder_model_name,\n            chunk_size=chunk_size,\n        )\n        self._query_tokenizer = AutoTokenizer.from_pretrained(\n            self.query_encoder_model_name, max_length=self.chunk_size\n        )\n        self._article_tokenizer = AutoTokenizer.from_pretrained(\n            self.article_encoder_model_name, max_length=self.chunk_size\n        )\n        self._query_encoder_model = AutoModel.from_pretrained(\n            self.query_encoder_model_name\n        ).to(get_torch_backend())\n        self._article_encoder_model = AutoModel.from_pretrained(\n            self.article_encoder_model_name\n        ).to(get_torch_backend())\n        self._chunk_dataset = chunk_dataset\n        self._vector_index = vector_index\n\n    def index(\n        self,\n        chunk_dataset: Union[str, Dataset],\n        chunk_dataset_split: str,\n        index_repo_id: Optional[str] = None,\n        cleanup: bool = True,\n        batch_size: int = 32,\n        streamlit_mode: bool = False,\n    ):\n        \"\"\"\n        Indexes a dataset of text chunks using the MedCPT model and optionally saves the vector index.\n\n        This method retrieves a dataset of text chunks from a specified source, encodes the text\n        chunks into vector representations using the article encoder model, and stores the\n        resulting vector index. If an `index_repo_id` is provided, the vector index is saved\n        to disk in the safetensors format and optionally logged as a Huggingface artifact.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from dotenv import load_dotenv\n\n            from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n            load_dotenv()\n            retriever = MedCPTRetriever()\n            retriever.index(\n                chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n                index_repo_id=\"geekyrakshit/grays-anatomy-index-medcpt\",\n            )\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n            cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n            batch_size (int, optional): The batch size to use for encoding the corpus.\n            streamlit_mode (bool): Whether or not this function is being called inside a streamlit app or not.\n        \"\"\"\n        self._chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        corpus = [row[\"text\"] for row in self._chunk_dataset]\n        vector_indices = []\n        streamlit_progressbar = (\n            st.progress(\n                0,\n                text=\"Indexing batches\",\n            )\n            if streamlit_mode and batch_size &gt; 1\n            else None\n        )\n        batch_idx = 1\n        with torch.no_grad():\n            for idx in track(\n                range(0, len(corpus), batch_size),\n                description=\"Encoding corpus using MedCPT\",\n            ):\n                batch = corpus[idx : idx + batch_size]\n                encoded = self._article_tokenizer(\n                    batch,\n                    truncation=True,\n                    padding=True,\n                    return_tensors=\"pt\",\n                    max_length=self.chunk_size,\n                ).to(get_torch_backend())\n                batch_vectors = (\n                    self._article_encoder_model(**encoded)\n                    .last_hidden_state[:, 0, :]\n                    .contiguous()\n                )\n                vector_indices.append(batch_vectors)\n                if streamlit_progressbar:\n                    progress_percentage = min(\n                        100, max(0, int(((idx + batch_size) / len(corpus)) * 100))\n                    )\n                    total = (len(corpus) // batch_size) + 1\n                    streamlit_progressbar.progress(\n                        progress_percentage,\n                        text=f\"Indexing batch ({batch_idx}/{total})\",\n                    )\n                    batch_idx += 1\n\n            vector_index = torch.cat(vector_indices, dim=0)\n            self._vector_index = vector_index\n            if index_repo_id:\n                index_save_dir = os.path.join(\n                    \".huggingface\", index_repo_id.split(\"/\")[-1]\n                )\n                os.makedirs(index_save_dir, exist_ok=True)\n                safetensors.torch.save_file(\n                    {\"vector_index\": self._vector_index.cpu()},\n                    os.path.join(index_save_dir, \"vector_index.safetensors\"),\n                )\n                commit_type = (\n                    \"update\"\n                    if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                    else \"add\"\n                )\n                with open(\n                    os.path.join(index_save_dir, \"config.json\"), \"w\"\n                ) as config_file:\n                    json.dump(\n                        {\n                            \"query_encoder_model_name\": self.query_encoder_model_name,\n                            \"article_encoder_model_name\": self.article_encoder_model_name,\n                            \"chunk_size\": self.chunk_size,\n                        },\n                        config_file,\n                        indent=4,\n                    )\n                save_to_huggingface(\n                    index_repo_id,\n                    index_save_dir,\n                    commit_message=f\"{commit_type}: Contriever index\",\n                )\n                if cleanup:\n                    shutil.rmtree(index_save_dir)\n\n    @classmethod\n    def from_index(\n        cls,\n        chunk_dataset: Union[str, Dataset],\n        index_repo_id: str,\n        chunk_dataset_split: Optional[str] = None,\n    ):\n        \"\"\"\n        Creates an instance of the class from a Huggingface repository.\n\n        This method retrieves a vector index and metadata from a Huggingface repository.\n        It also retrieves a dataset of text chunks from the specified source. The vector\n        index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU).\n        The method then returns an instance of the class initialized with the retrieved\n        model names, vector index, and chunk dataset.\n\n        !!! example \"Example Usage\"\n            ```python\n            from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n            retriever = MedCPTRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n            chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n        Returns:\n            An instance of the class initialized with the retrieved model name, vector index, and chunk dataset.\n        \"\"\"\n        index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n        with safetensors.torch.safe_open(\n            os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n        ) as f:\n            vector_index = f.get_tensor(\"vector_index\")\n        device = torch.device(get_torch_backend())\n        vector_index = vector_index.to(device)\n        with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n            metadata = json.load(config_file)\n        chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        return cls(\n            query_encoder_model_name=metadata[\"query_encoder_model_name\"],\n            article_encoder_model_name=metadata[\"article_encoder_model_name\"],\n            chunk_size=metadata[\"chunk_size\"],\n            vector_index=vector_index,\n            chunk_dataset=chunk_dataset,\n        )\n\n    @weave.op()\n    def retrieve(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method encodes the input query into an embedding and computes similarity scores between\n        the query embedding and the precomputed vector index. The similarity metric can be either\n        cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n        are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = MedCPTRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.retrieve(query=\"What is ribosome?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        query = [query]\n        device = torch.device(get_torch_backend())\n        with torch.no_grad():\n            encoded = self._query_tokenizer(\n                query,\n                truncation=True,\n                padding=True,\n                return_tensors=\"pt\",\n            ).to(device)\n            query_embedding = self._query_encoder_model(**encoded).last_hidden_state[\n                :, 0, :\n            ]\n            query_embedding = query_embedding.to(device)\n            if metric == SimilarityMetric.EUCLIDEAN:\n                scores = torch.squeeze(query_embedding @ self._vector_index.T)\n            else:\n                scores = F.cosine_similarity(query_embedding, self._vector_index)\n            scores = scores.cpu().numpy().tolist()\n        scores = argsort_scores(scores, descending=True)[:top_k]\n        retrieved_chunks = []\n        for score in scores:\n            retrieved_chunks.append(\n                {\n                    **self._chunk_dataset[score[\"original_index\"]],\n                    **{\"score\": score[\"item\"]},\n                }\n            )\n        return retrieved_chunks\n\n    @weave.op()\n    def predict(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Predicts the most relevant chunks for a given query.\n\n        This function uses the `retrieve` method to find the top-k relevant chunks\n        from the dataset based on the input query. It allows specifying the number\n        of top relevant chunks to retrieve and the similarity metric to use for scoring.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = MedCPTRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.predict(query=\"What is ribosome?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/medcpt/#medrag_multi_modal.retrieval.text_retrieval.medcpt_retrieval.MedCPTRetriever.from_index","title":"<code>from_index(chunk_dataset, index_repo_id, chunk_dataset_split=None)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class from a Huggingface repository.</p> <p>This method retrieves a vector index and metadata from a Huggingface repository. It also retrieves a dataset of text chunks from the specified source. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The method then returns an instance of the class initialized with the retrieved model names, vector index, and chunk dataset.</p> <p>Example Usage</p> <pre><code>from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\nretriever = MedCPTRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>str</code> <p>The Huggingface repository of the index artifact to be saved.</p> required <code>chunk_dataset_split</code> <code>Optional[str]</code> <p>The split of the dataset to be indexed.</p> <code>None</code> <p>Returns:</p> Type Description <p>An instance of the class initialized with the retrieved model name, vector index, and chunk dataset.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/medcpt_retrieval.py</code> <pre><code>@classmethod\ndef from_index(\n    cls,\n    chunk_dataset: Union[str, Dataset],\n    index_repo_id: str,\n    chunk_dataset_split: Optional[str] = None,\n):\n    \"\"\"\n    Creates an instance of the class from a Huggingface repository.\n\n    This method retrieves a vector index and metadata from a Huggingface repository.\n    It also retrieves a dataset of text chunks from the specified source. The vector\n    index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU).\n    The method then returns an instance of the class initialized with the retrieved\n    model names, vector index, and chunk dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n        retriever = MedCPTRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n        chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n    Returns:\n        An instance of the class initialized with the retrieved model name, vector index, and chunk dataset.\n    \"\"\"\n    index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n    with safetensors.torch.safe_open(\n        os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n    ) as f:\n        vector_index = f.get_tensor(\"vector_index\")\n    device = torch.device(get_torch_backend())\n    vector_index = vector_index.to(device)\n    with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n        metadata = json.load(config_file)\n    chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    return cls(\n        query_encoder_model_name=metadata[\"query_encoder_model_name\"],\n        article_encoder_model_name=metadata[\"article_encoder_model_name\"],\n        chunk_size=metadata[\"chunk_size\"],\n        vector_index=vector_index,\n        chunk_dataset=chunk_dataset,\n    )\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/medcpt/#medrag_multi_modal.retrieval.text_retrieval.medcpt_retrieval.MedCPTRetriever.index","title":"<code>index(chunk_dataset, chunk_dataset_split, index_repo_id=None, cleanup=True, batch_size=32, streamlit_mode=False)</code>","text":"<p>Indexes a dataset of text chunks using the MedCPT model and optionally saves the vector index.</p> <p>This method retrieves a dataset of text chunks from a specified source, encodes the text chunks into vector representations using the article encoder model, and stores the resulting vector index. If an <code>index_repo_id</code> is provided, the vector index is saved to disk in the safetensors format and optionally logged as a Huggingface artifact.</p> <p>Example Usage</p> <pre><code>import weave\nfrom dotenv import load_dotenv\n\nfrom medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\nload_dotenv()\nretriever = MedCPTRetriever()\nretriever.index(\n    chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n    index_repo_id=\"geekyrakshit/grays-anatomy-index-medcpt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>Optional[str]</code> <p>The Huggingface repository of the index artifact to be saved.</p> <code>None</code> <code>cleanup</code> <code>bool</code> <p>Whether to delete the local index directory after saving the vector index.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for encoding the corpus.</p> <code>32</code> <code>streamlit_mode</code> <code>bool</code> <p>Whether or not this function is being called inside a streamlit app or not.</p> <code>False</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/medcpt_retrieval.py</code> <pre><code>def index(\n    self,\n    chunk_dataset: Union[str, Dataset],\n    chunk_dataset_split: str,\n    index_repo_id: Optional[str] = None,\n    cleanup: bool = True,\n    batch_size: int = 32,\n    streamlit_mode: bool = False,\n):\n    \"\"\"\n    Indexes a dataset of text chunks using the MedCPT model and optionally saves the vector index.\n\n    This method retrieves a dataset of text chunks from a specified source, encodes the text\n    chunks into vector representations using the article encoder model, and stores the\n    resulting vector index. If an `index_repo_id` is provided, the vector index is saved\n    to disk in the safetensors format and optionally logged as a Huggingface artifact.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from dotenv import load_dotenv\n\n        from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n        load_dotenv()\n        retriever = MedCPTRetriever()\n        retriever.index(\n            chunk_dataset=\"geekyrakshit/grays-anatomy-chunks-test\",\n            index_repo_id=\"geekyrakshit/grays-anatomy-index-medcpt\",\n        )\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n        cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n        batch_size (int, optional): The batch size to use for encoding the corpus.\n        streamlit_mode (bool): Whether or not this function is being called inside a streamlit app or not.\n    \"\"\"\n    self._chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    corpus = [row[\"text\"] for row in self._chunk_dataset]\n    vector_indices = []\n    streamlit_progressbar = (\n        st.progress(\n            0,\n            text=\"Indexing batches\",\n        )\n        if streamlit_mode and batch_size &gt; 1\n        else None\n    )\n    batch_idx = 1\n    with torch.no_grad():\n        for idx in track(\n            range(0, len(corpus), batch_size),\n            description=\"Encoding corpus using MedCPT\",\n        ):\n            batch = corpus[idx : idx + batch_size]\n            encoded = self._article_tokenizer(\n                batch,\n                truncation=True,\n                padding=True,\n                return_tensors=\"pt\",\n                max_length=self.chunk_size,\n            ).to(get_torch_backend())\n            batch_vectors = (\n                self._article_encoder_model(**encoded)\n                .last_hidden_state[:, 0, :]\n                .contiguous()\n            )\n            vector_indices.append(batch_vectors)\n            if streamlit_progressbar:\n                progress_percentage = min(\n                    100, max(0, int(((idx + batch_size) / len(corpus)) * 100))\n                )\n                total = (len(corpus) // batch_size) + 1\n                streamlit_progressbar.progress(\n                    progress_percentage,\n                    text=f\"Indexing batch ({batch_idx}/{total})\",\n                )\n                batch_idx += 1\n\n        vector_index = torch.cat(vector_indices, dim=0)\n        self._vector_index = vector_index\n        if index_repo_id:\n            index_save_dir = os.path.join(\n                \".huggingface\", index_repo_id.split(\"/\")[-1]\n            )\n            os.makedirs(index_save_dir, exist_ok=True)\n            safetensors.torch.save_file(\n                {\"vector_index\": self._vector_index.cpu()},\n                os.path.join(index_save_dir, \"vector_index.safetensors\"),\n            )\n            commit_type = (\n                \"update\"\n                if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                else \"add\"\n            )\n            with open(\n                os.path.join(index_save_dir, \"config.json\"), \"w\"\n            ) as config_file:\n                json.dump(\n                    {\n                        \"query_encoder_model_name\": self.query_encoder_model_name,\n                        \"article_encoder_model_name\": self.article_encoder_model_name,\n                        \"chunk_size\": self.chunk_size,\n                    },\n                    config_file,\n                    indent=4,\n                )\n            save_to_huggingface(\n                index_repo_id,\n                index_save_dir,\n                commit_message=f\"{commit_type}: Contriever index\",\n            )\n            if cleanup:\n                shutil.rmtree(index_save_dir)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/medcpt/#medrag_multi_modal.retrieval.text_retrieval.medcpt_retrieval.MedCPTRetriever.predict","title":"<code>predict(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Predicts the most relevant chunks for a given query.</p> <p>This function uses the <code>retrieve</code> method to find the top-k relevant chunks from the dataset based on the input query. It allows specifying the number of top relevant chunks to retrieve and the similarity metric to use for scoring.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = MedCPTRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\nretriever.predict(query=\"What is ribosome?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring. Defaults to cosine similarity.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/medcpt_retrieval.py</code> <pre><code>@weave.op()\ndef predict(\n    self,\n    query: str,\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Predicts the most relevant chunks for a given query.\n\n    This function uses the `retrieve` method to find the top-k relevant chunks\n    from the dataset based on the input query. It allows specifying the number\n    of top relevant chunks to retrieve and the similarity metric to use for scoring.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = MedCPTRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        retriever.predict(query=\"What is ribosome?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/medcpt/#medrag_multi_modal.retrieval.text_retrieval.medcpt_retrieval.MedCPTRetriever.retrieve","title":"<code>retrieve(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This method encodes the input query into an embedding and computes similarity scores between the query embedding and the precomputed vector index. The similarity metric can be either cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores are returned as a list of dictionaries, each containing a chunk and its corresponding score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = MedCPTRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\nretriever.retrieve(query=\"What is ribosome?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve. Defaults to 2.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring. Defaults to cosine similarity.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/medcpt_retrieval.py</code> <pre><code>@weave.op()\ndef retrieve(\n    self,\n    query: str,\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n    This method encodes the input query into an embedding and computes similarity scores between\n    the query embedding and the precomputed vector index. The similarity metric can be either\n    cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n    are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from medrag_multi_modal.retrieval.text_retrieval import MedCPTRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = MedCPTRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-medcpt\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        retriever.retrieve(query=\"What is ribosome?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve. Defaults to 2.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring. Defaults to cosine similarity.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    query = [query]\n    device = torch.device(get_torch_backend())\n    with torch.no_grad():\n        encoded = self._query_tokenizer(\n            query,\n            truncation=True,\n            padding=True,\n            return_tensors=\"pt\",\n        ).to(device)\n        query_embedding = self._query_encoder_model(**encoded).last_hidden_state[\n            :, 0, :\n        ]\n        query_embedding = query_embedding.to(device)\n        if metric == SimilarityMetric.EUCLIDEAN:\n            scores = torch.squeeze(query_embedding @ self._vector_index.T)\n        else:\n            scores = F.cosine_similarity(query_embedding, self._vector_index)\n        scores = scores.cpu().numpy().tolist()\n    scores = argsort_scores(scores, descending=True)[:top_k]\n    retrieved_chunks = []\n    for score in scores:\n        retrieved_chunks.append(\n            {\n                **self._chunk_dataset[score[\"original_index\"]],\n                **{\"score\": score[\"item\"]},\n            }\n        )\n    return retrieved_chunks\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/","title":"Sentence Transformer Retrieval","text":""},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.NVEmbed2Retriever","title":"<code>NVEmbed2Retriever</code>","text":"<p>               Bases: <code>SentenceTransformerRetriever</code></p> <p><code>NVEmbed2Retriever</code> is a class for retrieving relevant text chunks from a dataset using the NV-Embed-v2 model.</p> <p>This class leverages the SentenceTransformer model to encode text chunks into vector representations and performs similarity-based retrieval. It supports indexing a dataset of text chunks, saving the vector index, and retrieving the most relevant chunks for a given query.</p> <p>Parameters:</p> Name Type Description Default <code>vector_index</code> <code>Optional[Tensor]</code> <p>The tensor containing the vector representations of the indexed chunks.</p> <code>None</code> <code>chunk_dataset</code> <code>Optional[list[dict]]</code> <p>The dataset of text chunks to be indexed.</p> <code>None</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>class NVEmbed2Retriever(SentenceTransformerRetriever):\n    \"\"\"\n    `NVEmbed2Retriever` is a class for retrieving relevant text chunks from a dataset using the\n    [NV-Embed-v2](https://huggingface.co/nvidia/NV-Embed-v2) model.\n\n    This class leverages the SentenceTransformer model to encode text chunks into vector representations and\n    performs similarity-based retrieval. It supports indexing a dataset of text chunks, saving the vector index,\n    and retrieving the most relevant chunks for a given query.\n\n    Args:\n        vector_index (Optional[torch.Tensor]): The tensor containing the vector representations of the indexed chunks.\n        chunk_dataset (Optional[list[dict]]): The dataset of text chunks to be indexed.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_index: Optional[torch.Tensor] = None,\n        chunk_dataset: Optional[list[dict]] = None,\n    ):\n        super().__init__(\n            self,\n            model_name=\"nvidia/NV-Embed-v2\",\n            vector_index=vector_index,\n            chunk_dataset=chunk_dataset,\n        )\n\n    @weave.op()\n    def predict(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method formats the input query string by prepending an instruction prompt and then calls the\n        `retrieve` method to get the most relevant chunks. The similarity metric can be either cosine similarity\n        or Euclidean distance. The top-k chunks with the highest similarity scores are returned.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = NVEmbed2Retriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.predict(query=\"What is ribosome?\")\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        query = [\n            f\"\"\"Instruct: Given a question, retrieve passages that answer the question\nQuery: {query}\"\"\"\n        ]\n        return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.NVEmbed2Retriever.predict","title":"<code>predict(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Predicts the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This method formats the input query string by prepending an instruction prompt and then calls the <code>retrieve</code> method to get the most relevant chunks. The similarity metric can be either cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores are returned.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = NVEmbed2Retriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\nretriever.predict(query=\"What is ribosome?\")\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>    @weave.op()\n    def predict(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method formats the input query string by prepending an instruction prompt and then calls the\n        `retrieve` method to get the most relevant chunks. The similarity metric can be either cosine similarity\n        or Euclidean distance. The top-k chunks with the highest similarity scores are returned.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = NVEmbed2Retriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.predict(query=\"What is ribosome?\")\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        query = [\n            f\"\"\"Instruct: Given a question, retrieve passages that answer the question\nQuery: {query}\"\"\"\n        ]\n        return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.SentenceTransformerRetriever","title":"<code>SentenceTransformerRetriever</code>","text":"<p>               Bases: <code>Model</code></p> <p>The <code>SentenceTransformerRetriever</code> class leverages a SentenceTransformer model to encode text chunks into vector representations and performs similarity-based retrieval. It supports indexing a dataset of text chunks, saving the vector index, and retrieving the most relevant chunks for a given query.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the pre-trained model to use for encoding.</p> <code>'nvidia/NV-Embed-v2'</code> <code>vector_index</code> <code>Optional[Tensor]</code> <p>The tensor containing the vector representations of the indexed chunks.</p> <code>None</code> <code>chunk_dataset</code> <code>Optional[list[dict]]</code> <p>The dataset of text chunks to be indexed.</p> <code>None</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>class SentenceTransformerRetriever(weave.Model):\n    \"\"\"\n    The `SentenceTransformerRetriever` class leverages a [SentenceTransformer](https://sbert.net/)\n    model to encode text chunks into vector representations and performs similarity-based retrieval.\n    It supports indexing a dataset of text chunks, saving the vector index, and retrieving the most\n    relevant chunks for a given query.\n\n    Args:\n        model_name (str): The name of the pre-trained model to use for encoding.\n        vector_index (Optional[torch.Tensor]): The tensor containing the vector representations of\n            the indexed chunks.\n        chunk_dataset (Optional[list[dict]]): The dataset of text chunks to be indexed.\n    \"\"\"\n\n    model_name: str\n    _chunk_dataset: Optional[list[dict]]\n    _model: SentenceTransformer\n    _vector_index: Optional[torch.Tensor]\n\n    def __init__(\n        self,\n        model_name: str = \"nvidia/NV-Embed-v2\",\n        vector_index: Optional[torch.Tensor] = None,\n        chunk_dataset: Optional[list[dict]] = None,\n    ):\n        super().__init__(model_name=model_name)\n        self._model = SentenceTransformer(\n            self.model_name,\n            trust_remote_code=True,\n            model_kwargs={\"torch_dtype\": torch.float16},\n            device=get_torch_backend(),\n        )\n        self._model.max_seq_length = 32768\n        self._model.tokenizer.padding_side = \"right\"\n        self._vector_index = vector_index\n        self._chunk_dataset = chunk_dataset\n\n    def add_end_of_sequence_tokens(self, input_examples):\n        input_examples = [\n            input_example + self._model.tokenizer.eos_token\n            for input_example in input_examples\n        ]\n        return input_examples\n\n    def index(\n        self,\n        chunk_dataset: Union[str, Dataset],\n        index_repo_id: Optional[str] = None,\n        cleanup: bool = True,\n        batch_size: int = 8,\n        streamlit_mode: bool = False,\n    ):\n        \"\"\"\n        Indexes a dataset of text chunks and optionally saves the vector index to a Huggingface repository.\n\n        This method retrieves a dataset of text chunks from a specified source, encodes the\n        text chunks into vector representations using the NV-Embed-v2 model, and stores the\n        resulting vector index. If an index repository ID is provided, the vector index is saved to\n        a file in the safetensors format within the specified Huggingface repository.\n\n        !!! example \"Example Usage\"\n            ```python\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            retriever = SentenceTransformerRetriever()\n            retriever.index(\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n            )\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n            cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n            batch_size (int, optional): The batch size to use for encoding the corpus.\n            streamlit_mode (bool): Whether or not this function is being called inside a streamlit app or not.\n        \"\"\"\n        self._chunk_dataset = (\n            load_dataset(chunk_dataset, split=\"chunks\")\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        corpus = [row[\"text\"] for row in self._chunk_dataset]\n        vector_indices = []\n        streamlit_progressbar = (\n            st.progress(\n                0,\n                text=\"Indexing batches\",\n            )\n            if streamlit_mode and batch_size &gt; 1\n            else None\n        )\n        batch_idx = 1\n\n        for idx in track(\n            range(0, len(corpus), batch_size),\n            description=\"Encoding corpus using NV-Embed-v2\",\n        ):\n            batch = corpus[idx : idx + batch_size]\n            batch_embeddings = self._model.encode(\n                self.add_end_of_sequence_tokens(batch),\n                batch_size=len(batch),\n                normalize_embeddings=True,\n            )\n            vector_indices.append(torch.tensor(batch_embeddings))\n            if streamlit_progressbar:\n                progress_percentage = min(\n                    100, max(0, int(((idx + batch_size) / len(corpus)) * 100))\n                )\n                total = (len(corpus) // batch_size) + 1\n                streamlit_progressbar.progress(\n                    progress_percentage,\n                    text=f\"Indexing batch ({batch_idx}/{total})\",\n                )\n                batch_idx += 1\n\n        self._vector_index = torch.cat(vector_indices, dim=0)\n        with torch.no_grad():\n            if index_repo_id:\n                index_save_dir = os.path.join(\n                    \".huggingface\", index_repo_id.split(\"/\")[-1]\n                )\n                os.makedirs(index_save_dir, exist_ok=True)\n                safetensors.torch.save_file(\n                    {\"vector_index\": self._vector_index.cpu()},\n                    os.path.join(index_save_dir, \"vector_index.safetensors\"),\n                )\n                commit_type = (\n                    \"update\"\n                    if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                    else \"add\"\n                )\n                with open(\n                    os.path.join(index_save_dir, \"config.json\"), \"w\"\n                ) as config_file:\n                    json.dump(\n                        {\"model_name\": self.model_name},\n                        config_file,\n                        indent=4,\n                    )\n                save_to_huggingface(\n                    index_repo_id,\n                    index_save_dir,\n                    commit_message=f\"{commit_type}: Contriever index\",\n                )\n                if cleanup:\n                    shutil.rmtree(index_save_dir)\n\n    @classmethod\n    def from_index(\n        cls,\n        chunk_dataset: Union[str, Dataset],\n        index_repo_id: str,\n        chunk_dataset_split: Optional[str] = None,\n    ):\n        \"\"\"\n        Creates an instance of the class from a Huggingface repository.\n\n        This method retrieves a vector index and metadata from a Huggingface repository. It also retrieves a dataset of text chunks from a Huggingface dataset repository. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The text chunks are converted into a list of dictionaries. The method then returns an instance of the class initialized with the retrieved model name, vector index, and chunk dataset.\n        Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave\n        reference. The vector index is loaded from a safetensors file and moved to the\n        appropriate device (CPU or GPU). The text chunks are converted into a list of\n        dictionaries. The method then returns an instance of the class initialized with\n        the retrieved model name, vector index, and chunk dataset.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            retriever = SentenceTransformerRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n                dataset repository name or a dataset object can be provided.\n            index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n            chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n        Returns:\n            An instance of the class initialized with the retrieved model name, vector index,\n            and chunk dataset.\n        \"\"\"\n        index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n        with safetensors.torch.safe_open(\n            os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n        ) as f:\n            vector_index = f.get_tensor(\"vector_index\")\n        device = torch.device(get_torch_backend())\n        vector_index = vector_index.to(device)\n        chunk_dataset = (\n            load_dataset(chunk_dataset, split=chunk_dataset_split)\n            if isinstance(chunk_dataset, str)\n            else chunk_dataset\n        )\n        with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n            metadata = json.load(config_file)\n        return cls(\n            model_name=metadata[\"model_name\"],\n            vector_index=vector_index,\n            chunk_dataset=chunk_dataset,\n        )\n\n    @weave.op()\n    def retrieve(\n        self,\n        query: list[str],\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method encodes the input query into an embedding and computes similarity scores between\n        the query embedding and the precomputed vector index. The similarity metric can be either\n        cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n        are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = SentenceTransformerRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.retrieve(query=\"What is ribosome?\")\n            ```\n\n        ??? note \"Optional Speedup using Flash Attention\"\n            If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n            installing the `flash-attn` package.\n\n            ```bash\n            uv pip install flash-attn --no-build-isolation\n            ```\n\n        Args:\n            query (list[str]): The input query strings to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        device = torch.device(get_torch_backend())\n        with torch.no_grad():\n            query_embedding = self._model.encode(\n                self.add_end_of_sequence_tokens(query), normalize_embeddings=True\n            )\n            query_embedding = torch.from_numpy(query_embedding).to(device)\n            if metric == SimilarityMetric.EUCLIDEAN:\n                scores = torch.squeeze(query_embedding @ self._vector_index.T)\n            else:\n                scores = F.cosine_similarity(query_embedding, self._vector_index)\n            scores = scores.cpu().numpy().tolist()\n        scores = argsort_scores(scores, descending=True)[:top_k]\n        retrieved_chunks = []\n        for score in scores:\n            retrieved_chunks.append(\n                {\n                    **self._chunk_dataset[score[\"original_index\"]],\n                    **{\"score\": score[\"item\"]},\n                }\n            )\n        return retrieved_chunks\n\n    @abstractmethod\n    @weave.op()\n    def predict(\n        self,\n        query: str,\n        top_k: int = 2,\n        metric: SimilarityMetric = SimilarityMetric.COSINE,\n    ):\n        \"\"\"\n        Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n        This method formats the input query string by prepending an instruction prompt and then calls the\n        `retrieve` method to get the most relevant chunks. The similarity metric can be either cosine similarity\n        or Euclidean distance. The top-k chunks with the highest similarity scores are returned.\n\n        !!! example \"Example Usage\"\n            ```python\n            import weave\n            from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n            weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n            retriever = SentenceTransformerRetriever.from_index(\n                index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n                chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            )\n            retriever.predict(query=\"What is ribosome?\")\n            ```\n\n        Args:\n            query (str): The input query string to search for relevant chunks.\n            top_k (int, optional): The number of top relevant chunks to retrieve.\n            metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n        Returns:\n            list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n        \"\"\"\n        return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.SentenceTransformerRetriever.from_index","title":"<code>from_index(chunk_dataset, index_repo_id, chunk_dataset_split=None)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class from a Huggingface repository.</p> <p>This method retrieves a vector index and metadata from a Huggingface repository. It also retrieves a dataset of text chunks from a Huggingface dataset repository. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The text chunks are converted into a list of dictionaries. The method then returns an instance of the class initialized with the retrieved model name, vector index, and chunk dataset. Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave reference. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The text chunks are converted into a list of dictionaries. The method then returns an instance of the class initialized with the retrieved model name, vector index, and chunk dataset.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\nretriever = SentenceTransformerRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>str</code> <p>The Huggingface repository of the index artifact to be saved.</p> required <code>chunk_dataset_split</code> <code>Optional[str]</code> <p>The split of the dataset to be indexed.</p> <code>None</code> <p>Returns:</p> Type Description <p>An instance of the class initialized with the retrieved model name, vector index,</p> <p>and chunk dataset.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>@classmethod\ndef from_index(\n    cls,\n    chunk_dataset: Union[str, Dataset],\n    index_repo_id: str,\n    chunk_dataset_split: Optional[str] = None,\n):\n    \"\"\"\n    Creates an instance of the class from a Huggingface repository.\n\n    This method retrieves a vector index and metadata from a Huggingface repository. It also retrieves a dataset of text chunks from a Huggingface dataset repository. The vector index is loaded from a safetensors file and moved to the appropriate device (CPU or GPU). The text chunks are converted into a list of dictionaries. The method then returns an instance of the class initialized with the retrieved model name, vector index, and chunk dataset.\n    Weights &amp; Biases (wandb). It also retrieves a dataset of text chunks from a Weave\n    reference. The vector index is loaded from a safetensors file and moved to the\n    appropriate device (CPU or GPU). The text chunks are converted into a list of\n    dictionaries. The method then returns an instance of the class initialized with\n    the retrieved model name, vector index, and chunk dataset.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n        retriever = SentenceTransformerRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (str): The Huggingface repository of the index artifact to be saved.\n        chunk_dataset_split (Optional[str]): The split of the dataset to be indexed.\n\n    Returns:\n        An instance of the class initialized with the retrieved model name, vector index,\n        and chunk dataset.\n    \"\"\"\n    index_dir = fetch_from_huggingface(index_repo_id, \".huggingface\")\n    with safetensors.torch.safe_open(\n        os.path.join(index_dir, \"vector_index.safetensors\"), framework=\"pt\"\n    ) as f:\n        vector_index = f.get_tensor(\"vector_index\")\n    device = torch.device(get_torch_backend())\n    vector_index = vector_index.to(device)\n    chunk_dataset = (\n        load_dataset(chunk_dataset, split=chunk_dataset_split)\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    with open(os.path.join(index_dir, \"config.json\"), \"r\") as config_file:\n        metadata = json.load(config_file)\n    return cls(\n        model_name=metadata[\"model_name\"],\n        vector_index=vector_index,\n        chunk_dataset=chunk_dataset,\n    )\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.SentenceTransformerRetriever.index","title":"<code>index(chunk_dataset, index_repo_id=None, cleanup=True, batch_size=8, streamlit_mode=False)</code>","text":"<p>Indexes a dataset of text chunks and optionally saves the vector index to a Huggingface repository.</p> <p>This method retrieves a dataset of text chunks from a specified source, encodes the text chunks into vector representations using the NV-Embed-v2 model, and stores the resulting vector index. If an index repository ID is provided, the vector index is saved to a file in the safetensors format within the specified Huggingface repository.</p> <p>Example Usage</p> <pre><code>from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\nretriever = SentenceTransformerRetriever()\nretriever.index(\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n)\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>chunk_dataset</code> <code>str</code> <p>The Huggingface dataset containing the text chunks to be indexed. Either a dataset repository name or a dataset object can be provided.</p> required <code>index_repo_id</code> <code>Optional[str]</code> <p>The Huggingface repository of the index artifact to be saved.</p> <code>None</code> <code>cleanup</code> <code>bool</code> <p>Whether to delete the local index directory after saving the vector index.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for encoding the corpus.</p> <code>8</code> <code>streamlit_mode</code> <code>bool</code> <p>Whether or not this function is being called inside a streamlit app or not.</p> <code>False</code> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>def index(\n    self,\n    chunk_dataset: Union[str, Dataset],\n    index_repo_id: Optional[str] = None,\n    cleanup: bool = True,\n    batch_size: int = 8,\n    streamlit_mode: bool = False,\n):\n    \"\"\"\n    Indexes a dataset of text chunks and optionally saves the vector index to a Huggingface repository.\n\n    This method retrieves a dataset of text chunks from a specified source, encodes the\n    text chunks into vector representations using the NV-Embed-v2 model, and stores the\n    resulting vector index. If an index repository ID is provided, the vector index is saved to\n    a file in the safetensors format within the specified Huggingface repository.\n\n    !!! example \"Example Usage\"\n        ```python\n        from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n        retriever = SentenceTransformerRetriever()\n        retriever.index(\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n        )\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        chunk_dataset (str): The Huggingface dataset containing the text chunks to be indexed. Either a\n            dataset repository name or a dataset object can be provided.\n        index_repo_id (Optional[str]): The Huggingface repository of the index artifact to be saved.\n        cleanup (bool, optional): Whether to delete the local index directory after saving the vector index.\n        batch_size (int, optional): The batch size to use for encoding the corpus.\n        streamlit_mode (bool): Whether or not this function is being called inside a streamlit app or not.\n    \"\"\"\n    self._chunk_dataset = (\n        load_dataset(chunk_dataset, split=\"chunks\")\n        if isinstance(chunk_dataset, str)\n        else chunk_dataset\n    )\n    corpus = [row[\"text\"] for row in self._chunk_dataset]\n    vector_indices = []\n    streamlit_progressbar = (\n        st.progress(\n            0,\n            text=\"Indexing batches\",\n        )\n        if streamlit_mode and batch_size &gt; 1\n        else None\n    )\n    batch_idx = 1\n\n    for idx in track(\n        range(0, len(corpus), batch_size),\n        description=\"Encoding corpus using NV-Embed-v2\",\n    ):\n        batch = corpus[idx : idx + batch_size]\n        batch_embeddings = self._model.encode(\n            self.add_end_of_sequence_tokens(batch),\n            batch_size=len(batch),\n            normalize_embeddings=True,\n        )\n        vector_indices.append(torch.tensor(batch_embeddings))\n        if streamlit_progressbar:\n            progress_percentage = min(\n                100, max(0, int(((idx + batch_size) / len(corpus)) * 100))\n            )\n            total = (len(corpus) // batch_size) + 1\n            streamlit_progressbar.progress(\n                progress_percentage,\n                text=f\"Indexing batch ({batch_idx}/{total})\",\n            )\n            batch_idx += 1\n\n    self._vector_index = torch.cat(vector_indices, dim=0)\n    with torch.no_grad():\n        if index_repo_id:\n            index_save_dir = os.path.join(\n                \".huggingface\", index_repo_id.split(\"/\")[-1]\n            )\n            os.makedirs(index_save_dir, exist_ok=True)\n            safetensors.torch.save_file(\n                {\"vector_index\": self._vector_index.cpu()},\n                os.path.join(index_save_dir, \"vector_index.safetensors\"),\n            )\n            commit_type = (\n                \"update\"\n                if huggingface_hub.repo_exists(index_repo_id, repo_type=\"model\")\n                else \"add\"\n            )\n            with open(\n                os.path.join(index_save_dir, \"config.json\"), \"w\"\n            ) as config_file:\n                json.dump(\n                    {\"model_name\": self.model_name},\n                    config_file,\n                    indent=4,\n                )\n            save_to_huggingface(\n                index_repo_id,\n                index_save_dir,\n                commit_message=f\"{commit_type}: Contriever index\",\n            )\n            if cleanup:\n                shutil.rmtree(index_save_dir)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.SentenceTransformerRetriever.predict","title":"<code>predict(query, top_k=2, metric=SimilarityMetric.COSINE)</code>  <code>abstractmethod</code>","text":"<p>Predicts the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This method formats the input query string by prepending an instruction prompt and then calls the <code>retrieve</code> method to get the most relevant chunks. The similarity metric can be either cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores are returned.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = SentenceTransformerRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\nretriever.predict(query=\"What is ribosome?\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The input query string to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>@abstractmethod\n@weave.op()\ndef predict(\n    self,\n    query: str,\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Predicts the top-k most relevant chunks for a given query using the specified similarity metric.\n\n    This method formats the input query string by prepending an instruction prompt and then calls the\n    `retrieve` method to get the most relevant chunks. The similarity metric can be either cosine similarity\n    or Euclidean distance. The top-k chunks with the highest similarity scores are returned.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = SentenceTransformerRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        retriever.predict(query=\"What is ribosome?\")\n        ```\n\n    Args:\n        query (str): The input query string to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    return self.retrieve(query, top_k, metric)\n</code></pre>"},{"location":"rag/retrieval/text_retrieval/sentence_transformer/#medrag_multi_modal.retrieval.text_retrieval.sentence_transformer_retrieval.SentenceTransformerRetriever.retrieve","title":"<code>retrieve(query, top_k=2, metric=SimilarityMetric.COSINE)</code>","text":"<p>Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.</p> <p>This method encodes the input query into an embedding and computes similarity scores between the query embedding and the precomputed vector index. The similarity metric can be either cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores are returned as a list of dictionaries, each containing a chunk and its corresponding score.</p> <p>Example Usage</p> <pre><code>import weave\nfrom medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\nweave.init(project_name=\"ml-colabs/medrag-multi-modal\")\nretriever = SentenceTransformerRetriever.from_index(\n    index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n    chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n)\nretriever.retrieve(query=\"What is ribosome?\")\n</code></pre> Optional Speedup using Flash Attention <p>If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply installing the <code>flash-attn</code> package.</p> <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The input query strings to search for relevant chunks.</p> required <code>top_k</code> <code>int</code> <p>The number of top relevant chunks to retrieve.</p> <code>2</code> <code>metric</code> <code>SimilarityMetric</code> <p>The similarity metric to use for scoring.</p> <code>COSINE</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, each containing a retrieved chunk and its relevance score.</p> Source code in <code>medrag_multi_modal/retrieval/text_retrieval/sentence_transformer_retrieval.py</code> <pre><code>@weave.op()\ndef retrieve(\n    self,\n    query: list[str],\n    top_k: int = 2,\n    metric: SimilarityMetric = SimilarityMetric.COSINE,\n):\n    \"\"\"\n    Retrieves the top-k most relevant chunks for a given query using the specified similarity metric.\n\n    This method encodes the input query into an embedding and computes similarity scores between\n    the query embedding and the precomputed vector index. The similarity metric can be either\n    cosine similarity or Euclidean distance. The top-k chunks with the highest similarity scores\n    are returned as a list of dictionaries, each containing a chunk and its corresponding score.\n\n    !!! example \"Example Usage\"\n        ```python\n        import weave\n        from medrag_multi_modal.retrieval.text_retrieval import SentenceTransformerRetriever\n\n        weave.init(project_name=\"ml-colabs/medrag-multi-modal\")\n        retriever = SentenceTransformerRetriever.from_index(\n            index_repo_id=\"ashwiniai/medrag-text-corpus-chunks-nv-embed-2\",\n            chunk_dataset=\"ashwiniai/medrag-text-corpus-chunks\",\n        )\n        retriever.retrieve(query=\"What is ribosome?\")\n        ```\n\n    ??? note \"Optional Speedup using Flash Attention\"\n        If you have a GPU with Flash Attention support, you can enable it for NV-Embed-v2 by simply\n        installing the `flash-attn` package.\n\n        ```bash\n        uv pip install flash-attn --no-build-isolation\n        ```\n\n    Args:\n        query (list[str]): The input query strings to search for relevant chunks.\n        top_k (int, optional): The number of top relevant chunks to retrieve.\n        metric (SimilarityMetric, optional): The similarity metric to use for scoring.\n\n    Returns:\n        list: A list of dictionaries, each containing a retrieved chunk and its relevance score.\n    \"\"\"\n    device = torch.device(get_torch_backend())\n    with torch.no_grad():\n        query_embedding = self._model.encode(\n            self.add_end_of_sequence_tokens(query), normalize_embeddings=True\n        )\n        query_embedding = torch.from_numpy(query_embedding).to(device)\n        if metric == SimilarityMetric.EUCLIDEAN:\n            scores = torch.squeeze(query_embedding @ self._vector_index.T)\n        else:\n            scores = F.cosine_similarity(query_embedding, self._vector_index)\n        scores = scores.cpu().numpy().tolist()\n    scores = argsort_scores(scores, descending=True)[:top_k]\n    retrieved_chunks = []\n    for score in scores:\n        retrieved_chunks.append(\n            {\n                **self._chunk_dataset[score[\"original_index\"]],\n                **{\"score\": score[\"item\"]},\n            }\n        )\n    return retrieved_chunks\n</code></pre>"}]}